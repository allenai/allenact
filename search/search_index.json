{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"FAQ/","text":"FAQ # How do I generate documentation? # Documentation is generated using mkdoc and pydoc-markdown . Building documentation locally # The mkdocs command used to build our documentation relies on all documentation existing as subdirectories of the docs folder. To ensure that all relevant markdown files are placed into this directory, you should always run bash scripts/build_docs.sh from the top-level project directory before running any of the mkdocs commands below. If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build Serving documentation locally # If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/ Modifying and serving documentation locally # If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#how-do-i-generate-documentation","text":"Documentation is generated using mkdoc and pydoc-markdown .","title":"How do I generate documentation?"},{"location":"FAQ/#building-documentation-locally","text":"The mkdocs command used to build our documentation relies on all documentation existing as subdirectories of the docs folder. To ensure that all relevant markdown files are placed into this directory, you should always run bash scripts/build_docs.sh from the top-level project directory before running any of the mkdocs commands below. If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build","title":"Building documentation locally"},{"location":"FAQ/#serving-documentation-locally","text":"If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/","title":"Serving documentation locally"},{"location":"FAQ/#modifying-and-serving-documentation-locally","text":"If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"Modifying and serving documentation locally"},{"location":"getting_started/abstractions/","text":"Primary abstractions # Our package relies on a collection of fundamental abstractions to define how, and in what task, the model should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstract (if relevant). The following provides a high-level description of how these abstractions interact. Experiment configuration # In allenact , experiments are definied by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation . Task sampler # A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation . Task # Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation . Sensor # Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation . Actor critic model # The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation . Training pipeline # The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied . Losses # Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Primary abstractions"},{"location":"getting_started/abstractions/#primary-abstractions","text":"Our package relies on a collection of fundamental abstractions to define how, and in what task, the model should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstract (if relevant). The following provides a high-level description of how these abstractions interact.","title":"Primary abstractions"},{"location":"getting_started/abstractions/#experiment-configuration","text":"In allenact , experiments are definied by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation .","title":"Experiment configuration"},{"location":"getting_started/abstractions/#task-sampler","text":"A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation .","title":"Task sampler"},{"location":"getting_started/abstractions/#task","text":"Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation .","title":"Task"},{"location":"getting_started/abstractions/#sensor","text":"Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation .","title":"Sensor"},{"location":"getting_started/abstractions/#actor-critic-model","text":"The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation .","title":"Actor critic model"},{"location":"getting_started/abstractions/#training-pipeline","text":"The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied .","title":"Training pipeline"},{"location":"getting_started/abstractions/#losses","text":"Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Losses"},{"location":"getting_started/running-your-first-experiment/","text":"Running your first experiment # Assuming you have installed all of the requirements , you can run your first experiment by calling python main.py minigrid_tutorial -b projects/tutorials -m 8 -o minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder minigrid_output will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir minigrid_output/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created designed can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with python main.py my_custom_experiment -b projects/tutorials .","title":"Run your first experiment"},{"location":"getting_started/running-your-first-experiment/#running-your-first-experiment","text":"Assuming you have installed all of the requirements , you can run your first experiment by calling python main.py minigrid_tutorial -b projects/tutorials -m 8 -o minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder minigrid_output will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir minigrid_output/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created designed can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with python main.py my_custom_experiment -b projects/tutorials .","title":"Running your first experiment"},{"location":"getting_started/structure/","text":"Structure of the codebase # The codebase consists of the following directories: core , docs , plugins , projects , scripts , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized. core directory # Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. core.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. core.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. core.models includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations. docs directory # Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation. plugins directory # Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: 1. configs to host useful configuration for the environment or tasks. 1. data to store data to be consumed by the environment or tasks. 1. scripts to setup the plugin or gather and process data. projects directory # Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data. scripts directory # Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments with super-user privileges in Linux with NVIDIA drivers and xserver-xorg installed. tests directory # Includes implementations of tests. utils directory # It includes different types of utilities, mainly divided into: utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. utils.viz_utils , including a SimpleViz class that can be instantiated with different visualization plugins during inference. utils.system , including logging and networking helpers. Other utils files, including utils.misc_utils , contain a number of helper functions for different purposes.","title":"Structure of the codebase"},{"location":"getting_started/structure/#structure-of-the-codebase","text":"The codebase consists of the following directories: core , docs , plugins , projects , scripts , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized.","title":"Structure of the codebase"},{"location":"getting_started/structure/#core-directory","text":"Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. core.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. core.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. core.models includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations.","title":"core directory"},{"location":"getting_started/structure/#docs-directory","text":"Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation.","title":"docs directory"},{"location":"getting_started/structure/#plugins-directory","text":"Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: 1. configs to host useful configuration for the environment or tasks. 1. data to store data to be consumed by the environment or tasks. 1. scripts to setup the plugin or gather and process data.","title":"plugins directory"},{"location":"getting_started/structure/#projects-directory","text":"Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data.","title":"projects directory"},{"location":"getting_started/structure/#scripts-directory","text":"Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments with super-user privileges in Linux with NVIDIA drivers and xserver-xorg installed.","title":"scripts directory"},{"location":"getting_started/structure/#tests-directory","text":"Includes implementations of tests.","title":"tests directory"},{"location":"getting_started/structure/#utils-directory","text":"It includes different types of utilities, mainly divided into: utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. utils.viz_utils , including a SimpleViz class that can be instantiated with different visualization plugins during inference. utils.system , including logging and networking helpers. Other utils files, including utils.misc_utils , contain a number of helper functions for different purposes.","title":"utils directory"},{"location":"howtos/changing-rewards-and-losses/","text":"Changing rewards and losses # In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level. Rewards # We will use the object navigation task in AI2-THOR as a running example. We can see how the ObjectNavTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward Losses # Currently we support A2C , PPO , and imitation losses. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . imitation . Imitation , ), \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Change rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#changing-rewards-and-losses","text":"In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level.","title":"Changing rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#rewards","text":"We will use the object navigation task in AI2-THOR as a running example. We can see how the ObjectNavTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward","title":"Rewards"},{"location":"howtos/changing-rewards-and-losses/#losses","text":"Currently we support A2C , PPO , and imitation losses. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . imitation . Imitation , ), \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Losses"},{"location":"howtos/defining-a-new-model/","text":"Defining a new model # All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example. Actor-critic model interface # As an example, let's build an object navigation agent. class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , ): super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . recurrent_hidden_state_size ) + object_type_embedding_dim , self . recurrent_hidden_state_size , ) self . actor = LinearActorHead ( self . recurrent_hidden_state_size , action_space . n ) self . critic = LinearCriticHead ( self . recurrent_hidden_state_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ): return self . _hidden_size def forward ( self , observations , rnn_hidden_states , prev_actions , masks ): target_encoding = self . object_type_embedding ( observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x = torch . cat ( x , dim = 1 ) x , rnn_hidden_states = self . state_encoder ( x , rnn_hidden_states , masks ) return ( ActorCriticOutput ( distributions = self . actor ( x ), values = self . critic ( x ), extras = {} ), rnn_hidden_states , ) ... On-policy RL engine requirements # Apart from the interface expected by all actor-critic models, we also need to provide a utility function to allow the on-policy RL engine to properly initalize the rollouts storage, num_recurrent_layers : class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): ... @property def num_recurrent_layers ( self ): return self . state_encoder . num_recurrent_layers","title":"Define a new model"},{"location":"howtos/defining-a-new-model/#defining-a-new-model","text":"All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example.","title":"Defining a new model"},{"location":"howtos/defining-a-new-model/#actor-critic-model-interface","text":"As an example, let's build an object navigation agent. class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , ): super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . recurrent_hidden_state_size ) + object_type_embedding_dim , self . recurrent_hidden_state_size , ) self . actor = LinearActorHead ( self . recurrent_hidden_state_size , action_space . n ) self . critic = LinearCriticHead ( self . recurrent_hidden_state_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ): return self . _hidden_size def forward ( self , observations , rnn_hidden_states , prev_actions , masks ): target_encoding = self . object_type_embedding ( observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x = torch . cat ( x , dim = 1 ) x , rnn_hidden_states = self . state_encoder ( x , rnn_hidden_states , masks ) return ( ActorCriticOutput ( distributions = self . actor ( x ), values = self . critic ( x ), extras = {} ), rnn_hidden_states , ) ...","title":"Actor-critic model interface"},{"location":"howtos/defining-a-new-model/#on-policy-rl-engine-requirements","text":"Apart from the interface expected by all actor-critic models, we also need to provide a utility function to allow the on-policy RL engine to properly initalize the rollouts storage, num_recurrent_layers : class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): ... @property def num_recurrent_layers ( self ): return self . state_encoder . num_recurrent_layers","title":"On-policy RL engine requirements"},{"location":"howtos/defining-a-new-task/","text":"Defining a new task # In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing. Task # Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition. Initialization, action space and termination # Let's start with the definition of the action space and task initialization: class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( 'MOVE_AHEAD' , 'ROTATE_LEFT' , 'ROTATE_RIGHT' , 'LOOK_DOWN' , 'LOOK_UP' , 'END' ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ... Step method # Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : int ) -> RLStepResult : action_str = self . class_action_names ()[ action ] if action_str == 'END' : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result def _is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Metrics, rendering and expert actions # Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self ) TaskSampler # We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR. Initialization and termination # class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env Task sampling # Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () object_types_in_scene = set ( [ o [ \"objectType\" ] for o in self . env . last_event . metadata [ \"objects\" ]] ) task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Define a new task"},{"location":"howtos/defining-a-new-task/#defining-a-new-task","text":"In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing.","title":"Defining a new task"},{"location":"howtos/defining-a-new-task/#task","text":"Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition.","title":"Task"},{"location":"howtos/defining-a-new-task/#initialization-action-space-and-termination","text":"Let's start with the definition of the action space and task initialization: class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( 'MOVE_AHEAD' , 'ROTATE_LEFT' , 'ROTATE_RIGHT' , 'LOOK_DOWN' , 'LOOK_UP' , 'END' ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ...","title":"Initialization, action space and termination"},{"location":"howtos/defining-a-new-task/#step-method","text":"Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : int ) -> RLStepResult : action_str = self . class_action_names ()[ action ] if action_str == 'END' : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result def _is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward )","title":"Step method"},{"location":"howtos/defining-a-new-task/#metrics-rendering-and-expert-actions","text":"Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self )","title":"Metrics, rendering and expert actions"},{"location":"howtos/defining-a-new-task/#tasksampler","text":"We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR.","title":"TaskSampler"},{"location":"howtos/defining-a-new-task/#initialization-and-termination","text":"class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env","title":"Initialization and termination"},{"location":"howtos/defining-a-new-task/#task-sampling","text":"Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () object_types_in_scene = set ( [ o [ \"objectType\" ] for o in self . env . last_event . metadata [ \"objects\" ]] ) task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Task sampling"},{"location":"howtos/defining-a-new-training-pipeline/","text":"Defining a new training pipeline # Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. On-policy # We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return TrainingPipeline ( named_losses = { \"ppo_loss\" : PPO ( ** PPOConfig ), }, optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , # Log after every step (in practice after every rollout) num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Imitation (), # We add an imitation loss. \"ppo_loss\" : PPO ( ** PPOConfig ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e5 ) ), ], ) Off-policy # We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Define a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#defining-a-new-training-pipeline","text":"Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers.","title":"Defining a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#on-policy","text":"We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return TrainingPipeline ( named_losses = { \"ppo_loss\" : PPO ( ** PPOConfig ), }, optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , # Log after every step (in practice after every rollout) num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Imitation (), # We add an imitation loss. \"ppo_loss\" : PPO ( ** PPOConfig ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e5 ) ), ], )","title":"On-policy"},{"location":"howtos/defining-a-new-training-pipeline/#off-policy","text":"We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Off-policy"},{"location":"howtos/defining-an-experiment/","text":"Defining an experiment # Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. The interface to be implemented by the experiment specification is defined in core.base_abstractions.experiment_config . The first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ... Model creation # Next, create_model will be used to instantiate object navigation baseline actor-critic models : class ObjectNavThorExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... SCREEN_SIZE = 224 ... OBJECT_TYPES = sorted ([ \"Tomato\" ]) ... SENSORS = [ plugins . ithor_plugin . ithor_sensors . RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), plugins . ithor_plugin . ithor_sensors . GoalObjectTypeThorSensor ( { \"object_types\" : OBJECT_TYPES } ), ] @classmethod def create_model ( cls , ** kwargs ) -> torch . nn . Module : return models . object_nav_models . ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask . class_action_names ()) ), observation_space = core . base_abstractions . sensor . SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ... Training pipeline # In this section we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return utils . experiment_utils . TrainingPipeline ( named_losses = { \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, optimizer = utils . experiment_utils . Builder ( torch . optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks num_mini_batch = 1 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complicated pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), GoalObjectTypeThorSensor ({ \"object_types\" : OBJECT_TYPES }), ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks optimizer = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 if not torch . cuda . is_available () else 30 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Builder ( Imitation ,), # We add an imitation loss. \"ppo_loss\" : Builder ( PPO , default = PPOConfig ,), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e4 ) ), ], ) Note that, in order for the saved configs in the experiment output folder to be fully usable, we currently need to import the module with the parent experiment config relative to the current location. Machine configuration # In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): on_server = torch . cuda . is_available () if mode == \"train\" : nprocesses = 6 if not on_server else 20 gpu_ids = [] if not on_server else [ 0 ] elif mode == \"valid\" : nprocesses = 0 gpu_ids = [] if not on_server else [ 1 ] elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not on_server else [ 0 ] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.is_available() ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers. Task sampling # The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> core . base_abstractions . task . TaskSampler : return plugins . ithor_plugin . ithor_task_samplers . ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : typing . Optional [ typing . List [ int ]] = None , seeds : typing . Optional [ typing . List [ int ]] = None , deterministic_cudnn : bool = False , ) -> typing . Dict [ str , typing . Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = self . SCENE_PERIOD res [ \"env_args\" ][ \"x_display\" ] = None return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Define an experiment"},{"location":"howtos/defining-an-experiment/#defining-an-experiment","text":"Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. The interface to be implemented by the experiment specification is defined in core.base_abstractions.experiment_config . The first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ...","title":"Defining an  experiment"},{"location":"howtos/defining-an-experiment/#model-creation","text":"Next, create_model will be used to instantiate object navigation baseline actor-critic models : class ObjectNavThorExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... SCREEN_SIZE = 224 ... OBJECT_TYPES = sorted ([ \"Tomato\" ]) ... SENSORS = [ plugins . ithor_plugin . ithor_sensors . RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), plugins . ithor_plugin . ithor_sensors . GoalObjectTypeThorSensor ( { \"object_types\" : OBJECT_TYPES } ), ] @classmethod def create_model ( cls , ** kwargs ) -> torch . nn . Module : return models . object_nav_models . ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask . class_action_names ()) ), observation_space = core . base_abstractions . sensor . SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ...","title":"Model creation"},{"location":"howtos/defining-an-experiment/#training-pipeline","text":"In this section we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return utils . experiment_utils . TrainingPipeline ( named_losses = { \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, optimizer = utils . experiment_utils . Builder ( torch . optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks num_mini_batch = 1 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complicated pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), GoalObjectTypeThorSensor ({ \"object_types\" : OBJECT_TYPES }), ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks optimizer = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 if not torch . cuda . is_available () else 30 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Builder ( Imitation ,), # We add an imitation loss. \"ppo_loss\" : Builder ( PPO , default = PPOConfig ,), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e4 ) ), ], ) Note that, in order for the saved configs in the experiment output folder to be fully usable, we currently need to import the module with the parent experiment config relative to the current location.","title":"Training pipeline"},{"location":"howtos/defining-an-experiment/#machine-configuration","text":"In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): on_server = torch . cuda . is_available () if mode == \"train\" : nprocesses = 6 if not on_server else 20 gpu_ids = [] if not on_server else [ 0 ] elif mode == \"valid\" : nprocesses = 0 gpu_ids = [] if not on_server else [ 1 ] elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not on_server else [ 0 ] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.is_available() ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers.","title":"Machine configuration"},{"location":"howtos/defining-an-experiment/#task-sampling","text":"The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> core . base_abstractions . task . TaskSampler : return plugins . ithor_plugin . ithor_task_samplers . ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : typing . Optional [ typing . List [ int ]] = None , seeds : typing . Optional [ typing . List [ int ]] = None , deterministic_cudnn : bool = False , ) -> typing . Dict [ str , typing . Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = self . SCENE_PERIOD res [ \"env_args\" ][ \"x_display\" ] = None return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Task sampling"},{"location":"howtos/running-a-multi-agent-experiment/","text":"To-do #","title":"To-do"},{"location":"howtos/running-a-multi-agent-experiment/#to-do","text":"","title":"To-do"},{"location":"howtos/visualizing-results/","text":"To-do #","title":"To-do"},{"location":"howtos/visualizing-results/#to-do","text":"","title":"To-do"},{"location":"installation/download-datasets/","text":"Downloading datasets # PointNav # RoboTHOR # To get the PointNav dataset and precomputed distance caches for RoboTHOR run the following command: ```shell script cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor ### iTHOR To get the PointNav dataset and precomputed distance caches for `iTHOR` run the following command : `` ` shell script cd projects / pointnav_baselines / dataset sh download_pointnav_dataset . sh ithor Habitat # To get the PointNav habitat dataset download and install the allenact-habitat docker container as described in this tutorial . The dataset is included in the docker iage ObjectNav # RoboTHOR # To get the ObjectNav dataset and precomputed distance caches for RoboTHOR run the following command: ```shell script cd projects/objectnav_baselines/dataset sh download_objectnav_dataset.sh robothor ### iTHOR To get the ObjectNav dataset and precomputed distance caches for `iTHOR` run the following command : `` ` shell script cd projects / objectnav_baselines / dataset sh download_objectnav_dataset . sh ithor TODO: ALFRED #","title":"Download datasets"},{"location":"installation/download-datasets/#downloading-datasets","text":"","title":"Downloading datasets"},{"location":"installation/download-datasets/#pointnav","text":"","title":"PointNav"},{"location":"installation/download-datasets/#robothor","text":"To get the PointNav dataset and precomputed distance caches for RoboTHOR run the following command: ```shell script cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor ### iTHOR To get the PointNav dataset and precomputed distance caches for `iTHOR` run the following command : `` ` shell script cd projects / pointnav_baselines / dataset sh download_pointnav_dataset . sh ithor","title":"RoboTHOR"},{"location":"installation/download-datasets/#habitat","text":"To get the PointNav habitat dataset download and install the allenact-habitat docker container as described in this tutorial . The dataset is included in the docker iage","title":"Habitat"},{"location":"installation/download-datasets/#objectnav","text":"","title":"ObjectNav"},{"location":"installation/download-datasets/#robothor_1","text":"To get the ObjectNav dataset and precomputed distance caches for RoboTHOR run the following command: ```shell script cd projects/objectnav_baselines/dataset sh download_objectnav_dataset.sh robothor ### iTHOR To get the ObjectNav dataset and precomputed distance caches for `iTHOR` run the following command : `` ` shell script cd projects / objectnav_baselines / dataset sh download_objectnav_dataset . sh ithor","title":"RoboTHOR"},{"location":"installation/download-datasets/#todo-alfred","text":"","title":"TODO: ALFRED"},{"location":"installation/installation-allenact/","text":"Installation of AllenAct # Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. In order to install requirements we recommend using pipenv but also include instructions if you would prefer to install things directly using pip . Installing requirements with pipenv ( recommended ) # If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev Installing requirements with pip # Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer using pip , you may install all requirements as follows pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above. Installing supported environments # We also provide installation instructions for the environments supported in AllenAct here .","title":"Install AllenAct"},{"location":"installation/installation-allenact/#installation-of-allenact","text":"Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. In order to install requirements we recommend using pipenv but also include instructions if you would prefer to install things directly using pip .","title":"Installation of AllenAct"},{"location":"installation/installation-allenact/#installing-requirements-with-pipenv-recommended","text":"If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev","title":"Installing requirements with pipenv (recommended)"},{"location":"installation/installation-allenact/#installing-requirements-with-pip","text":"Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer using pip , you may install all requirements as follows pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above.","title":"Installing requirements with pip"},{"location":"installation/installation-allenact/#installing-supported-environments","text":"We also provide installation instructions for the environments supported in AllenAct here .","title":"Installing supported environments"},{"location":"installation/installation-framework/","text":"Installation of supported environments # Below we provide installation instruction for a number of environments that we support. Installation of Minigrid # pip install gym-minigrid Note that gym-minigrid is listed a dependency of allenact and it will be automatically installed along with the framework. Installation of iTHOR # To install iTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install iTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine). Installation of RoboTHOR # To install RoboTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with RoboTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install RoboTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine). Installation of Habitat # To run experiments using Habitat please use our docker image using the following command: docker pull klemenkotar/allenact-habitat:latest This container includes the 0.1.0 release of allenact , the 0.1.5 release of habitat as well as the Gibson point navigation dataset (this is a set of start and goal positions provided by habitat). You then need to launch the container and attach into it: docker run klemenkotar/allenact-habitat --runtime = nvidia -it Once inside the container activate the conda environment: conda activate allenact From within the container download the Gibson scene files into the dataset folder using the instructions provided by the authors . Then proceed to run your experiments using allenact as you normally would.","title":"Install environments"},{"location":"installation/installation-framework/#installation-of-supported-environments","text":"Below we provide installation instruction for a number of environments that we support.","title":"Installation of supported environments"},{"location":"installation/installation-framework/#installation-of-minigrid","text":"pip install gym-minigrid Note that gym-minigrid is listed a dependency of allenact and it will be automatically installed along with the framework.","title":"Installation of Minigrid"},{"location":"installation/installation-framework/#installation-of-ithor","text":"To install iTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install iTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine).","title":"Installation of iTHOR"},{"location":"installation/installation-framework/#installation-of-robothor","text":"To install RoboTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with RoboTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install RoboTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine).","title":"Installation of RoboTHOR"},{"location":"installation/installation-framework/#installation-of-habitat","text":"To run experiments using Habitat please use our docker image using the following command: docker pull klemenkotar/allenact-habitat:latest This container includes the 0.1.0 release of allenact , the 0.1.5 release of habitat as well as the Gibson point navigation dataset (this is a set of start and goal positions provided by habitat). You then need to launch the container and attach into it: docker run klemenkotar/allenact-habitat --runtime = nvidia -it Once inside the container activate the conda environment: conda activate allenact From within the container download the Gibson scene files into the dataset folder using the instructions provided by the authors . Then proceed to run your experiments using allenact as you normally would.","title":"Installation of Habitat"},{"location":"notebooks/firstbook/","text":"To-do #","title":"To-do"},{"location":"notebooks/firstbook/#to-do","text":"","title":"To-do"},{"location":"projects/advisor_2020/","text":"Experiments for Advisor # TODO: # Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#experiments-for-advisor","text":"","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#todo","text":"Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"TODO:"},{"location":"projects/babyai_baselines/","text":"Baseline experiments for the BabyAI environment # We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"BabyAI baselines"},{"location":"projects/babyai_baselines/#baseline-experiments-for-the-babyai-environment","text":"We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"Baseline experiments for the BabyAI environment"},{"location":"projects/objectnav_baselines/","text":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments # TODO: # Add some details. Cite the RoboTHOR paper. Give a list of things you can run with bash commands","title":"ObjectNav baselines"},{"location":"projects/objectnav_baselines/#baseline-models-for-the-object-navigation-task-in-the-robothor-and-ithor-environments","text":"","title":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments"},{"location":"projects/objectnav_baselines/#todo","text":"Add some details. Cite the RoboTHOR paper. Give a list of things you can run with bash commands","title":"TODO:"},{"location":"projects/pointnav_baselines/","text":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments # TODO: # Add some details. Cite the Habitat, Navigation and RoboTHOR paper. Give a list of things you can run with bash commands.","title":"PointNav baselines"},{"location":"projects/pointnav_baselines/#baseline-models-for-the-point-navigation-task-in-the-habitat-robothor-and-ithor-environments","text":"","title":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments"},{"location":"projects/pointnav_baselines/#todo","text":"Add some details. Cite the Habitat, Navigation and RoboTHOR paper. Give a list of things you can run with bash commands.","title":"TODO:"},{"location":"projects/two_body_problem_2019/","text":"Experiments for the Two Body Problem paper # TODO: # Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#experiments-for-the-two-body-problem-paper","text":"","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#todo","text":"Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"TODO:"},{"location":"tutorials/","text":"AllenAct Tutorials # We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework. Navigation in MiniGrid # We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here. PointNav in RoboTHOR # We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here. Swapping in a new environment # This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"AllenAct Tutorials"},{"location":"tutorials/#allenact-tutorials","text":"We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework.","title":"AllenAct Tutorials"},{"location":"tutorials/#navigation-in-minigrid","text":"We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here.","title":"Navigation in MiniGrid"},{"location":"tutorials/#pointnav-in-robothor","text":"We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here.","title":"PointNav in RoboTHOR"},{"location":"tutorials/#swapping-in-a-new-environment","text":"This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"Swapping in a new environment"},{"location":"tutorials/minigrid-tutorial/","text":"Tutorial: Navigation in MiniGrid # In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known. The task # A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls. Experiment configuration file # Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc. Preliminaries # We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\" Sensors and Model # A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in https://github.com/maximecb/gym-minigrid#wrappers. The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , ) Task samplers # We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\" Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation. Machine parameters # Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"gpu_ids\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids . Training pipeline # The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = Builder ( PPO , kwargs = {}, default = PPOConfig ,)), pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known. Training and validation # We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o /PATH/TO/minigrid_output we set the output folder. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: Testing # The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python main.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Navigation in Minigrid"},{"location":"tutorials/minigrid-tutorial/#tutorial-navigation-in-minigrid","text":"In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known.","title":"Tutorial: Navigation in MiniGrid"},{"location":"tutorials/minigrid-tutorial/#the-task","text":"A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls.","title":"The task"},{"location":"tutorials/minigrid-tutorial/#experiment-configuration-file","text":"Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc.","title":"Experiment configuration file"},{"location":"tutorials/minigrid-tutorial/#preliminaries","text":"We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\"","title":"Preliminaries"},{"location":"tutorials/minigrid-tutorial/#sensors-and-model","text":"A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in https://github.com/maximecb/gym-minigrid#wrappers. The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , )","title":"Sensors and Model"},{"location":"tutorials/minigrid-tutorial/#task-samplers","text":"We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\" Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation.","title":"Task samplers"},{"location":"tutorials/minigrid-tutorial/#machine-parameters","text":"Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"gpu_ids\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids .","title":"Machine parameters"},{"location":"tutorials/minigrid-tutorial/#training-pipeline","text":"The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = Builder ( PPO , kwargs = {}, default = PPOConfig ,)), pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known.","title":"Training pipeline"},{"location":"tutorials/minigrid-tutorial/#training-and-validation","text":"We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o /PATH/TO/minigrid_output we set the output folder. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to:","title":"Training and validation"},{"location":"tutorials/minigrid-tutorial/#testing","text":"The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python main.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Testing"},{"location":"tutorials/running_inference_on_a_pretrained_model/","text":"Tutorial: Inference with a pre-trained model #","title":"Using a pre-trained model"},{"location":"tutorials/running_inference_on_a_pretrained_model/#tutorial-inference-with-a-pre-trained-model","text":"","title":"Tutorial: Inference with a pre-trained model"},{"location":"tutorials/training-a-pointnav-model/","text":"Tutorial: PointNav in RoboTHOR # Introduction # One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short. Pointnav # At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the house to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings. What is an environment anyways? # Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment. Learning algorithm # Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While allenact offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward. Environemnt Setup # To setup the RoboTHOR environment please consult the installation guide . Dataset Setup # To train the model on the PointNav task, we need to download the dataset and precomputed cache of distances to the target. The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes. The precomputed cache of distances is a large dictionary containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). We can download and extract the data by navigating to the pointnav_baselines project and running the following script: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor Config File Setup # Now comes the most important part of the tutorial, we are going to write an experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. We will start by creating a new directory inside the projects directory. We can name this whatever we want but for now, we will go with robothor_pointnav_tutuorial . Then we can create a directory called experiments inside the new directory we just created. This hierarchy is not necessary but it helps keep our experiments neatly organized. Now we create a file called pointnav_robothor_rgb_ddppo inside the experiments folder (again the name of this file is arbitrary). We start off by importing everything we will need: import glob from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) from plugins.ithor_plugin.ithor_sensors import RGBSensorThor from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.preprocessor import ObservationSet from core.base_abstractions.task import TaskSampler from plugins.habitat_plugin.habitat_preprocessors import ResnetPreProcessorHabitat from plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from plugins.robothor_plugin.robothor_task_samplers import PointNavDatasetTaskSampler from plugins.robothor_plugin.robothor_tasks import PointNavTask from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Next we define a new experiment config class: class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD = 10 ** 13 NUM_PROCESSES = 60 TRAINING_GPUS = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] VALIDATION_GPUS = [ 7 ] TESTING_GPUS = [ 7 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/val\" Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In allenact the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , \"parallel\" : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES >= ndevices , \"NUM_PROCESSES {} < ndevices\" . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = \"train\" , ** kwargs ): if mode == \"train\" : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS render_video = False elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS render_video = False elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS render_video = False else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) # Disable parallelization for validation process if mode == \"valid\" : for prep in self . PREPROCESSORS : prep . kwargs [ \"parallel\" ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == 'train' or nprocesses > 0 else None return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids , \"sampler_devices\" : sampler_devices if mode == \"train\" else gpu_ids , \"observation_set\" : observation_set , \"render_video\" : render_video , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"observation_set\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = scenes_dir + \"*.json.gz\" if scenes_dir [ - 1 ] == \"/\" else scenes_dir + \"/*.json.gz\" scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = \"10.0\" return res This is it! If we copy all of the code into a file we should be able to run our experiment! Training Model On Debug Dataset # We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. First we need to change the dataset path to point to our small debug dataset. Modify these lines in your file # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" Note that we changed both the train and test dataset to debug, so the model will train and validate on the same 4 episodes. We can not train a model by running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Training Model On Full Dataset # We can also train the model on the full dataset by changing back our dataset path and running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> But be aware, training this takes nearly 2 days on a machine with 8 GPU. For our current setup the following command would work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Conclusion # In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#tutorial-pointnav-in-robothor","text":"","title":"Tutorial: PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#introduction","text":"One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short.","title":"Introduction"},{"location":"tutorials/training-a-pointnav-model/#pointnav","text":"At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the house to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings.","title":"Pointnav"},{"location":"tutorials/training-a-pointnav-model/#what-is-an-environment-anyways","text":"Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment.","title":"What is an environment anyways?"},{"location":"tutorials/training-a-pointnav-model/#learning-algorithm","text":"Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While allenact offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward.","title":"Learning algorithm"},{"location":"tutorials/training-a-pointnav-model/#environemnt-setup","text":"To setup the RoboTHOR environment please consult the installation guide .","title":"Environemnt Setup"},{"location":"tutorials/training-a-pointnav-model/#dataset-setup","text":"To train the model on the PointNav task, we need to download the dataset and precomputed cache of distances to the target. The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes. The precomputed cache of distances is a large dictionary containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). We can download and extract the data by navigating to the pointnav_baselines project and running the following script: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor","title":"Dataset Setup"},{"location":"tutorials/training-a-pointnav-model/#config-file-setup","text":"Now comes the most important part of the tutorial, we are going to write an experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. We will start by creating a new directory inside the projects directory. We can name this whatever we want but for now, we will go with robothor_pointnav_tutuorial . Then we can create a directory called experiments inside the new directory we just created. This hierarchy is not necessary but it helps keep our experiments neatly organized. Now we create a file called pointnav_robothor_rgb_ddppo inside the experiments folder (again the name of this file is arbitrary). We start off by importing everything we will need: import glob from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) from plugins.ithor_plugin.ithor_sensors import RGBSensorThor from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.preprocessor import ObservationSet from core.base_abstractions.task import TaskSampler from plugins.habitat_plugin.habitat_preprocessors import ResnetPreProcessorHabitat from plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from plugins.robothor_plugin.robothor_task_samplers import PointNavDatasetTaskSampler from plugins.robothor_plugin.robothor_tasks import PointNavTask from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Next we define a new experiment config class: class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD = 10 ** 13 NUM_PROCESSES = 60 TRAINING_GPUS = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] VALIDATION_GPUS = [ 7 ] TESTING_GPUS = [ 7 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/val\" Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In allenact the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , \"parallel\" : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES >= ndevices , \"NUM_PROCESSES {} < ndevices\" . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = \"train\" , ** kwargs ): if mode == \"train\" : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS render_video = False elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS render_video = False elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS render_video = False else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) # Disable parallelization for validation process if mode == \"valid\" : for prep in self . PREPROCESSORS : prep . kwargs [ \"parallel\" ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == 'train' or nprocesses > 0 else None return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids , \"sampler_devices\" : sampler_devices if mode == \"train\" else gpu_ids , \"observation_set\" : observation_set , \"render_video\" : render_video , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"observation_set\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = scenes_dir + \"*.json.gz\" if scenes_dir [ - 1 ] == \"/\" else scenes_dir + \"/*.json.gz\" scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = \"10.0\" return res This is it! If we copy all of the code into a file we should be able to run our experiment!","title":"Config File Setup"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-debug-dataset","text":"We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. First we need to change the dataset path to point to our small debug dataset. Modify these lines in your file # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" Note that we changed both the train and test dataset to debug, so the model will train and validate on the same 4 episodes. We can not train a model by running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model On Debug Dataset"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-full-dataset","text":"We can also train the model on the full dataset by changing back our dataset path and running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> But be aware, training this takes nearly 2 days on a machine with 8 GPU. For our current setup the following command would work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model On Full Dataset"},{"location":"tutorials/training-a-pointnav-model/#conclusion","text":"In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"Conclusion"},{"location":"tutorials/training-pipelines/","text":"Tutorial: IL to RL with a training pipeline #","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/training-pipelines/#tutorial-il-to-rl-with-a-training-pipeline","text":"","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/transfering-to-a-different-environment-framework/","text":"Tutorial: Swapping in a new environment # Introduction # This tutorial was designed as a continuation of the Robothor Pointnav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another. RoboTHOR to iTHOR # Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/val\" We also have to download the iTHOR-Pointnav dataset, if we have not done so already: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor That's it! We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" RoboTHOR to Habitat # Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = habitat . get_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = \"habitat/habitat-api/data/scene_datasets/\" CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters. Conclusion # In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Swapping environments"},{"location":"tutorials/transfering-to-a-different-environment-framework/#tutorial-swapping-in-a-new-environment","text":"","title":"Tutorial: Swapping in a new environment"},{"location":"tutorials/transfering-to-a-different-environment-framework/#introduction","text":"This tutorial was designed as a continuation of the Robothor Pointnav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another.","title":"Introduction"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-ithor","text":"Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/val\" We also have to download the iTHOR-Pointnav dataset, if we have not done so already: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor That's it! We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\"","title":"RoboTHOR to iTHOR"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-habitat","text":"Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = habitat . get_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = \"habitat/habitat-api/data/scene_datasets/\" CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters.","title":"RoboTHOR to Habitat"},{"location":"tutorials/transfering-to-a-different-environment-framework/#conclusion","text":"In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Conclusion"}]}