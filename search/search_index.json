{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An open source framework for research in Embodied-AI AllenAct is a modular and flexible learning framework designed with a focus on the unique requirements of Embodied-AI research. It provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. AllenAct is built and backed by the Allen Institute for AI (AI2) . AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. Quick Links # Website & Docs Github Install Tutorials Citation Features & Highlights # Support for multiple environments : Support for the iTHOR , RoboTHOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : Trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch. Contributions # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines . Acknowledgments # This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-lab . License # AllenAct is MIT licensed, as found in the LICENSE file. Team # AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2). Citation # If you use this work, please cite: @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Embodied AI Research} , year = {2020} , journal = {arXiv} , }","title":"Overview"},{"location":"#quick-links","text":"Website & Docs Github Install Tutorials Citation","title":"Quick Links"},{"location":"#features-highlights","text":"Support for multiple environments : Support for the iTHOR , RoboTHOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : Trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch.","title":"Features &amp; Highlights"},{"location":"#contributions","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines .","title":"Contributions"},{"location":"#acknowledgments","text":"This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-lab .","title":"Acknowledgments"},{"location":"#license","text":"AllenAct is MIT licensed, as found in the LICENSE file.","title":"License"},{"location":"#team","text":"AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2).","title":"Team"},{"location":"#citation","text":"If you use this work, please cite: @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Embodied AI Research} , year = {2020} , journal = {arXiv} , }","title":"Citation"},{"location":"CONTRIBUTING/","text":"Contributing # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines. Found a bug or want to suggest an enhancement? # Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement. Making a pull request? # When making a pull request we require that any code respects several guidelines detailed below. Auto-formatting # All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings). Type-checking # Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden. Updating, adding, or removing packages? # We recommend using pipenv to keep track of dependencies, ensure reproducibility, and keep things synchronized. If you are doing so and have modified any installed packages please run: pipenv-setup sync --pipfile # Syncs packages to setup.py pip freeze > requirements.txt # Syncs packages to requirements.py before submitting a pull request. If you are not using pipenv , you are still required to update the file Pipfile with newly installed or modified packages. Moreover you must manually update the install_requires field of the setup.py file. Setting up pre-commit hooks (optional) # Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines.","title":"Contributing"},{"location":"CONTRIBUTING/#found-a-bug-or-want-to-suggest-an-enhancement","text":"Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement.","title":"Found a bug or want to suggest an enhancement?"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When making a pull request we require that any code respects several guidelines detailed below.","title":"Making a pull request?"},{"location":"CONTRIBUTING/#auto-formatting","text":"All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings).","title":"Auto-formatting"},{"location":"CONTRIBUTING/#type-checking","text":"Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden.","title":"Type-checking"},{"location":"CONTRIBUTING/#updating-adding-or-removing-packages","text":"We recommend using pipenv to keep track of dependencies, ensure reproducibility, and keep things synchronized. If you are doing so and have modified any installed packages please run: pipenv-setup sync --pipfile # Syncs packages to setup.py pip freeze > requirements.txt # Syncs packages to requirements.py before submitting a pull request. If you are not using pipenv , you are still required to update the file Pipfile with newly installed or modified packages. Moreover you must manually update the install_requires field of the setup.py file.","title":"Updating, adding, or removing packages?"},{"location":"CONTRIBUTING/#setting-up-pre-commit-hooks-optional","text":"Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Setting up pre-commit hooks (optional)"},{"location":"FAQ/","text":"FAQ # How do I generate documentation? # Documentation is generated using mkdoc and pydoc-markdown . Building documentation locally # If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build Serving documentation locally # If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/ Modifying and serving documentation locally # If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#how-do-i-generate-documentation","text":"Documentation is generated using mkdoc and pydoc-markdown .","title":"How do I generate documentation?"},{"location":"FAQ/#building-documentation-locally","text":"If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build","title":"Building documentation locally"},{"location":"FAQ/#serving-documentation-locally","text":"If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/","title":"Serving documentation locally"},{"location":"FAQ/#modifying-and-serving-documentation-locally","text":"If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"Modifying and serving documentation locally"},{"location":"LICENSE/","text":"MIT License Original work Copyright (c) 2017 Ilya Kostrikov Original work Copyright (c) Facebook, Inc. and its affiliates. Modified work Copyright (c) 2020 Allen Institute for Artificial Intelligence Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"api/constants/","text":"constants [source] #","title":"constants"},{"location":"api/constants/#constants-source","text":"","title":"constants [source]"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/","text":"core.algorithms.offpolicy_sync.losses.abstract_offpolicy_loss [source] # Defining abstract loss classes for actor critic models. AbstractOffPolicyLoss # AbstractOffPolicyLoss ( self , * args , ** kwargs ) Abstract class representing an off-policy loss function used to train a model. loss # AbstractOffPolicyLoss . loss ( self , model : ~ ModelType , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], memory : core . base_abstractions . misc . Memory , args , kwargs , ) -> Tuple [ torch . FloatTensor , Dict [ str , float ], core . base_abstractions . misc . Memory , int ] Computes the loss. Loss after processing a batch of data with (part of) a model (possibly with memory). Parameters model : model to run on data batch (both assumed to be on the same device) batch : data to use as input for model (already on the same device as model) memory : model memory before processing current data batch Returns A tuple with : current_loss : total loss current_info : additional information about the current loss memory : model memory after processing current data batch bsize : batch size","title":"abstract_offpolicy_loss"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#corealgorithmsoffpolicy_synclossesabstract_offpolicy_loss-source","text":"Defining abstract loss classes for actor critic models.","title":"core.algorithms.offpolicy_sync.losses.abstract_offpolicy_loss [source]"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#abstractoffpolicyloss","text":"AbstractOffPolicyLoss ( self , * args , ** kwargs ) Abstract class representing an off-policy loss function used to train a model.","title":"AbstractOffPolicyLoss"},{"location":"api/core/algorithms/offpolicy_sync/losses/abstract_offpolicy_loss/#loss","text":"AbstractOffPolicyLoss . loss ( self , model : ~ ModelType , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], memory : core . base_abstractions . misc . Memory , args , kwargs , ) -> Tuple [ torch . FloatTensor , Dict [ str , float ], core . base_abstractions . misc . Memory , int ] Computes the loss. Loss after processing a batch of data with (part of) a model (possibly with memory). Parameters model : model to run on data batch (both assumed to be on the same device) batch : data to use as input for model (already on the same device as model) memory : model memory before processing current data batch Returns A tuple with : current_loss : total loss current_info : additional information about the current loss memory : model memory after processing current data batch bsize : batch size","title":"loss"},{"location":"api/core/algorithms/onpolicy_sync/engine/","text":"core.algorithms.onpolicy_sync.engine [source] # Defines the reinforcement learning OnPolicyRLEngine . OnPolicyRLEngine # OnPolicyRLEngine ( self , experiment_name : str , config : core . base_abstractions . experiment_config . ExperimentConfig , results_queue : < bound method BaseContext . Queue of < multiprocessing . context . DefaultContext object at 0x10e2c9c50 >> , checkpoints_queue : Optional [ < bound method BaseContext . Queue of < multiprocessing . context . DefaultContext object at 0x10e2c9c50 >> ], checkpoints_dir : str , mode : str = 'train' , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , mp_ctx : Optional [ multiprocessing . context . BaseContext ] = None , worker_id : int = 0 , num_workers : int = 1 , device : Union [ str , torch . device , int ] = 'cpu' , distributed_port : int = 0 , max_sampler_processes_per_worker : Optional [ int ] = None , kwargs , ) The reinforcement learning primary controller. This OnPolicyRLEngine class handles all training, validation, and testing as well as logging and checkpointing. You are not expected to instantiate this class yourself, instead you should define an experiment which will then be used to instantiate an OnPolicyRLEngine and perform any desired tasks. worker_seeds # OnPolicyRLEngine . worker_seeds ( nprocesses : int , initial_seed : Optional [ int ], ) -> List [ int ] Create a collection of seeds for workers without modifying the RNG state.","title":"engine"},{"location":"api/core/algorithms/onpolicy_sync/engine/#corealgorithmsonpolicy_syncengine-source","text":"Defines the reinforcement learning OnPolicyRLEngine .","title":"core.algorithms.onpolicy_sync.engine [source]"},{"location":"api/core/algorithms/onpolicy_sync/engine/#onpolicyrlengine","text":"OnPolicyRLEngine ( self , experiment_name : str , config : core . base_abstractions . experiment_config . ExperimentConfig , results_queue : < bound method BaseContext . Queue of < multiprocessing . context . DefaultContext object at 0x10e2c9c50 >> , checkpoints_queue : Optional [ < bound method BaseContext . Queue of < multiprocessing . context . DefaultContext object at 0x10e2c9c50 >> ], checkpoints_dir : str , mode : str = 'train' , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , mp_ctx : Optional [ multiprocessing . context . BaseContext ] = None , worker_id : int = 0 , num_workers : int = 1 , device : Union [ str , torch . device , int ] = 'cpu' , distributed_port : int = 0 , max_sampler_processes_per_worker : Optional [ int ] = None , kwargs , ) The reinforcement learning primary controller. This OnPolicyRLEngine class handles all training, validation, and testing as well as logging and checkpointing. You are not expected to instantiate this class yourself, instead you should define an experiment which will then be used to instantiate an OnPolicyRLEngine and perform any desired tasks.","title":"OnPolicyRLEngine"},{"location":"api/core/algorithms/onpolicy_sync/engine/#worker_seeds","text":"OnPolicyRLEngine . worker_seeds ( nprocesses : int , initial_seed : Optional [ int ], ) -> List [ int ] Create a collection of seeds for workers without modifying the RNG state.","title":"worker_seeds"},{"location":"api/core/algorithms/onpolicy_sync/policy/","text":"core.algorithms.onpolicy_sync.policy [source] # ActorCriticModel # ActorCriticModel ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , ) Abstract class defining a deep (recurrent) actor critic agent. When defining a new agent, you should over subclass this class and implement the abstract methods. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This is of type gym.spaces.dict . forward # ActorCriticModel . forward ( self , observations : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], memory : core . base_abstractions . misc . Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ core . base_abstractions . misc . ActorCriticOutput [ ~ DistributionType ], Optional [ core . base_abstractions . misc . Memory ]] Transforms input observations (& previous hidden state) into action probabilities and the state value. Parameters observations : Multi-level map from key strings to tensors of shape [steps, samplers, (agents,) ...] with the current observations. memory : Memory object with recurrent memory. The shape of each tensor is determined by the corresponding entry in _recurrent_memory_specification . prev_actions : tensor of shape [steps, samplers, agents, ...] with the previous actions. masks : tensor of shape [steps, samplers, agents, 1] with zeros indicating steps where a new episode/task starts. Returns A tuple whose first element is an object of class ActorCriticOutput which stores the agent's probability distribution over possible actions (shape [steps, samplers, agents, num_actions]), the agent's value for the state (shape [steps, samplers, agents, 1]), and any extra information needed for loss computations. The second element is an optional Memory , which is only used in models with recurrent memory. recurrent_memory_specification # The memory specification for the ActorCriticModel . See docs for _recurrent_memory_shape Returns The memory specification from _recurrent_memory_shape .","title":"policy"},{"location":"api/core/algorithms/onpolicy_sync/policy/#corealgorithmsonpolicy_syncpolicy-source","text":"","title":"core.algorithms.onpolicy_sync.policy [source]"},{"location":"api/core/algorithms/onpolicy_sync/policy/#actorcriticmodel","text":"ActorCriticModel ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , ) Abstract class defining a deep (recurrent) actor critic agent. When defining a new agent, you should over subclass this class and implement the abstract methods. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This is of type gym.spaces.dict .","title":"ActorCriticModel"},{"location":"api/core/algorithms/onpolicy_sync/policy/#forward","text":"ActorCriticModel . forward ( self , observations : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], memory : core . base_abstractions . misc . Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ core . base_abstractions . misc . ActorCriticOutput [ ~ DistributionType ], Optional [ core . base_abstractions . misc . Memory ]] Transforms input observations (& previous hidden state) into action probabilities and the state value. Parameters observations : Multi-level map from key strings to tensors of shape [steps, samplers, (agents,) ...] with the current observations. memory : Memory object with recurrent memory. The shape of each tensor is determined by the corresponding entry in _recurrent_memory_specification . prev_actions : tensor of shape [steps, samplers, agents, ...] with the previous actions. masks : tensor of shape [steps, samplers, agents, 1] with zeros indicating steps where a new episode/task starts. Returns A tuple whose first element is an object of class ActorCriticOutput which stores the agent's probability distribution over possible actions (shape [steps, samplers, agents, num_actions]), the agent's value for the state (shape [steps, samplers, agents, 1]), and any extra information needed for loss computations. The second element is an optional Memory , which is only used in models with recurrent memory.","title":"forward"},{"location":"api/core/algorithms/onpolicy_sync/policy/#recurrent_memory_specification","text":"The memory specification for the ActorCriticModel . See docs for _recurrent_memory_shape Returns The memory specification from _recurrent_memory_shape .","title":"recurrent_memory_specification"},{"location":"api/core/algorithms/onpolicy_sync/runner/","text":"core.algorithms.onpolicy_sync.runner [source] # Defines the reinforcement learning OnPolicyRunner .","title":"runner"},{"location":"api/core/algorithms/onpolicy_sync/runner/#corealgorithmsonpolicy_syncrunner-source","text":"Defines the reinforcement learning OnPolicyRunner .","title":"core.algorithms.onpolicy_sync.runner [source]"},{"location":"api/core/algorithms/onpolicy_sync/storage/","text":"core.algorithms.onpolicy_sync.storage [source] # RolloutStorage # RolloutStorage ( self , num_steps : int , num_samplers : int , actor_critic : core . algorithms . onpolicy_sync . policy . ActorCriticModel , args , kwargs , ) Class for storing rollout information for RL trainers. FLATTEN_SEPARATOR # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"storage"},{"location":"api/core/algorithms/onpolicy_sync/storage/#corealgorithmsonpolicy_syncstorage-source","text":"","title":"core.algorithms.onpolicy_sync.storage [source]"},{"location":"api/core/algorithms/onpolicy_sync/storage/#rolloutstorage","text":"RolloutStorage ( self , num_steps : int , num_samplers : int , actor_critic : core . algorithms . onpolicy_sync . policy . ActorCriticModel , args , kwargs , ) Class for storing rollout information for RL trainers.","title":"RolloutStorage"},{"location":"api/core/algorithms/onpolicy_sync/storage/#flatten_separator","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"FLATTEN_SEPARATOR"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/","text":"core.algorithms.onpolicy_sync.vector_sampled_tasks [source] # SingleProcessVectorSampledTasks # SingleProcessVectorSampledTasks ( self , make_sampler_fn : Callable [ ... , core . base_abstractions . task . TaskSampler ], sampler_fn_args_list : Sequence [ Dict [ str , Any ]] = None , auto_resample_when_done : bool = True , should_log : bool = True , metrics_out_queue : Optional [ queue . Queue ] = None , ) -> None Vectorized collection of tasks. Simultaneously handles the state of multiple TaskSamplers and their associated tasks. Allows for interacting with these tasks in a vectorized manner. When a task completes, another task is sampled from the appropriate task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. attr # SingleProcessVectorSampledTasks . attr ( self , attr_names : Union [ List [ str ], str ], ) -> List [ Any ] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions. attr_at # SingleProcessVectorSampledTasks . attr_at ( self , sampler_index : int , attr_name : str , ) -> Any Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function. call # SingleProcessVectorSampledTasks . call ( self , function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None , ) -> List [ Any ] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions. call_at # SingleProcessVectorSampledTasks . call_at ( self , sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None , ) -> Any Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. command_at # SingleProcessVectorSampledTasks . command_at ( self , sampler_index : int , command : str , data : Optional [ Any ] = None , ) -> Any Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. get_observations # SingleProcessVectorSampledTasks . get_observations ( self ) Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks. is_closed # Has the vector task been closed. next_task # SingleProcessVectorSampledTasks . next_task ( self , ** kwargs ) Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks. next_task_at # SingleProcessVectorSampledTasks . next_task_at ( self , index_process : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the generator to be reset. Returns List of length one containing the observations the newly sampled task. num_unpaused_tasks # Number of unpaused processes. Returns Number of unpaused processes. pause_at # SingleProcessVectorSampledTasks . pause_at ( self , sampler_index : int ) -> None Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one. render # SingleProcessVectorSampledTasks . render ( self , mode : str = 'human' , args , kwargs , ) -> Union [ numpy . ndarray , NoneType , List [ numpy . ndarray ]] Render observations from all Tasks in a tiled image or a list of images. reset_all # SingleProcessVectorSampledTasks . reset_all ( self ) Reset all task samplers to their initial state (except for the RNG seed). resume_all # SingleProcessVectorSampledTasks . resume_all ( self ) -> None Resumes any paused processes. set_seeds # SingleProcessVectorSampledTasks . set_seeds ( self , seeds : List [ int ]) Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds. step # SingleProcessVectorSampledTasks . step ( self , actions : List [ List [ int ]]) Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks. step_at # SingleProcessVectorSampledTasks . step_at ( self , index_process : int , action : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Step in the index_process task in the vector. Parameters index_process : Index of the process to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process. VectorSampledTasks # VectorSampledTasks ( self , make_sampler_fn : Callable [ ... , core . base_abstractions . task . TaskSampler ], sampler_fn_args : Sequence [ Dict [ str , Any ]] = None , auto_resample_when_done : bool = True , multiprocessing_start_method : Optional [ str ] = 'forkserver' , mp_ctx : Optional [ multiprocessing . context . BaseContext ] = None , metrics_out_queue : < bound method BaseContext . Queue of < multiprocessing . context . DefaultContext object at 0x10838fbe0 >> = None , should_log : bool = True , max_processes : Optional [ int ] = None , ) -> None Vectorized collection of tasks. Creates multiple processes where each process runs its own TaskSampler. Each process generates one Task from its TaskSampler at a time and this class allows for interacting with these tasks in a vectorized manner. When a task on a process completes, the process samples another task from its task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. multiprocessing_start_method : the multiprocessing method used to spawn worker processes. Valid methods are {'spawn', 'forkserver', 'fork'} 'forkserver' is the recommended method as it works well with CUDA. If 'fork' is used, the subproccess must be started before any other GPU useage. async_step # VectorSampledTasks . async_step ( self , actions : List [ List [ int ]]) -> None Asynchronously step in the vectorized Tasks. Parameters actions : actions to be performed in the vectorized Tasks. attr # VectorSampledTasks . attr ( self , attr_names : Union [ List [ str ], str ]) -> List [ Any ] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions. attr_at # VectorSampledTasks . attr_at ( self , sampler_index : int , attr_name : str ) -> Any Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function. call # VectorSampledTasks . call ( self , function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None , ) -> List [ Any ] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions. call_at # VectorSampledTasks . call_at ( self , sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None , ) -> Any Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function. command_at # VectorSampledTasks . command_at ( self , sampler_index : int , command : str , data : Optional [ Any ] = None , ) -> Any Runs the command on the selected task and returns the result. Parameters Returns Result of the command. get_observations # VectorSampledTasks . get_observations ( self ) Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks. is_closed # Has the vector task been closed. mp_ctx # Get the multiprocessing process used by the vector task. Returns The multiprocessing context. next_task # VectorSampledTasks . next_task ( self , ** kwargs ) Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks. next_task_at # VectorSampledTasks . next_task_at ( self , sampler_index : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the process to be reset. Returns List of length one containing the observations the newly sampled task. num_unpaused_tasks # Number of unpaused processes. Returns Number of unpaused processes. pause_at # VectorSampledTasks . pause_at ( self , sampler_index : int ) -> None Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one. render # VectorSampledTasks . render ( self , mode : str = 'human' , args , kwargs , ) -> Union [ numpy . ndarray , NoneType , List [ numpy . ndarray ]] Render observations from all Tasks in a tiled image or list of images. reset_all # VectorSampledTasks . reset_all ( self ) Reset all task samplers to their initial state (except for the RNG seed). resume_all # VectorSampledTasks . resume_all ( self ) -> None Resumes any paused processes. set_seeds # VectorSampledTasks . set_seeds ( self , seeds : List [ int ]) Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds. step # VectorSampledTasks . step ( self , actions : List [ List [ int ]]) Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks. step_at # VectorSampledTasks . step_at ( self , sampler_index : int , action : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Step in the index_process task in the vector. Parameters sampler_index : Index of the sampler to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process. wait_step # VectorSampledTasks . wait_step ( self ) -> List [ Dict [ str , Any ]] Wait until all the asynchronized processes have synchronized.","title":"vector_sampled_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#corealgorithmsonpolicy_syncvector_sampled_tasks-source","text":"","title":"core.algorithms.onpolicy_sync.vector_sampled_tasks [source]"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#singleprocessvectorsampledtasks","text":"SingleProcessVectorSampledTasks ( self , make_sampler_fn : Callable [ ... , core . base_abstractions . task . TaskSampler ], sampler_fn_args_list : Sequence [ Dict [ str , Any ]] = None , auto_resample_when_done : bool = True , should_log : bool = True , metrics_out_queue : Optional [ queue . Queue ] = None , ) -> None Vectorized collection of tasks. Simultaneously handles the state of multiple TaskSamplers and their associated tasks. Allows for interacting with these tasks in a vectorized manner. When a task completes, another task is sampled from the appropriate task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks.","title":"SingleProcessVectorSampledTasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#attr","text":"SingleProcessVectorSampledTasks . attr ( self , attr_names : Union [ List [ str ], str ], ) -> List [ Any ] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions.","title":"attr"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#attr_at","text":"SingleProcessVectorSampledTasks . attr_at ( self , sampler_index : int , attr_name : str , ) -> Any Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function.","title":"attr_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#call","text":"SingleProcessVectorSampledTasks . call ( self , function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None , ) -> List [ Any ] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions.","title":"call"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#call_at","text":"SingleProcessVectorSampledTasks . call_at ( self , sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None , ) -> Any Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"call_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#command_at","text":"SingleProcessVectorSampledTasks . command_at ( self , sampler_index : int , command : str , data : Optional [ Any ] = None , ) -> Any Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"command_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#get_observations","text":"SingleProcessVectorSampledTasks . get_observations ( self ) Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks.","title":"get_observations"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#is_closed","text":"Has the vector task been closed.","title":"is_closed"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#next_task","text":"SingleProcessVectorSampledTasks . next_task ( self , ** kwargs ) Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks.","title":"next_task"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#next_task_at","text":"SingleProcessVectorSampledTasks . next_task_at ( self , index_process : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the generator to be reset. Returns List of length one containing the observations the newly sampled task.","title":"next_task_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#num_unpaused_tasks","text":"Number of unpaused processes. Returns Number of unpaused processes.","title":"num_unpaused_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#pause_at","text":"SingleProcessVectorSampledTasks . pause_at ( self , sampler_index : int ) -> None Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one.","title":"pause_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#render","text":"SingleProcessVectorSampledTasks . render ( self , mode : str = 'human' , args , kwargs , ) -> Union [ numpy . ndarray , NoneType , List [ numpy . ndarray ]] Render observations from all Tasks in a tiled image or a list of images.","title":"render"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#reset_all","text":"SingleProcessVectorSampledTasks . reset_all ( self ) Reset all task samplers to their initial state (except for the RNG seed).","title":"reset_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#resume_all","text":"SingleProcessVectorSampledTasks . resume_all ( self ) -> None Resumes any paused processes.","title":"resume_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#set_seeds","text":"SingleProcessVectorSampledTasks . set_seeds ( self , seeds : List [ int ]) Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds.","title":"set_seeds"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#step","text":"SingleProcessVectorSampledTasks . step ( self , actions : List [ List [ int ]]) Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks.","title":"step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#step_at","text":"SingleProcessVectorSampledTasks . step_at ( self , index_process : int , action : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Step in the index_process task in the vector. Parameters index_process : Index of the process to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process.","title":"step_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#vectorsampledtasks","text":"VectorSampledTasks ( self , make_sampler_fn : Callable [ ... , core . base_abstractions . task . TaskSampler ], sampler_fn_args : Sequence [ Dict [ str , Any ]] = None , auto_resample_when_done : bool = True , multiprocessing_start_method : Optional [ str ] = 'forkserver' , mp_ctx : Optional [ multiprocessing . context . BaseContext ] = None , metrics_out_queue : < bound method BaseContext . Queue of < multiprocessing . context . DefaultContext object at 0x10838fbe0 >> = None , should_log : bool = True , max_processes : Optional [ int ] = None , ) -> None Vectorized collection of tasks. Creates multiple processes where each process runs its own TaskSampler. Each process generates one Task from its TaskSampler at a time and this class allows for interacting with these tasks in a vectorized manner. When a task on a process completes, the process samples another task from its task sampler. All the tasks are synchronized (for step and new_task methods). Attributes make_sampler_fn : function which creates a single TaskSampler. sampler_fn_args : sequence of dictionaries describing the args to pass to make_sampler_fn on each individual process. auto_resample_when_done : automatically sample a new Task from the TaskSampler when the Task completes. If False, a new Task will not be resampled until all Tasks on all processes have completed. This functionality is provided for seamless training of vectorized Tasks. multiprocessing_start_method : the multiprocessing method used to spawn worker processes. Valid methods are {'spawn', 'forkserver', 'fork'} 'forkserver' is the recommended method as it works well with CUDA. If 'fork' is used, the subproccess must be started before any other GPU useage.","title":"VectorSampledTasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#async_step","text":"VectorSampledTasks . async_step ( self , actions : List [ List [ int ]]) -> None Asynchronously step in the vectorized Tasks. Parameters actions : actions to be performed in the vectorized Tasks.","title":"async_step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#attr_1","text":"VectorSampledTasks . attr ( self , attr_names : Union [ List [ str ], str ]) -> List [ Any ] Gets the attributes (specified by name) on the tasks. Parameters attr_names : The name of the functions to call on the tasks. Returns List of results of calling the functions.","title":"attr"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#attr_at_1","text":"VectorSampledTasks . attr_at ( self , sampler_index : int , attr_name : str ) -> Any Gets the attribute (specified by name) on the selected task and returns it. Parameters index : Which task to call the function on. attr_name : The name of the function to call on the task. Returns Result of calling the function.","title":"attr_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#call_1","text":"VectorSampledTasks . call ( self , function_names : Union [ str , List [ str ]], function_args_list : Optional [ List [ Any ]] = None , ) -> List [ Any ] Calls a list of functions (which are passed by name) on the corresponding task (by index). Parameters function_names : The name of the functions to call on the tasks. function_args_list : List of function args for each function. If provided, len(function_args_list) should be as long as len(function_names). Returns List of results of calling the functions.","title":"call"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#call_at_1","text":"VectorSampledTasks . call_at ( self , sampler_index : int , function_name : str , function_args : Optional [ List [ Any ]] = None , ) -> Any Calls a function (which is passed by name) on the selected task and returns the result. Parameters index : Which task to call the function on. function_name : The name of the function to call on the task. function_args : Optional function args. Returns Result of calling the function.","title":"call_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#command_at_1","text":"VectorSampledTasks . command_at ( self , sampler_index : int , command : str , data : Optional [ Any ] = None , ) -> Any Runs the command on the selected task and returns the result. Parameters Returns Result of the command.","title":"command_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#get_observations_1","text":"VectorSampledTasks . get_observations ( self ) Get observations for all unpaused tasks. Returns List of observations for each of the unpaused tasks.","title":"get_observations"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#is_closed_1","text":"Has the vector task been closed.","title":"is_closed"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#mp_ctx","text":"Get the multiprocessing process used by the vector task. Returns The multiprocessing context.","title":"mp_ctx"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#next_task_1","text":"VectorSampledTasks . next_task ( self , ** kwargs ) Move to the the next Task for all TaskSamplers. Parameters kwargs : key word arguments passed to the next_task function of the samplers. Returns List of initial observations for each of the new tasks.","title":"next_task"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#next_task_at_1","text":"VectorSampledTasks . next_task_at ( self , sampler_index : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Move to the the next Task from the TaskSampler in index_process process in the vector. Parameters index_process : Index of the process to be reset. Returns List of length one containing the observations the newly sampled task.","title":"next_task_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#num_unpaused_tasks_1","text":"Number of unpaused processes. Returns Number of unpaused processes.","title":"num_unpaused_tasks"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#pause_at_1","text":"VectorSampledTasks . pause_at ( self , sampler_index : int ) -> None Pauses computation on the Task in process index without destroying the Task. This is useful for not needing to call steps on all Tasks when only some are active (for example during the last samples of running eval). Parameters index : which process to pause. All indexes after this one will be shifted down by one.","title":"pause_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#render_1","text":"VectorSampledTasks . render ( self , mode : str = 'human' , args , kwargs , ) -> Union [ numpy . ndarray , NoneType , List [ numpy . ndarray ]] Render observations from all Tasks in a tiled image or list of images.","title":"render"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#reset_all_1","text":"VectorSampledTasks . reset_all ( self ) Reset all task samplers to their initial state (except for the RNG seed).","title":"reset_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#resume_all_1","text":"VectorSampledTasks . resume_all ( self ) -> None Resumes any paused processes.","title":"resume_all"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#set_seeds_1","text":"VectorSampledTasks . set_seeds ( self , seeds : List [ int ]) Sets new tasks' RNG seeds. Parameters seeds : List of size _num_samplers containing new RNG seeds.","title":"set_seeds"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#step_1","text":"VectorSampledTasks . step ( self , actions : List [ List [ int ]]) Perform actions in the vectorized tasks. Parameters actions : List of size _num_samplers containing action to be taken in each task. Returns List of outputs from the step method of tasks.","title":"step"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#step_at_1","text":"VectorSampledTasks . step_at ( self , sampler_index : int , action : int , ) -> List [ core . base_abstractions . misc . RLStepResult ] Step in the index_process task in the vector. Parameters sampler_index : Index of the sampler to be reset. action : The action to take. Returns List containing the output of step method on the task in the indexed process.","title":"step_at"},{"location":"api/core/algorithms/onpolicy_sync/vector_sampled_tasks/#wait_step","text":"VectorSampledTasks . wait_step ( self ) -> List [ Dict [ str , Any ]] Wait until all the asynchronized processes have synchronized.","title":"wait_step"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/","text":"core.algorithms.onpolicy_sync.losses.a2cacktr [source] # Implementation of A2C and ACKTR losses. A2C # A2C ( self , value_loss_coef , entropy_coef , * args , ** kwargs ) A2C Loss. A2CACKTR # A2CACKTR ( self , value_loss_coef , entropy_coef , acktr = False , * args , ** kwargs ) Class implementing A2C and ACKTR losses. Attributes acktr : True if should use ACKTR loss (currently not supported), otherwise uses A2C loss. value_loss_coef : Weight of value loss. entropy_coef : Weight of entropy (encouraging) loss. ACKTR # ACKTR ( self , value_loss_coef , entropy_coef , * args , ** kwargs ) ACKTR Loss. This code is not supported as it currently lacks an implementation for recurrent models.","title":"a2cacktr"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#corealgorithmsonpolicy_synclossesa2cacktr-source","text":"Implementation of A2C and ACKTR losses.","title":"core.algorithms.onpolicy_sync.losses.a2cacktr [source]"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#a2c","text":"A2C ( self , value_loss_coef , entropy_coef , * args , ** kwargs ) A2C Loss.","title":"A2C"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#a2cacktr","text":"A2CACKTR ( self , value_loss_coef , entropy_coef , acktr = False , * args , ** kwargs ) Class implementing A2C and ACKTR losses. Attributes acktr : True if should use ACKTR loss (currently not supported), otherwise uses A2C loss. value_loss_coef : Weight of value loss. entropy_coef : Weight of entropy (encouraging) loss.","title":"A2CACKTR"},{"location":"api/core/algorithms/onpolicy_sync/losses/a2cacktr/#acktr","text":"ACKTR ( self , value_loss_coef , entropy_coef , * args , ** kwargs ) ACKTR Loss. This code is not supported as it currently lacks an implementation for recurrent models.","title":"ACKTR"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/","text":"core.algorithms.onpolicy_sync.losses.abstract_loss [source] # Defining abstract loss classes for actor critic models. AbstractActorCriticLoss # AbstractActorCriticLoss ( self , * args , ** kwargs ) Abstract class representing a loss function used to train an ActorCriticModel. loss # AbstractActorCriticLoss . loss ( self , step_count : int , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], actor_critic_output : core . base_abstractions . misc . ActorCriticOutput , args , kwargs , ) -> Tuple [ torch . FloatTensor , Dict [ str , float ]] Computes the loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. kwargs : Extra kwargs. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"abstract_loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/#corealgorithmsonpolicy_synclossesabstract_loss-source","text":"Defining abstract loss classes for actor critic models.","title":"core.algorithms.onpolicy_sync.losses.abstract_loss [source]"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/#abstractactorcriticloss","text":"AbstractActorCriticLoss ( self , * args , ** kwargs ) Abstract class representing a loss function used to train an ActorCriticModel.","title":"AbstractActorCriticLoss"},{"location":"api/core/algorithms/onpolicy_sync/losses/abstract_loss/#loss","text":"AbstractActorCriticLoss . loss ( self , step_count : int , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], actor_critic_output : core . base_abstractions . misc . ActorCriticOutput , args , kwargs , ) -> Tuple [ torch . FloatTensor , Dict [ str , float ]] Computes the loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. kwargs : Extra kwargs. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/","text":"core.algorithms.onpolicy_sync.losses.imitation [source] # Defining imitation losses for actor critic type models. Imitation # Imitation ( self , * args , ** kwargs ) Expert imitation loss. loss # Imitation . loss ( self , step_count : int , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], actor_critic_output : core . base_abstractions . misc . ActorCriticOutput , args , kwargs , ) Computes the imitation loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"imitation"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/#corealgorithmsonpolicy_synclossesimitation-source","text":"Defining imitation losses for actor critic type models.","title":"core.algorithms.onpolicy_sync.losses.imitation [source]"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/#imitation","text":"Imitation ( self , * args , ** kwargs ) Expert imitation loss.","title":"Imitation"},{"location":"api/core/algorithms/onpolicy_sync/losses/imitation/#loss","text":"Imitation . loss ( self , step_count : int , batch : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], actor_critic_output : core . base_abstractions . misc . ActorCriticOutput , args , kwargs , ) Computes the imitation loss. Parameters batch : A batch of data corresponding to the information collected when rolling out (possibly many) agents over a fixed number of steps. In particular this batch should have the same format as that returned by RolloutStorage.recurrent_generator . Here batch[\"observations\"] must contain \"expert_action\" observations or \"expert_policy\" observations. See ExpertActionSensor (or ExpertPolicySensor ) for an example of a sensor producing such observations. actor_critic_output : The output of calling an ActorCriticModel on the observations in batch . args : Extra args. Ignored. kwargs : Extra kwargs. Ignored. Returns A (0-dimensional) torch.FloatTensor corresponding to the computed loss. .backward() will be called on this tensor in order to compute a gradient update to the ActorCriticModel's parameters.","title":"loss"},{"location":"api/core/algorithms/onpolicy_sync/losses/kfac/","text":"core.algorithms.onpolicy_sync.losses.kfac [source] # Implementation of the KFAC optimizer. TODO: this code is not supported as it currently lacks an implementation for recurrent models.","title":"kfac"},{"location":"api/core/algorithms/onpolicy_sync/losses/kfac/#corealgorithmsonpolicy_synclosseskfac-source","text":"Implementation of the KFAC optimizer. TODO: this code is not supported as it currently lacks an implementation for recurrent models.","title":"core.algorithms.onpolicy_sync.losses.kfac [source]"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/","text":"core.algorithms.onpolicy_sync.losses.ppo [source] # Defining the PPO loss for actor critic type models. PPO # PPO ( self , clip_param : float , value_loss_coef : float , entropy_coef : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , args , kwargs , ) Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. value_loss_coef : Weight of the value loss. entropy_coef : Weight of the entropy (encouraging) loss. use_clipped_value_loss : Whether or not to also clip the value loss. PPOValue # PPOValue ( self , clip_param : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , args , kwargs , ) Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. use_clipped_value_loss : Whether or not to also clip the value loss.","title":"ppo"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#corealgorithmsonpolicy_synclossesppo-source","text":"Defining the PPO loss for actor critic type models.","title":"core.algorithms.onpolicy_sync.losses.ppo [source]"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#ppo","text":"PPO ( self , clip_param : float , value_loss_coef : float , entropy_coef : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , args , kwargs , ) Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. value_loss_coef : Weight of the value loss. entropy_coef : Weight of the entropy (encouraging) loss. use_clipped_value_loss : Whether or not to also clip the value loss.","title":"PPO"},{"location":"api/core/algorithms/onpolicy_sync/losses/ppo/#ppovalue","text":"PPOValue ( self , clip_param : float , use_clipped_value_loss = True , clip_decay : Optional [ Callable [[ int ], float ]] = None , args , kwargs , ) Implementation of the Proximal Policy Optimization loss. Attributes clip_param : The clipping parameter to use. use_clipped_value_loss : Whether or not to also clip the value loss.","title":"PPOValue"},{"location":"api/core/base_abstractions/distributions/","text":"core.base_abstractions.distributions [source] # AddBias # AddBias ( self , bias : torch . FloatTensor ) Adding bias parameters to input values. forward # AddBias . forward ( self , x : torch . FloatTensor ) -> torch . FloatTensor Adds the stored bias parameters to x . Bernoulli # Bernoulli ( self , num_inputs , num_outputs ) A learned Bernoulli distribution. CategoricalDistr # CategoricalDistr ( self , probs = None , logits = None , validate_args = None ) A categorical distribution extending PyTorch's Categorical. DiagGaussian # DiagGaussian ( self , num_inputs , num_outputs ) A learned diagonal Gaussian distribution. FixedBernoulli # FixedBernoulli ( self , probs = None , logits = None , validate_args = None ) A fixed Bernoulli distribution extending PyTorch's Bernoulli. FixedNormal # FixedNormal ( self , loc , scale , validate_args = None ) A fixed normal distribution extending PyTorch's Normal.","title":"distributions"},{"location":"api/core/base_abstractions/distributions/#corebase_abstractionsdistributions-source","text":"","title":"core.base_abstractions.distributions [source]"},{"location":"api/core/base_abstractions/distributions/#addbias","text":"AddBias ( self , bias : torch . FloatTensor ) Adding bias parameters to input values.","title":"AddBias"},{"location":"api/core/base_abstractions/distributions/#forward","text":"AddBias . forward ( self , x : torch . FloatTensor ) -> torch . FloatTensor Adds the stored bias parameters to x .","title":"forward"},{"location":"api/core/base_abstractions/distributions/#bernoulli","text":"Bernoulli ( self , num_inputs , num_outputs ) A learned Bernoulli distribution.","title":"Bernoulli"},{"location":"api/core/base_abstractions/distributions/#categoricaldistr","text":"CategoricalDistr ( self , probs = None , logits = None , validate_args = None ) A categorical distribution extending PyTorch's Categorical.","title":"CategoricalDistr"},{"location":"api/core/base_abstractions/distributions/#diaggaussian","text":"DiagGaussian ( self , num_inputs , num_outputs ) A learned diagonal Gaussian distribution.","title":"DiagGaussian"},{"location":"api/core/base_abstractions/distributions/#fixedbernoulli","text":"FixedBernoulli ( self , probs = None , logits = None , validate_args = None ) A fixed Bernoulli distribution extending PyTorch's Bernoulli.","title":"FixedBernoulli"},{"location":"api/core/base_abstractions/distributions/#fixednormal","text":"FixedNormal ( self , loc , scale , validate_args = None ) A fixed normal distribution extending PyTorch's Normal.","title":"FixedNormal"},{"location":"api/core/base_abstractions/experiment_config/","text":"core.base_abstractions.experiment_config [source] # Defines the ExperimentConfig abstract class used as the basis of all experiments. ExperimentConfig # ExperimentConfig ( self , / , * args , ** kwargs ) Abstract class used to define experiments. Instead of using yaml or text files, experiments in our framework are defined as a class. In particular, to define an experiment one must define a new class inheriting from this class which implements all of the below methods. The below methods will then be called when running the experiment. create_model # ExperimentConfig . create_model ( ** kwargs ) -> torch . nn . modules . module . Module Create the neural model. machine_params # ExperimentConfig . machine_params ( mode = 'train' , ** kwargs ) -> Dict [ str , Any ] Parameters used to specify machine information. Machine information includes at least (1) the number of processes to train with and (2) the gpu devices indices to use. mode : Whether or not the machine parameters should be those for \"train\", \"valid\", or \"test\". kwargs : Extra kwargs. Returns A dictionary of the form {\"nprocesses\" : ..., \"gpu_ids\": ..., ...} . Here nprocesses must be a non-negative integer, gpu_ids must be a sequence of non-negative integers (if empty, then everything will be run on the cpu). make_sampler_fn # ExperimentConfig . make_sampler_fn ( kwargs , ) -> core . base_abstractions . task . TaskSampler Create the TaskSampler given keyword arguments. These kwargs will be generated by one of ExperimentConfig.train_task_sampler_args , ExperimentConfig.valid_task_sampler_args , or ExperimentConfig.test_task_sampler_args depending on whether the user has chosen to train, validate, or test. tag # ExperimentConfig . tag () -> str A string describing the experiment. test_task_sampler_args # ExperimentConfig . test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ] Specifies the test parameters for the process_ind th test process. See ExperimentConfig.train_task_sampler_args for parameter definitions. train_task_sampler_args # ExperimentConfig . train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ] Specifies the training parameters for the process_ind th training process. These parameters are meant be passed as keyword arguments to ExperimentConfig.make_sampler_fn to generate a task sampler. Parameters process_ind : The unique index of the training process ( 0 \u2264 process_ind < total_processes ). total_processes : The total number of training processes. devices : Gpu devices (if any) to use. seeds : The seeds to use, if any. deterministic_cudnn : Whether or not to use deterministic cudnn. Returns The parameters for make_sampler_fn training_pipeline # ExperimentConfig . training_pipeline ( kwargs , ) -> utils . experiment_utils . TrainingPipeline Creates the training pipeline. Parameters kwargs : Extra kwargs. Currently unused. Returns An instantiate TrainingPipeline object. valid_task_sampler_args # ExperimentConfig . valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ] Specifies the validation parameters for the process_ind th validation process. See ExperimentConfig.train_task_sampler_args for parameter definitions. FrozenClassVariables # FrozenClassVariables ( self , / , * args , ** kwargs ) Metaclass for ExperimentConfig. Ensures ExperimentConfig class-level attributes cannot be modified. ExperimentConfig attributes can still be modified at the object level.","title":"experiment_config"},{"location":"api/core/base_abstractions/experiment_config/#corebase_abstractionsexperiment_config-source","text":"Defines the ExperimentConfig abstract class used as the basis of all experiments.","title":"core.base_abstractions.experiment_config [source]"},{"location":"api/core/base_abstractions/experiment_config/#experimentconfig","text":"ExperimentConfig ( self , / , * args , ** kwargs ) Abstract class used to define experiments. Instead of using yaml or text files, experiments in our framework are defined as a class. In particular, to define an experiment one must define a new class inheriting from this class which implements all of the below methods. The below methods will then be called when running the experiment.","title":"ExperimentConfig"},{"location":"api/core/base_abstractions/experiment_config/#create_model","text":"ExperimentConfig . create_model ( ** kwargs ) -> torch . nn . modules . module . Module Create the neural model.","title":"create_model"},{"location":"api/core/base_abstractions/experiment_config/#machine_params","text":"ExperimentConfig . machine_params ( mode = 'train' , ** kwargs ) -> Dict [ str , Any ] Parameters used to specify machine information. Machine information includes at least (1) the number of processes to train with and (2) the gpu devices indices to use. mode : Whether or not the machine parameters should be those for \"train\", \"valid\", or \"test\". kwargs : Extra kwargs. Returns A dictionary of the form {\"nprocesses\" : ..., \"gpu_ids\": ..., ...} . Here nprocesses must be a non-negative integer, gpu_ids must be a sequence of non-negative integers (if empty, then everything will be run on the cpu).","title":"machine_params"},{"location":"api/core/base_abstractions/experiment_config/#make_sampler_fn","text":"ExperimentConfig . make_sampler_fn ( kwargs , ) -> core . base_abstractions . task . TaskSampler Create the TaskSampler given keyword arguments. These kwargs will be generated by one of ExperimentConfig.train_task_sampler_args , ExperimentConfig.valid_task_sampler_args , or ExperimentConfig.test_task_sampler_args depending on whether the user has chosen to train, validate, or test.","title":"make_sampler_fn"},{"location":"api/core/base_abstractions/experiment_config/#tag","text":"ExperimentConfig . tag () -> str A string describing the experiment.","title":"tag"},{"location":"api/core/base_abstractions/experiment_config/#test_task_sampler_args","text":"ExperimentConfig . test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ] Specifies the test parameters for the process_ind th test process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"test_task_sampler_args"},{"location":"api/core/base_abstractions/experiment_config/#train_task_sampler_args","text":"ExperimentConfig . train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ] Specifies the training parameters for the process_ind th training process. These parameters are meant be passed as keyword arguments to ExperimentConfig.make_sampler_fn to generate a task sampler. Parameters process_ind : The unique index of the training process ( 0 \u2264 process_ind < total_processes ). total_processes : The total number of training processes. devices : Gpu devices (if any) to use. seeds : The seeds to use, if any. deterministic_cudnn : Whether or not to use deterministic cudnn. Returns The parameters for make_sampler_fn","title":"train_task_sampler_args"},{"location":"api/core/base_abstractions/experiment_config/#training_pipeline","text":"ExperimentConfig . training_pipeline ( kwargs , ) -> utils . experiment_utils . TrainingPipeline Creates the training pipeline. Parameters kwargs : Extra kwargs. Currently unused. Returns An instantiate TrainingPipeline object.","title":"training_pipeline"},{"location":"api/core/base_abstractions/experiment_config/#valid_task_sampler_args","text":"ExperimentConfig . valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ] Specifies the validation parameters for the process_ind th validation process. See ExperimentConfig.train_task_sampler_args for parameter definitions.","title":"valid_task_sampler_args"},{"location":"api/core/base_abstractions/experiment_config/#frozenclassvariables","text":"FrozenClassVariables ( self , / , * args , ** kwargs ) Metaclass for ExperimentConfig. Ensures ExperimentConfig class-level attributes cannot be modified. ExperimentConfig attributes can still be modified at the object level.","title":"FrozenClassVariables"},{"location":"api/core/base_abstractions/misc/","text":"core.base_abstractions.misc [source] # Memory # Memory ( self , * args , ** kwargs ) check_append # Memory . check_append ( self , key : str , tensor : torch . Tensor , sampler_dim : int , ) -> 'Memory' Appends a new memory type given its identifier, its memory tensor and its sampler dim. Parameters key : string identifier of the memory type tensor : memory tensor sampler_dim : sampler dimension Returns Updated Memory sampler_dim # Memory . sampler_dim ( self , key : str ) -> int Returns the sampler dimension for the given memory type. Parameters key : string identifier of the memory type Returns The sampler dim sampler_select # Memory . sampler_select ( self , keep : Sequence [ int ]) -> 'Memory' Equivalent to PyTorch index_select along the sampler_dim of each memory type. Parameters keep : a list of sampler indices to keep Returns Selected memory set_tensor # Memory . set_tensor ( self , key : str , tensor : torch . Tensor ) -> 'Memory' Replaces tensor for given key with an updated version. Parameters key : memory type identifier to update tensor : updated tensor Returns Updated memory slice # Memory . slice ( self , dim : int , start : Optional [ int ] = None , stop : Optional [ int ] = None , step : int = 1 , ) -> 'Memory' Slicing for dimensions that have same extents in all memory types. It also accepts negative indices. Parameters dim : the dimension to slice start : the index of the first item to keep if given (default 0 if None) stop : the index of the first item to discard if given (default tensor size along dim if None) step : the increment between consecutive indices (default 1) Returns Sliced memory step_select # Memory . step_select ( self , step : int ) -> 'Memory' Equivalent to slicing with length 1 for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step step_squeeze # Memory . step_squeeze ( self , step : int ) -> 'Memory' Equivalent to simple indexing for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step (and squeezed step dimension) tensor # Memory . tensor ( self , key : str ) -> torch . Tensor Returns the memory tensor for a given memory type. Parameters key : string identifier of the memory type Returns Memory tensor for type key RLStepResult # RLStepResult ( self , / , * args , ** kwargs ) RLStepResult(observation, reward, done, info) done # Alias for field number 2 info # Alias for field number 3 observation # Alias for field number 0 reward # Alias for field number 1","title":"misc"},{"location":"api/core/base_abstractions/misc/#corebase_abstractionsmisc-source","text":"","title":"core.base_abstractions.misc [source]"},{"location":"api/core/base_abstractions/misc/#memory","text":"Memory ( self , * args , ** kwargs )","title":"Memory"},{"location":"api/core/base_abstractions/misc/#check_append","text":"Memory . check_append ( self , key : str , tensor : torch . Tensor , sampler_dim : int , ) -> 'Memory' Appends a new memory type given its identifier, its memory tensor and its sampler dim. Parameters key : string identifier of the memory type tensor : memory tensor sampler_dim : sampler dimension Returns Updated Memory","title":"check_append"},{"location":"api/core/base_abstractions/misc/#sampler_dim","text":"Memory . sampler_dim ( self , key : str ) -> int Returns the sampler dimension for the given memory type. Parameters key : string identifier of the memory type Returns The sampler dim","title":"sampler_dim"},{"location":"api/core/base_abstractions/misc/#sampler_select","text":"Memory . sampler_select ( self , keep : Sequence [ int ]) -> 'Memory' Equivalent to PyTorch index_select along the sampler_dim of each memory type. Parameters keep : a list of sampler indices to keep Returns Selected memory","title":"sampler_select"},{"location":"api/core/base_abstractions/misc/#set_tensor","text":"Memory . set_tensor ( self , key : str , tensor : torch . Tensor ) -> 'Memory' Replaces tensor for given key with an updated version. Parameters key : memory type identifier to update tensor : updated tensor Returns Updated memory","title":"set_tensor"},{"location":"api/core/base_abstractions/misc/#slice","text":"Memory . slice ( self , dim : int , start : Optional [ int ] = None , stop : Optional [ int ] = None , step : int = 1 , ) -> 'Memory' Slicing for dimensions that have same extents in all memory types. It also accepts negative indices. Parameters dim : the dimension to slice start : the index of the first item to keep if given (default 0 if None) stop : the index of the first item to discard if given (default tensor size along dim if None) step : the increment between consecutive indices (default 1) Returns Sliced memory","title":"slice"},{"location":"api/core/base_abstractions/misc/#step_select","text":"Memory . step_select ( self , step : int ) -> 'Memory' Equivalent to slicing with length 1 for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step","title":"step_select"},{"location":"api/core/base_abstractions/misc/#step_squeeze","text":"Memory . step_squeeze ( self , step : int ) -> 'Memory' Equivalent to simple indexing for the step (i.e first) dimension in rollouts storage. Parameters step : step to keep Returns Sliced memory with a single step (and squeezed step dimension)","title":"step_squeeze"},{"location":"api/core/base_abstractions/misc/#tensor","text":"Memory . tensor ( self , key : str ) -> torch . Tensor Returns the memory tensor for a given memory type. Parameters key : string identifier of the memory type Returns Memory tensor for type key","title":"tensor"},{"location":"api/core/base_abstractions/misc/#rlstepresult","text":"RLStepResult ( self , / , * args , ** kwargs ) RLStepResult(observation, reward, done, info)","title":"RLStepResult"},{"location":"api/core/base_abstractions/misc/#done","text":"Alias for field number 2","title":"done"},{"location":"api/core/base_abstractions/misc/#info","text":"Alias for field number 3","title":"info"},{"location":"api/core/base_abstractions/misc/#observation","text":"Alias for field number 0","title":"observation"},{"location":"api/core/base_abstractions/misc/#reward","text":"Alias for field number 1","title":"reward"},{"location":"api/core/base_abstractions/preprocessor/","text":"core.base_abstractions.preprocessor [source] # ObservationSet # ObservationSet ( self , source_ids : List [ str ], all_preprocessors : List [ Union [ core . base_abstractions . preprocessor . Preprocessor , utils . experiment_utils . Builder [ core . base_abstractions . preprocessor . Preprocessor ]]], all_sensors : List [ core . base_abstractions . sensor . Sensor ], ) -> None Represents a list of source_ids, corresponding to sensors and preprocessors, with each source being identified through a unique id. Attributes source_ids : List containing sensor and preprocessor ids to be consumed by agents. Each source uuid must be unique. graph : Computation graph for all preprocessors. observation_spaces : Observation spaces of all output sources. device : Device where the PreprocessorGraph is executed. get # ObservationSet . get ( self , uuid : str , ) -> core . base_abstractions . preprocessor . Preprocessor Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid . get_observations # ObservationSet . get_observations ( self , obs : Dict [ str , Any ], args : Any , kwargs : Any , ) -> Dict [ str , Any ] Get all observations within a dictionary. Returns Collect observations from all sources and return them packaged inside a Dict. Preprocessor # Preprocessor ( self , input_uuids : List [ str ], output_uuid : str , observation_space : gym . spaces . space . Space , kwargs : Any , ) -> None Represents a preprocessor that transforms data from a sensor or another preprocessor to the input of agents or other preprocessors. The user of this class needs to implement the process method and the user is also required to set the below attributes: Attributes: input_uuids : List of input universally unique ids . uuid : Universally unique id . observation_space : `` gym . Space `` object corresponding to processed observation spaces . process # Preprocessor . process ( self , obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Any Returns processed observations from sensors or other preprocessors. Parameters obs : Dict with available observations and processed observations. Returns Processed observation. PreprocessorGraph # PreprocessorGraph ( self , preprocessors : List [ Union [ core . base_abstractions . preprocessor . Preprocessor , utils . experiment_utils . Builder [ core . base_abstractions . preprocessor . Preprocessor ]]], ) -> None Represents a graph of preprocessors, with each preprocessor being identified through a universally unique id. Attributes preprocessors : List containing preprocessors with required input uuids, output uuid of each sensor must be unique. get # PreprocessorGraph . get ( self , uuid : str , ) -> core . base_abstractions . preprocessor . Preprocessor Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid . get_observations # PreprocessorGraph . get_observations ( self , obs : Dict [ str , Any ], args : Any , kwargs : Any , ) -> Dict [ str , Any ] Get processed observations. Returns Collect observations processed from all sensors and return them packaged inside a Dict.","title":"preprocessor"},{"location":"api/core/base_abstractions/preprocessor/#corebase_abstractionspreprocessor-source","text":"","title":"core.base_abstractions.preprocessor [source]"},{"location":"api/core/base_abstractions/preprocessor/#observationset","text":"ObservationSet ( self , source_ids : List [ str ], all_preprocessors : List [ Union [ core . base_abstractions . preprocessor . Preprocessor , utils . experiment_utils . Builder [ core . base_abstractions . preprocessor . Preprocessor ]]], all_sensors : List [ core . base_abstractions . sensor . Sensor ], ) -> None Represents a list of source_ids, corresponding to sensors and preprocessors, with each source being identified through a unique id. Attributes source_ids : List containing sensor and preprocessor ids to be consumed by agents. Each source uuid must be unique. graph : Computation graph for all preprocessors. observation_spaces : Observation spaces of all output sources. device : Device where the PreprocessorGraph is executed.","title":"ObservationSet"},{"location":"api/core/base_abstractions/preprocessor/#get","text":"ObservationSet . get ( self , uuid : str , ) -> core . base_abstractions . preprocessor . Preprocessor Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid .","title":"get"},{"location":"api/core/base_abstractions/preprocessor/#get_observations","text":"ObservationSet . get_observations ( self , obs : Dict [ str , Any ], args : Any , kwargs : Any , ) -> Dict [ str , Any ] Get all observations within a dictionary. Returns Collect observations from all sources and return them packaged inside a Dict.","title":"get_observations"},{"location":"api/core/base_abstractions/preprocessor/#preprocessor","text":"Preprocessor ( self , input_uuids : List [ str ], output_uuid : str , observation_space : gym . spaces . space . Space , kwargs : Any , ) -> None Represents a preprocessor that transforms data from a sensor or another preprocessor to the input of agents or other preprocessors. The user of this class needs to implement the process method and the user is also required to set the below attributes: Attributes: input_uuids : List of input universally unique ids . uuid : Universally unique id . observation_space : `` gym . Space `` object corresponding to processed observation spaces .","title":"Preprocessor"},{"location":"api/core/base_abstractions/preprocessor/#process","text":"Preprocessor . process ( self , obs : Dict [ str , Any ], * args : Any , ** kwargs : Any ) -> Any Returns processed observations from sensors or other preprocessors. Parameters obs : Dict with available observations and processed observations. Returns Processed observation.","title":"process"},{"location":"api/core/base_abstractions/preprocessor/#preprocessorgraph","text":"PreprocessorGraph ( self , preprocessors : List [ Union [ core . base_abstractions . preprocessor . Preprocessor , utils . experiment_utils . Builder [ core . base_abstractions . preprocessor . Preprocessor ]]], ) -> None Represents a graph of preprocessors, with each preprocessor being identified through a universally unique id. Attributes preprocessors : List containing preprocessors with required input uuids, output uuid of each sensor must be unique.","title":"PreprocessorGraph"},{"location":"api/core/base_abstractions/preprocessor/#get_1","text":"PreprocessorGraph . get ( self , uuid : str , ) -> core . base_abstractions . preprocessor . Preprocessor Return preprocessor with the given uuid . Parameters uuid : The unique id of the preprocessor. Returns The preprocessor with unique id uuid .","title":"get"},{"location":"api/core/base_abstractions/preprocessor/#get_observations_1","text":"PreprocessorGraph . get_observations ( self , obs : Dict [ str , Any ], args : Any , kwargs : Any , ) -> Dict [ str , Any ] Get processed observations. Returns Collect observations processed from all sensors and return them packaged inside a Dict.","title":"get_observations"},{"location":"api/core/base_abstractions/sensor/","text":"core.base_abstractions.sensor [source] # ResNetSensor # ResNetSensor ( self , mean : Optional [ numpy . ndarray ] = None , stdev : Optional [ numpy . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'resnet' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - inf , unnormalized_supremum : float = inf , scale_first : bool = True , kwargs : Any , ) to # ResNetSensor . to ( self , device : torch . device ) -> 'ResNetSensor' Moves sensor to specified device. Parameters device : The device for the sensor. Sensor # Sensor ( self , uuid : str , observation_space : gym . spaces . space . Space , kwargs : Any , ) -> None Represents a sensor that provides data from the environment to agent. The user of this class needs to implement the get_observation method and the user is also required to set the below attributes: Attributes uuid : universally unique id. observation_space : gym.Space object corresponding to observation of sensor. get_observation # Sensor . get_observation ( self , env : ~ EnvType , task : Optional [ ~ SubTaskType ], args : Any , kwargs : Any , ) -> Any Returns observations from the environment (or task). Parameters env : The environment the sensor is used upon. task : (Optionally) a Task from which the sensor should get data. Returns Current observation for Sensor. SensorSuite # SensorSuite ( self , sensors : Sequence [ core . base_abstractions . sensor . Sensor ]) -> None Represents a set of sensors, with each sensor being identified through a unique id. Attributes sensors : list containing sensors for the environment, uuid of each sensor must be unique. get # SensorSuite . get ( self , uuid : str ) -> core . base_abstractions . sensor . Sensor Return sensor with the given uuid . Parameters uuid : The unique id of the sensor Returns The sensor with unique id uuid . get_observations # SensorSuite . get_observations ( self , env : ~ EnvType , task : Optional [ ~ SubTaskType ], kwargs : Any , ) -> Dict [ str , Any ] Get all observations corresponding to the sensors in the suite. Parameters env : The environment from which to get the observation. task : (Optionally) the task from which to get the observation. Returns Data from all sensors packaged inside a Dict. VisionSensor # VisionSensor ( self , mean : Optional [ numpy . ndarray ] = None , stdev : Optional [ numpy . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'vision' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - inf , unnormalized_supremum : float = inf , scale_first : bool = True , kwargs : Any , ) height # Height that input image will be rescale to have. Returns The height as a non-negative integer or None if no rescaling is done. width # Width that input image will be rescale to have. Returns The width as a non-negative integer or None if no rescaling is done.","title":"sensor"},{"location":"api/core/base_abstractions/sensor/#corebase_abstractionssensor-source","text":"","title":"core.base_abstractions.sensor [source]"},{"location":"api/core/base_abstractions/sensor/#resnetsensor","text":"ResNetSensor ( self , mean : Optional [ numpy . ndarray ] = None , stdev : Optional [ numpy . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'resnet' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - inf , unnormalized_supremum : float = inf , scale_first : bool = True , kwargs : Any , )","title":"ResNetSensor"},{"location":"api/core/base_abstractions/sensor/#to","text":"ResNetSensor . to ( self , device : torch . device ) -> 'ResNetSensor' Moves sensor to specified device. Parameters device : The device for the sensor.","title":"to"},{"location":"api/core/base_abstractions/sensor/#sensor","text":"Sensor ( self , uuid : str , observation_space : gym . spaces . space . Space , kwargs : Any , ) -> None Represents a sensor that provides data from the environment to agent. The user of this class needs to implement the get_observation method and the user is also required to set the below attributes: Attributes uuid : universally unique id. observation_space : gym.Space object corresponding to observation of sensor.","title":"Sensor"},{"location":"api/core/base_abstractions/sensor/#get_observation","text":"Sensor . get_observation ( self , env : ~ EnvType , task : Optional [ ~ SubTaskType ], args : Any , kwargs : Any , ) -> Any Returns observations from the environment (or task). Parameters env : The environment the sensor is used upon. task : (Optionally) a Task from which the sensor should get data. Returns Current observation for Sensor.","title":"get_observation"},{"location":"api/core/base_abstractions/sensor/#sensorsuite","text":"SensorSuite ( self , sensors : Sequence [ core . base_abstractions . sensor . Sensor ]) -> None Represents a set of sensors, with each sensor being identified through a unique id. Attributes sensors : list containing sensors for the environment, uuid of each sensor must be unique.","title":"SensorSuite"},{"location":"api/core/base_abstractions/sensor/#get","text":"SensorSuite . get ( self , uuid : str ) -> core . base_abstractions . sensor . Sensor Return sensor with the given uuid . Parameters uuid : The unique id of the sensor Returns The sensor with unique id uuid .","title":"get"},{"location":"api/core/base_abstractions/sensor/#get_observations","text":"SensorSuite . get_observations ( self , env : ~ EnvType , task : Optional [ ~ SubTaskType ], kwargs : Any , ) -> Dict [ str , Any ] Get all observations corresponding to the sensors in the suite. Parameters env : The environment from which to get the observation. task : (Optionally) the task from which to get the observation. Returns Data from all sensors packaged inside a Dict.","title":"get_observations"},{"location":"api/core/base_abstractions/sensor/#visionsensor","text":"VisionSensor ( self , mean : Optional [ numpy . ndarray ] = None , stdev : Optional [ numpy . ndarray ] = None , height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'vision' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : Optional [ int ] = None , unnormalized_infimum : float = - inf , unnormalized_supremum : float = inf , scale_first : bool = True , kwargs : Any , )","title":"VisionSensor"},{"location":"api/core/base_abstractions/sensor/#height","text":"Height that input image will be rescale to have. Returns The height as a non-negative integer or None if no rescaling is done.","title":"height"},{"location":"api/core/base_abstractions/sensor/#width","text":"Width that input image will be rescale to have. Returns The width as a non-negative integer or None if no rescaling is done.","title":"width"},{"location":"api/core/base_abstractions/task/","text":"core.base_abstractions.task [source] # Defines the primary data structures by which agents interact with their environment. Task # Task ( self , env : ~ EnvType , sensors : Union [ core . base_abstractions . sensor . SensorSuite , Sequence [ core . base_abstractions . sensor . Sensor ]], task_info : Dict [ str , Any ], max_steps : int , kwargs , ) -> None An abstract class defining a, goal directed, 'task.' Agents interact with their environment through a task by taking a step after which they receive new observations, rewards, and (potentially) other useful information. A Task is a helpful generalization of the OpenAI gym's Env class and allows for multiple tasks (e.g. point and object navigation) to be defined on a single environment (e.g. AI2-THOR). Attributes env : The environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. action_names # Task . action_names ( self ) -> Tuple [ str , ... ] Action names of the Task instance. This method should be overwritten if class_action_names requires key word arguments to determine the number of actions. action_space # Task's action space. Returns The action space for the task. class_action_names # Task . class_action_names ( ** kwargs ) -> Tuple [ str , ... ] A tuple of action names. Parameters kwargs : Keyword arguments. Returns Tuple of (ordered) action names so that taking action running task.step(i) corresponds to taking action task.class_action_names()[i]. close # Task . close ( self ) -> None Closes the environment and any other files opened by the Task (if applicable). cumulative_reward # Mean per-agent total cumulative in the task so far. Returns Mean per-agent cumulative reward as a float. index_to_action # Task . index_to_action ( self , index : int ) -> str Returns the action name correspond to index . is_done # Task . is_done ( self ) -> bool Did the agent reach a terminal state or performed the maximum number of steps. metrics # Task . metrics ( self ) -> Dict [ str , Any ] Computes metrics related to the task after the task's completion. By default this function is automatically called during training and the reported metrics logged to tensorboard. Returns A dictionary where every key is a string (the metric's name) and the value is the value of the metric. num_steps_taken # Task . num_steps_taken ( self ) -> int Number of steps taken by the agent in the task so far. query_expert # Task . query_expert ( self , ** kwargs ) -> Tuple [ Any , bool ] Query the expert policy for this task. Returns A tuple (x, y) where x is the expert action (or policy) and y is False if the expert could not determine the optimal action (otherwise True). Here y is used for masking. Even when y is False, x should still lie in the space of possible values (e.g. if x is the expert policy then x should be the correct length, sum to 1, and have non-negative entries). reached_max_steps # Task . reached_max_steps ( self ) -> bool Has the agent reached the maximum number of steps. reached_terminal_state # Task . reached_terminal_state ( self ) -> bool Has the agent reached a terminal state (excluding reaching the maximum number of steps). render # Task . render ( self , mode : str = 'rgb' , * args , ** kwargs ) -> numpy . ndarray Render the current task state. Rendered task state can come in any supported modes. Parameters mode : The mode in which to render. For example, you might have a 'rgb' mode that renders the agent's egocentric viewpoint or a 'dev' mode returning additional information. args : Extra args. kwargs : Extra kwargs. Returns An numpy array corresponding to the requested render. step # Task . step ( self , action : Union [ int , Sequence [ int ]], ) -> core . base_abstractions . misc . RLStepResult Take an action in the environment (one per agent). Takes the action in the environment corresponding to self.class_action_names()[action] for each action if it's a Sequence and returns observations (& rewards and any additional information) corresponding to the agent's new state. Note that this function should not be overwritten without care (instead implement the _step function). Parameters action : The action to take. Returns A RLStepResult object encoding the new observations, reward, and (possibly) additional information. total_actions # Total number of actions available to an agent in this Task. TaskSampler # TaskSampler ( self , / , * args , ** kwargs ) Abstract class defining a how new tasks are sampled. all_observation_spaces_equal # Checks if all observation spaces of tasks that can be sampled are equal. This will almost always simply return True . A case in which it should return False includes, for example, a setting where you design a TaskSampler that can generate different types of tasks, i.e. point navigation tasks and object navigation tasks. In this case, these different tasks may output different types of observations. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. close # TaskSampler . close ( self ) -> None Closes any open environments or streams. Should be run when done sampling. last_sampled_task # Get the most recently sampled Task. Returns The most recently sampled Task. length # Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). next_task # TaskSampler . next_task ( self , force_advance_scene : bool = False , ) -> Optional [ core . base_abstractions . task . Task ] Get the next task in the sampler's stream. Parameters force_advance_scene : Used to (if applicable) force the task sampler to use a new scene for the next task. This is useful if, during training, you would like to train with one scene for some number of steps and then explicitly control when you begin training with the next scene. Returns The next Task in the sampler's stream if a next task exists. Otherwise None. reset # TaskSampler . reset ( self ) -> None Resets task sampler to its original state (except for any seed). set_seed # TaskSampler . set_seed ( self , seed : int ) -> None Sets new RNG seed. Parameters seed : New seed. total_unique # Total unique tasks. Returns Total number of unique tasks that can be sampled. Can be float('inf') or, if the total unique is not known, None.","title":"task"},{"location":"api/core/base_abstractions/task/#corebase_abstractionstask-source","text":"Defines the primary data structures by which agents interact with their environment.","title":"core.base_abstractions.task [source]"},{"location":"api/core/base_abstractions/task/#task","text":"Task ( self , env : ~ EnvType , sensors : Union [ core . base_abstractions . sensor . SensorSuite , Sequence [ core . base_abstractions . sensor . Sensor ]], task_info : Dict [ str , Any ], max_steps : int , kwargs , ) -> None An abstract class defining a, goal directed, 'task.' Agents interact with their environment through a task by taking a step after which they receive new observations, rewards, and (potentially) other useful information. A Task is a helpful generalization of the OpenAI gym's Env class and allows for multiple tasks (e.g. point and object navigation) to be defined on a single environment (e.g. AI2-THOR). Attributes env : The environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : Dictionary of (k, v) pairs defining task goals and other task information. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"Task"},{"location":"api/core/base_abstractions/task/#action_names","text":"Task . action_names ( self ) -> Tuple [ str , ... ] Action names of the Task instance. This method should be overwritten if class_action_names requires key word arguments to determine the number of actions.","title":"action_names"},{"location":"api/core/base_abstractions/task/#action_space","text":"Task's action space. Returns The action space for the task.","title":"action_space"},{"location":"api/core/base_abstractions/task/#class_action_names","text":"Task . class_action_names ( ** kwargs ) -> Tuple [ str , ... ] A tuple of action names. Parameters kwargs : Keyword arguments. Returns Tuple of (ordered) action names so that taking action running task.step(i) corresponds to taking action task.class_action_names()[i].","title":"class_action_names"},{"location":"api/core/base_abstractions/task/#close","text":"Task . close ( self ) -> None Closes the environment and any other files opened by the Task (if applicable).","title":"close"},{"location":"api/core/base_abstractions/task/#cumulative_reward","text":"Mean per-agent total cumulative in the task so far. Returns Mean per-agent cumulative reward as a float.","title":"cumulative_reward"},{"location":"api/core/base_abstractions/task/#index_to_action","text":"Task . index_to_action ( self , index : int ) -> str Returns the action name correspond to index .","title":"index_to_action"},{"location":"api/core/base_abstractions/task/#is_done","text":"Task . is_done ( self ) -> bool Did the agent reach a terminal state or performed the maximum number of steps.","title":"is_done"},{"location":"api/core/base_abstractions/task/#metrics","text":"Task . metrics ( self ) -> Dict [ str , Any ] Computes metrics related to the task after the task's completion. By default this function is automatically called during training and the reported metrics logged to tensorboard. Returns A dictionary where every key is a string (the metric's name) and the value is the value of the metric.","title":"metrics"},{"location":"api/core/base_abstractions/task/#num_steps_taken","text":"Task . num_steps_taken ( self ) -> int Number of steps taken by the agent in the task so far.","title":"num_steps_taken"},{"location":"api/core/base_abstractions/task/#query_expert","text":"Task . query_expert ( self , ** kwargs ) -> Tuple [ Any , bool ] Query the expert policy for this task. Returns A tuple (x, y) where x is the expert action (or policy) and y is False if the expert could not determine the optimal action (otherwise True). Here y is used for masking. Even when y is False, x should still lie in the space of possible values (e.g. if x is the expert policy then x should be the correct length, sum to 1, and have non-negative entries).","title":"query_expert"},{"location":"api/core/base_abstractions/task/#reached_max_steps","text":"Task . reached_max_steps ( self ) -> bool Has the agent reached the maximum number of steps.","title":"reached_max_steps"},{"location":"api/core/base_abstractions/task/#reached_terminal_state","text":"Task . reached_terminal_state ( self ) -> bool Has the agent reached a terminal state (excluding reaching the maximum number of steps).","title":"reached_terminal_state"},{"location":"api/core/base_abstractions/task/#render","text":"Task . render ( self , mode : str = 'rgb' , * args , ** kwargs ) -> numpy . ndarray Render the current task state. Rendered task state can come in any supported modes. Parameters mode : The mode in which to render. For example, you might have a 'rgb' mode that renders the agent's egocentric viewpoint or a 'dev' mode returning additional information. args : Extra args. kwargs : Extra kwargs. Returns An numpy array corresponding to the requested render.","title":"render"},{"location":"api/core/base_abstractions/task/#step","text":"Task . step ( self , action : Union [ int , Sequence [ int ]], ) -> core . base_abstractions . misc . RLStepResult Take an action in the environment (one per agent). Takes the action in the environment corresponding to self.class_action_names()[action] for each action if it's a Sequence and returns observations (& rewards and any additional information) corresponding to the agent's new state. Note that this function should not be overwritten without care (instead implement the _step function). Parameters action : The action to take. Returns A RLStepResult object encoding the new observations, reward, and (possibly) additional information.","title":"step"},{"location":"api/core/base_abstractions/task/#total_actions","text":"Total number of actions available to an agent in this Task.","title":"total_actions"},{"location":"api/core/base_abstractions/task/#tasksampler","text":"TaskSampler ( self , / , * args , ** kwargs ) Abstract class defining a how new tasks are sampled.","title":"TaskSampler"},{"location":"api/core/base_abstractions/task/#all_observation_spaces_equal","text":"Checks if all observation spaces of tasks that can be sampled are equal. This will almost always simply return True . A case in which it should return False includes, for example, a setting where you design a TaskSampler that can generate different types of tasks, i.e. point navigation tasks and object navigation tasks. In this case, these different tasks may output different types of observations. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"all_observation_spaces_equal"},{"location":"api/core/base_abstractions/task/#close_1","text":"TaskSampler . close ( self ) -> None Closes any open environments or streams. Should be run when done sampling.","title":"close"},{"location":"api/core/base_abstractions/task/#last_sampled_task","text":"Get the most recently sampled Task. Returns The most recently sampled Task.","title":"last_sampled_task"},{"location":"api/core/base_abstractions/task/#length","text":"Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"length"},{"location":"api/core/base_abstractions/task/#next_task","text":"TaskSampler . next_task ( self , force_advance_scene : bool = False , ) -> Optional [ core . base_abstractions . task . Task ] Get the next task in the sampler's stream. Parameters force_advance_scene : Used to (if applicable) force the task sampler to use a new scene for the next task. This is useful if, during training, you would like to train with one scene for some number of steps and then explicitly control when you begin training with the next scene. Returns The next Task in the sampler's stream if a next task exists. Otherwise None.","title":"next_task"},{"location":"api/core/base_abstractions/task/#reset","text":"TaskSampler . reset ( self ) -> None Resets task sampler to its original state (except for any seed).","title":"reset"},{"location":"api/core/base_abstractions/task/#set_seed","text":"TaskSampler . set_seed ( self , seed : int ) -> None Sets new RNG seed. Parameters seed : New seed.","title":"set_seed"},{"location":"api/core/base_abstractions/task/#total_unique","text":"Total unique tasks. Returns Total number of unique tasks that can be sampled. Can be float('inf') or, if the total unique is not known, None.","title":"total_unique"},{"location":"api/core/models/basic_models/","text":"core.models.basic_models [source] # Basic building block torch networks that can be used across a variety of tasks. RNNStateEncoder # RNNStateEncoder ( self , input_size : int , hidden_size : int , num_layers : int = 1 , rnn_type : str = 'GRU' , trainable_masked_hidden_state : bool = False , ) A simple RNN-based model playing a role in many baseline embodied- navigation agents. See seq_forward for more details of how this model is used. layer_init # RNNStateEncoder . layer_init ( self ) Initialize the RNN parameters in the model. num_recurrent_layers # The number of recurrent layers in the network. seq_forward # RNNStateEncoder . seq_forward ( self , x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor , ) -> Tuple [ torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]]] Forward for a sequence of length T. Parameters x : (Steps, Samplers, Agents, -1) tensor. hidden_states : The starting hidden states. masks : A (Steps, Samplers, Agents) tensor. The masks to be applied to hidden state at every timestep, equal to 0 whenever the previous step finalized the task, 1 elsewhere. single_forward # RNNStateEncoder . single_forward ( self , x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor , ) -> Tuple [ torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]]] Forward for a single-step input. SimpleCNN # SimpleCNN ( self , observation_space : gym . spaces . dict . Dict , output_size : int , layer_channels : Sequence [ int ] = ( 32 , 64 , 32 ), kernel_sizes : Sequence [ Tuple [ int , int ]] = (( 8 , 8 ), ( 4 , 4 ), ( 3 , 3 )), layers_stride : Sequence [ Tuple [ int , int ]] = (( 4 , 4 ), ( 2 , 2 ), ( 1 , 1 )), paddings : Sequence [ Tuple [ int , int ]] = (( 0 , 0 ), ( 0 , 0 ), ( 0 , 0 )), dilations : Sequence [ Tuple [ int , int ]] = (( 1 , 1 ), ( 1 , 1 ), ( 1 , 1 )), rgb_uuid : str = 'rgb' , depth_uuid : str = 'depth' , flatten : bool = True , output_relu : bool = True , ) A Simple N-Conv CNN followed by a fully connected layer. Takes in observations (of type gym.spaces.dict) and produces an embedding of the rgb_uuid and/or depth_uuid components. Attributes observation_space : The observation_space of the agent, should have rgb_uuid or depth_uuid as a component (otherwise it is a blind model). output_size : The size of the embedding vector to produce. is_blind # True if the observation space doesn't include self.rgb_uuid or self.depth_uuid . layer_init # SimpleCNN . layer_init ( cnn ) -> None Initialize layer parameters using Kaiming normal.","title":"basic_models"},{"location":"api/core/models/basic_models/#coremodelsbasic_models-source","text":"Basic building block torch networks that can be used across a variety of tasks.","title":"core.models.basic_models [source]"},{"location":"api/core/models/basic_models/#rnnstateencoder","text":"RNNStateEncoder ( self , input_size : int , hidden_size : int , num_layers : int = 1 , rnn_type : str = 'GRU' , trainable_masked_hidden_state : bool = False , ) A simple RNN-based model playing a role in many baseline embodied- navigation agents. See seq_forward for more details of how this model is used.","title":"RNNStateEncoder"},{"location":"api/core/models/basic_models/#layer_init","text":"RNNStateEncoder . layer_init ( self ) Initialize the RNN parameters in the model.","title":"layer_init"},{"location":"api/core/models/basic_models/#num_recurrent_layers","text":"The number of recurrent layers in the network.","title":"num_recurrent_layers"},{"location":"api/core/models/basic_models/#seq_forward","text":"RNNStateEncoder . seq_forward ( self , x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor , ) -> Tuple [ torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]]] Forward for a sequence of length T. Parameters x : (Steps, Samplers, Agents, -1) tensor. hidden_states : The starting hidden states. masks : A (Steps, Samplers, Agents) tensor. The masks to be applied to hidden state at every timestep, equal to 0 whenever the previous step finalized the task, 1 elsewhere.","title":"seq_forward"},{"location":"api/core/models/basic_models/#single_forward","text":"RNNStateEncoder . single_forward ( self , x : torch . FloatTensor , hidden_states : torch . FloatTensor , masks : torch . FloatTensor , ) -> Tuple [ torch . FloatTensor , Union [ torch . FloatTensor , Tuple [ torch . FloatTensor , ... ]]] Forward for a single-step input.","title":"single_forward"},{"location":"api/core/models/basic_models/#simplecnn","text":"SimpleCNN ( self , observation_space : gym . spaces . dict . Dict , output_size : int , layer_channels : Sequence [ int ] = ( 32 , 64 , 32 ), kernel_sizes : Sequence [ Tuple [ int , int ]] = (( 8 , 8 ), ( 4 , 4 ), ( 3 , 3 )), layers_stride : Sequence [ Tuple [ int , int ]] = (( 4 , 4 ), ( 2 , 2 ), ( 1 , 1 )), paddings : Sequence [ Tuple [ int , int ]] = (( 0 , 0 ), ( 0 , 0 ), ( 0 , 0 )), dilations : Sequence [ Tuple [ int , int ]] = (( 1 , 1 ), ( 1 , 1 ), ( 1 , 1 )), rgb_uuid : str = 'rgb' , depth_uuid : str = 'depth' , flatten : bool = True , output_relu : bool = True , ) A Simple N-Conv CNN followed by a fully connected layer. Takes in observations (of type gym.spaces.dict) and produces an embedding of the rgb_uuid and/or depth_uuid components. Attributes observation_space : The observation_space of the agent, should have rgb_uuid or depth_uuid as a component (otherwise it is a blind model). output_size : The size of the embedding vector to produce.","title":"SimpleCNN"},{"location":"api/core/models/basic_models/#is_blind","text":"True if the observation space doesn't include self.rgb_uuid or self.depth_uuid .","title":"is_blind"},{"location":"api/core/models/basic_models/#layer_init_1","text":"SimpleCNN . layer_init ( cnn ) -> None Initialize layer parameters using Kaiming normal.","title":"layer_init"},{"location":"api/plugins/babyai_plugin/babyai_constants/","text":"plugins.babyai_plugin.babyai_constants [source] #","title":"babyai_constants"},{"location":"api/plugins/babyai_plugin/babyai_constants/#pluginsbabyai_pluginbabyai_constants-source","text":"","title":"plugins.babyai_plugin.babyai_constants [source]"},{"location":"api/plugins/babyai_plugin/babyai_models/","text":"plugins.babyai_plugin.babyai_models [source] # BabyAIACModelWrapped # BabyAIACModelWrapped ( self , obs_space : Dict [ str , int ], action_space : gym . spaces . discrete . Discrete , image_dim = 128 , memory_dim = 128 , instr_dim = 128 , use_instr = False , lang_model = 'gru' , use_memory = False , arch = 'cnn1' , aux_info = None , include_auxiliary_head : bool = False , ) forward_once # BabyAIACModelWrapped . forward_once ( self , obs , memory , instr_embedding = None ) Copied (with minor modifications) from babyai.model.ACModel.forward(...) .","title":"babyai_models"},{"location":"api/plugins/babyai_plugin/babyai_models/#pluginsbabyai_pluginbabyai_models-source","text":"","title":"plugins.babyai_plugin.babyai_models [source]"},{"location":"api/plugins/babyai_plugin/babyai_models/#babyaiacmodelwrapped","text":"BabyAIACModelWrapped ( self , obs_space : Dict [ str , int ], action_space : gym . spaces . discrete . Discrete , image_dim = 128 , memory_dim = 128 , instr_dim = 128 , use_instr = False , lang_model = 'gru' , use_memory = False , arch = 'cnn1' , aux_info = None , include_auxiliary_head : bool = False , )","title":"BabyAIACModelWrapped"},{"location":"api/plugins/babyai_plugin/babyai_models/#forward_once","text":"BabyAIACModelWrapped . forward_once ( self , obs , memory , instr_embedding = None ) Copied (with minor modifications) from babyai.model.ACModel.forward(...) .","title":"forward_once"},{"location":"api/plugins/babyai_plugin/babyai_tasks/","text":"plugins.babyai_plugin.babyai_tasks [source] #","title":"babyai_tasks"},{"location":"api/plugins/babyai_plugin/babyai_tasks/#pluginsbabyai_pluginbabyai_tasks-source","text":"","title":"plugins.babyai_plugin.babyai_tasks [source]"},{"location":"api/plugins/babyai_plugin/scripts/download_babyai_expert_demos/","text":"plugins.babyai_plugin.scripts.download_babyai_expert_demos [source] #","title":"download_babyai_expert_demos"},{"location":"api/plugins/babyai_plugin/scripts/download_babyai_expert_demos/#pluginsbabyai_pluginscriptsdownload_babyai_expert_demos-source","text":"","title":"plugins.babyai_plugin.scripts.download_babyai_expert_demos [source]"},{"location":"api/plugins/babyai_plugin/scripts/get_instr_length_percentiles/","text":"plugins.babyai_plugin.scripts.get_instr_length_percentiles [source] #","title":"get_instr_length_percentiles"},{"location":"api/plugins/babyai_plugin/scripts/get_instr_length_percentiles/#pluginsbabyai_pluginscriptsget_instr_length_percentiles-source","text":"","title":"plugins.babyai_plugin.scripts.get_instr_length_percentiles [source]"},{"location":"api/plugins/babyai_plugin/scripts/truncate_expert_demos/","text":"plugins.babyai_plugin.scripts.truncate_expert_demos [source] #","title":"truncate_expert_demos"},{"location":"api/plugins/babyai_plugin/scripts/truncate_expert_demos/#pluginsbabyai_pluginscriptstruncate_expert_demos-source","text":"","title":"plugins.babyai_plugin.scripts.truncate_expert_demos [source]"},{"location":"api/plugins/habitat_plugin/habitat_constants/","text":"plugins.habitat_plugin.habitat_constants [source] #","title":"habitat_constants"},{"location":"api/plugins/habitat_plugin/habitat_constants/#pluginshabitat_pluginhabitat_constants-source","text":"","title":"plugins.habitat_plugin.habitat_constants [source]"},{"location":"api/plugins/habitat_plugin/habitat_preprocessors/","text":"plugins.habitat_plugin.habitat_preprocessors [source] # ResnetPreProcessorHabitat # ResnetPreProcessorHabitat ( self , input_uuids : List [ str ], output_uuid : str , input_height : int , input_width : int , output_height : int , output_width : int , output_dims : int , pool : bool , torchvision_resnet_model : Callable [ ... , torchvision . models . resnet . ResNet ] = < function resnet18 at 0x1245a9e18 > , parallel : bool = True , device : Optional [ torch . device ] = None , device_ids : Optional [ List [ torch . device ]] = None , kwargs : Any , ) Preprocess RGB or depth image using a ResNet model.","title":"habitat_preprocessors"},{"location":"api/plugins/habitat_plugin/habitat_preprocessors/#pluginshabitat_pluginhabitat_preprocessors-source","text":"","title":"plugins.habitat_plugin.habitat_preprocessors [source]"},{"location":"api/plugins/habitat_plugin/habitat_preprocessors/#resnetpreprocessorhabitat","text":"ResnetPreProcessorHabitat ( self , input_uuids : List [ str ], output_uuid : str , input_height : int , input_width : int , output_height : int , output_width : int , output_dims : int , pool : bool , torchvision_resnet_model : Callable [ ... , torchvision . models . resnet . ResNet ] = < function resnet18 at 0x1245a9e18 > , parallel : bool = True , device : Optional [ torch . device ] = None , device_ids : Optional [ List [ torch . device ]] = None , kwargs : Any , ) Preprocess RGB or depth image using a ResNet model.","title":"ResnetPreProcessorHabitat"},{"location":"api/plugins/ithor_plugin/ithor_constants/","text":"plugins.ithor_plugin.ithor_constants [source] # Common constants used when training agents to complete tasks in iTHOR, the interactive version of AI2-THOR.","title":"ithor_constants"},{"location":"api/plugins/ithor_plugin/ithor_constants/#pluginsithor_pluginithor_constants-source","text":"Common constants used when training agents to complete tasks in iTHOR, the interactive version of AI2-THOR.","title":"plugins.ithor_plugin.ithor_constants [source]"},{"location":"api/plugins/ithor_plugin/ithor_environment/","text":"plugins.ithor_plugin.ithor_environment [source] # A wrapper for engaging with the THOR environment. IThorEnvironment # IThorEnvironment ( self , x_display : Optional [ str ] = None , docker_enabled : bool = False , local_thor_build : Optional [ str ] = None , visibility_distance : float = 1.25 , fov : float = 90.0 , player_screen_width : int = 300 , player_screen_height : int = 300 , quality : str = 'Very Low' , restrict_to_initially_reachable_points : bool = False , make_agents_visible : bool = True , object_open_speed : float = 1.0 , simplify_physics : bool = False , ) -> None Wrapper for the ai2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on AI2-THOR. Attributes controller : The ai2thor controller. all_objects # IThorEnvironment . all_objects ( self ) -> List [ Dict [ str , Any ]] Return all object metadata. all_objects_with_properties # IThorEnvironment . all_objects_with_properties ( self , properties : Dict [ str , Any ], ) -> List [ Dict [ str , Any ]] Find all objects with the given properties. closest_object_of_type # IThorEnvironment . closest_object_of_type ( self , object_type : str , ) -> Optional [ Dict [ str , Any ]] Find the object closest to the agent that has the given type. closest_object_with_properties # IThorEnvironment . closest_object_with_properties ( self , properties : Dict [ str , Any ], ) -> Optional [ Dict [ str , Any ]] Find the object closest to the agent that has the given properties. closest_reachable_point_to_position # IThorEnvironment . closest_reachable_point_to_position ( self , position : Dict [ str , float ], ) -> Tuple [ Dict [ str , float ], float ] Of all reachable positions, find the one that is closest to the given location. closest_visible_object_of_type # IThorEnvironment . closest_visible_object_of_type ( self , object_type : str , ) -> Optional [ Dict [ str , Any ]] Find the object closest to the agent that is visible and has the given type. current_frame # Returns rgb image corresponding to the agent's egocentric view. currently_reachable_points # List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. get_agent_location # IThorEnvironment . get_agent_location ( self ) -> Dict [ str , Union [ float , bool ]] Gets agent's location. GRAPH_ACTIONS_SET # set() -> new empty set object set(iterable) -> new set object Build an unordered collection of unique elements. initially_reachable_points # List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that were reachable after initially resetting. initially_reachable_points_set # Set of (x,z) locations in the scene that were reachable after initially resetting. last_action # Last action, as a string, taken by the agent. last_action_return # Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . last_action_success # Was the last action taken by the agent a success? last_event # Last event returned by the controller. object_in_hand # IThorEnvironment . object_in_hand ( self ) Object metadata for the object in the agent's hand. object_pixels_in_frame # IThorEnvironment . object_pixels_in_frame ( self , object_id : str , hide_all : bool = True , hide_transparent : bool = False , ) -> numpy . ndarray Return an mask for a given object in the agent's current view. Parameters object_id : The id of the object. hide_all : Whether or not to hide all other objects in the scene before getting the mask. hide_transparent : Whether or not partially transparent objects are considered to occlude the object. Returns A numpy array of the mask. object_pixels_on_grid # IThorEnvironment . object_pixels_on_grid ( self , object_id : str , grid_shape : Tuple [ int , int ], hide_all : bool = True , hide_transparent : bool = False , ) -> numpy . ndarray Like object_pixels_in_frame but counts object pixels in a partitioning of the image. position_dist # IThorEnvironment . position_dist ( p0 : Mapping [ str , Any ], p1 : Mapping [ str , Any ], ignore_y : bool = False , ) -> float Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}. random_reachable_state # IThorEnvironment . random_reachable_state ( self , seed : int = None ) -> Dict Returns a random reachable location in the scene. randomize_agent_location # IThorEnvironment . randomize_agent_location ( self , seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None , ) -> Dict Teleports the agent to a random reachable location in the scene. reset # IThorEnvironment . reset ( self , scene_name : Optional [ str ], move_mag : float = 0.25 , kwargs , ) Resets the ai2thor in a new scene. Resets ai2thor into a new scene and initializes the scene/agents with prespecified settings (e.g. move magnitude). Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to the controller \"Initialize\" action. rotation_dist # IThorEnvironment . rotation_dist ( a : Dict [ str , float ], b : Dict [ str , float ]) Distance between rotations. scene_name # Current ai2thor scene. start # IThorEnvironment . start ( self , scene_name : Optional [ str ], move_mag : float = 0.25 , kwargs , ) -> None Starts the ai2thor controller if it was previously stopped. After starting, reset will be called with the scene name and move magnitude. Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to reset. started # Has the ai2thor controller been started. step # IThorEnvironment . step ( self , action_dict : Dict [ str , Union [ str , int , float ]], ) -> ai2thor . server . Event Take a step in the ai2thor environment. stop # IThorEnvironment . stop ( self ) -> None Stops the ai2thor controller. teleport_agent_to # IThorEnvironment . teleport_agent_to ( self , x : float , y : float , z : float , rotation : float , horizon : float , standing : Optional [ bool ] = None , force_action : bool = False , only_initially_reachable : Optional [ bool ] = None , verbose = True , ignore_y_diffs = False , ) -> None Helper function teleporting the agent to a given location. visible_objects # IThorEnvironment . visible_objects ( self ) -> List [ Dict [ str , Any ]] Return all visible objects.","title":"ithor_environment"},{"location":"api/plugins/ithor_plugin/ithor_environment/#pluginsithor_pluginithor_environment-source","text":"A wrapper for engaging with the THOR environment.","title":"plugins.ithor_plugin.ithor_environment [source]"},{"location":"api/plugins/ithor_plugin/ithor_environment/#ithorenvironment","text":"IThorEnvironment ( self , x_display : Optional [ str ] = None , docker_enabled : bool = False , local_thor_build : Optional [ str ] = None , visibility_distance : float = 1.25 , fov : float = 90.0 , player_screen_width : int = 300 , player_screen_height : int = 300 , quality : str = 'Very Low' , restrict_to_initially_reachable_points : bool = False , make_agents_visible : bool = True , object_open_speed : float = 1.0 , simplify_physics : bool = False , ) -> None Wrapper for the ai2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on AI2-THOR. Attributes controller : The ai2thor controller.","title":"IThorEnvironment"},{"location":"api/plugins/ithor_plugin/ithor_environment/#all_objects","text":"IThorEnvironment . all_objects ( self ) -> List [ Dict [ str , Any ]] Return all object metadata.","title":"all_objects"},{"location":"api/plugins/ithor_plugin/ithor_environment/#all_objects_with_properties","text":"IThorEnvironment . all_objects_with_properties ( self , properties : Dict [ str , Any ], ) -> List [ Dict [ str , Any ]] Find all objects with the given properties.","title":"all_objects_with_properties"},{"location":"api/plugins/ithor_plugin/ithor_environment/#closest_object_of_type","text":"IThorEnvironment . closest_object_of_type ( self , object_type : str , ) -> Optional [ Dict [ str , Any ]] Find the object closest to the agent that has the given type.","title":"closest_object_of_type"},{"location":"api/plugins/ithor_plugin/ithor_environment/#closest_object_with_properties","text":"IThorEnvironment . closest_object_with_properties ( self , properties : Dict [ str , Any ], ) -> Optional [ Dict [ str , Any ]] Find the object closest to the agent that has the given properties.","title":"closest_object_with_properties"},{"location":"api/plugins/ithor_plugin/ithor_environment/#closest_reachable_point_to_position","text":"IThorEnvironment . closest_reachable_point_to_position ( self , position : Dict [ str , float ], ) -> Tuple [ Dict [ str , float ], float ] Of all reachable positions, find the one that is closest to the given location.","title":"closest_reachable_point_to_position"},{"location":"api/plugins/ithor_plugin/ithor_environment/#closest_visible_object_of_type","text":"IThorEnvironment . closest_visible_object_of_type ( self , object_type : str , ) -> Optional [ Dict [ str , Any ]] Find the object closest to the agent that is visible and has the given type.","title":"closest_visible_object_of_type"},{"location":"api/plugins/ithor_plugin/ithor_environment/#current_frame","text":"Returns rgb image corresponding to the agent's egocentric view.","title":"current_frame"},{"location":"api/plugins/ithor_plugin/ithor_environment/#currently_reachable_points","text":"List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"currently_reachable_points"},{"location":"api/plugins/ithor_plugin/ithor_environment/#get_agent_location","text":"IThorEnvironment . get_agent_location ( self ) -> Dict [ str , Union [ float , bool ]] Gets agent's location.","title":"get_agent_location"},{"location":"api/plugins/ithor_plugin/ithor_environment/#graph_actions_set","text":"set() -> new empty set object set(iterable) -> new set object Build an unordered collection of unique elements.","title":"GRAPH_ACTIONS_SET"},{"location":"api/plugins/ithor_plugin/ithor_environment/#initially_reachable_points","text":"List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that were reachable after initially resetting.","title":"initially_reachable_points"},{"location":"api/plugins/ithor_plugin/ithor_environment/#initially_reachable_points_set","text":"Set of (x,z) locations in the scene that were reachable after initially resetting.","title":"initially_reachable_points_set"},{"location":"api/plugins/ithor_plugin/ithor_environment/#last_action","text":"Last action, as a string, taken by the agent.","title":"last_action"},{"location":"api/plugins/ithor_plugin/ithor_environment/#last_action_return","text":"Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"last_action_return"},{"location":"api/plugins/ithor_plugin/ithor_environment/#last_action_success","text":"Was the last action taken by the agent a success?","title":"last_action_success"},{"location":"api/plugins/ithor_plugin/ithor_environment/#last_event","text":"Last event returned by the controller.","title":"last_event"},{"location":"api/plugins/ithor_plugin/ithor_environment/#object_in_hand","text":"IThorEnvironment . object_in_hand ( self ) Object metadata for the object in the agent's hand.","title":"object_in_hand"},{"location":"api/plugins/ithor_plugin/ithor_environment/#object_pixels_in_frame","text":"IThorEnvironment . object_pixels_in_frame ( self , object_id : str , hide_all : bool = True , hide_transparent : bool = False , ) -> numpy . ndarray Return an mask for a given object in the agent's current view. Parameters object_id : The id of the object. hide_all : Whether or not to hide all other objects in the scene before getting the mask. hide_transparent : Whether or not partially transparent objects are considered to occlude the object. Returns A numpy array of the mask.","title":"object_pixels_in_frame"},{"location":"api/plugins/ithor_plugin/ithor_environment/#object_pixels_on_grid","text":"IThorEnvironment . object_pixels_on_grid ( self , object_id : str , grid_shape : Tuple [ int , int ], hide_all : bool = True , hide_transparent : bool = False , ) -> numpy . ndarray Like object_pixels_in_frame but counts object pixels in a partitioning of the image.","title":"object_pixels_on_grid"},{"location":"api/plugins/ithor_plugin/ithor_environment/#position_dist","text":"IThorEnvironment . position_dist ( p0 : Mapping [ str , Any ], p1 : Mapping [ str , Any ], ignore_y : bool = False , ) -> float Distance between two points of the form {\"x\": x, \"y\":y, \"z\":z\"}.","title":"position_dist"},{"location":"api/plugins/ithor_plugin/ithor_environment/#random_reachable_state","text":"IThorEnvironment . random_reachable_state ( self , seed : int = None ) -> Dict Returns a random reachable location in the scene.","title":"random_reachable_state"},{"location":"api/plugins/ithor_plugin/ithor_environment/#randomize_agent_location","text":"IThorEnvironment . randomize_agent_location ( self , seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None , ) -> Dict Teleports the agent to a random reachable location in the scene.","title":"randomize_agent_location"},{"location":"api/plugins/ithor_plugin/ithor_environment/#reset","text":"IThorEnvironment . reset ( self , scene_name : Optional [ str ], move_mag : float = 0.25 , kwargs , ) Resets the ai2thor in a new scene. Resets ai2thor into a new scene and initializes the scene/agents with prespecified settings (e.g. move magnitude). Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to the controller \"Initialize\" action.","title":"reset"},{"location":"api/plugins/ithor_plugin/ithor_environment/#rotation_dist","text":"IThorEnvironment . rotation_dist ( a : Dict [ str , float ], b : Dict [ str , float ]) Distance between rotations.","title":"rotation_dist"},{"location":"api/plugins/ithor_plugin/ithor_environment/#scene_name","text":"Current ai2thor scene.","title":"scene_name"},{"location":"api/plugins/ithor_plugin/ithor_environment/#start","text":"IThorEnvironment . start ( self , scene_name : Optional [ str ], move_mag : float = 0.25 , kwargs , ) -> None Starts the ai2thor controller if it was previously stopped. After starting, reset will be called with the scene name and move magnitude. Parameters scene_name : The scene to load. move_mag : The amount of distance the agent moves in a single MoveAhead step. kwargs : additional kwargs, passed to reset.","title":"start"},{"location":"api/plugins/ithor_plugin/ithor_environment/#started","text":"Has the ai2thor controller been started.","title":"started"},{"location":"api/plugins/ithor_plugin/ithor_environment/#step","text":"IThorEnvironment . step ( self , action_dict : Dict [ str , Union [ str , int , float ]], ) -> ai2thor . server . Event Take a step in the ai2thor environment.","title":"step"},{"location":"api/plugins/ithor_plugin/ithor_environment/#stop","text":"IThorEnvironment . stop ( self ) -> None Stops the ai2thor controller.","title":"stop"},{"location":"api/plugins/ithor_plugin/ithor_environment/#teleport_agent_to","text":"IThorEnvironment . teleport_agent_to ( self , x : float , y : float , z : float , rotation : float , horizon : float , standing : Optional [ bool ] = None , force_action : bool = False , only_initially_reachable : Optional [ bool ] = None , verbose = True , ignore_y_diffs = False , ) -> None Helper function teleporting the agent to a given location.","title":"teleport_agent_to"},{"location":"api/plugins/ithor_plugin/ithor_environment/#visible_objects","text":"IThorEnvironment . visible_objects ( self ) -> List [ Dict [ str , Any ]] Return all visible objects.","title":"visible_objects"},{"location":"api/plugins/ithor_plugin/ithor_sensors/","text":"plugins.ithor_plugin.ithor_sensors [source] # RGBSensorThor # RGBSensorThor ( self , use_resnet_normalization : bool = False , mean : Optional [ numpy . ndarray ] = [[[ 0.485 0.456 0.406 ]]], stdev : Optional [ numpy . ndarray ] = [[[ 0.229 0.224 0.225 ]]], height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'rgb' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , kwargs : Any , ) Sensor for RGB images in iTHOR. Returns from a running IThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"ithor_sensors"},{"location":"api/plugins/ithor_plugin/ithor_sensors/#pluginsithor_pluginithor_sensors-source","text":"","title":"plugins.ithor_plugin.ithor_sensors [source]"},{"location":"api/plugins/ithor_plugin/ithor_sensors/#rgbsensorthor","text":"RGBSensorThor ( self , use_resnet_normalization : bool = False , mean : Optional [ numpy . ndarray ] = [[[ 0.485 0.456 0.406 ]]], stdev : Optional [ numpy . ndarray ] = [[[ 0.229 0.224 0.225 ]]], height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'rgb' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , kwargs : Any , ) Sensor for RGB images in iTHOR. Returns from a running IThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorThor"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/","text":"plugins.ithor_plugin.ithor_task_samplers [source] # ObjectNavTaskSampler # ObjectNavTaskSampler ( self , scenes : List [ str ], object_types : str , sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , scene_period : Optional [ int , str ] = None , max_tasks : Optional [ int ] = None , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , fixed_tasks : Optional [ List [ Dict [ str , Any ]]] = None , args , kwargs , ) -> None all_observation_spaces_equal # Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. length # Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"ithor_task_samplers"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#pluginsithor_pluginithor_task_samplers-source","text":"","title":"plugins.ithor_plugin.ithor_task_samplers [source]"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#objectnavtasksampler","text":"ObjectNavTaskSampler ( self , scenes : List [ str ], object_types : str , sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , scene_period : Optional [ int , str ] = None , max_tasks : Optional [ int ] = None , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , fixed_tasks : Optional [ List [ Dict [ str , Any ]]] = None , args , kwargs , ) -> None","title":"ObjectNavTaskSampler"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#all_observation_spaces_equal","text":"Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"all_observation_spaces_equal"},{"location":"api/plugins/ithor_plugin/ithor_task_samplers/#length","text":"Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"length"},{"location":"api/plugins/ithor_plugin/ithor_tasks/","text":"plugins.ithor_plugin.ithor_tasks [source] # ObjectNavTask # ObjectNavTask ( self , env : plugins . ithor_plugin . ithor_environment . IThorEnvironment , sensors : List [ core . base_abstractions . sensor . Sensor ], task_info : Dict [ str , Any ], max_steps : int , kwargs , ) -> None Defines the object navigation task in AI2-THOR. In object navigation an agent is randomly initialized into an AI2-THOR scene and must find an object of a given type (e.g. tomato, television, etc). An object is considered found if the agent takes an End action and the object is visible to the agent (see here for a definition of visibiliy in AI2-THOR). The actions available to an agent in this task are: Move ahead Moves agent ahead by 0.25 meters. Rotate left / rotate right Rotates the agent by 90 degrees counter-clockwise / clockwise. Look down / look up Changes agent view angle by 30 degrees up or down. An agent cannot look more than 30 degrees above horizontal or less than 60 degrees below horizontal. End Ends the task and the agent receives a positive reward if the object type is visible to the agent, otherwise it receives a negative reward. Attributes env : The ai2thor environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : The task info. Must contain a field \"object_type\" that specifies, as a string, the goal object type. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors. judge # ObjectNavTask . judge ( self ) -> float Compute the reward after having taken a step.","title":"ithor_tasks"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#pluginsithor_pluginithor_tasks-source","text":"","title":"plugins.ithor_plugin.ithor_tasks [source]"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#objectnavtask","text":"ObjectNavTask ( self , env : plugins . ithor_plugin . ithor_environment . IThorEnvironment , sensors : List [ core . base_abstractions . sensor . Sensor ], task_info : Dict [ str , Any ], max_steps : int , kwargs , ) -> None Defines the object navigation task in AI2-THOR. In object navigation an agent is randomly initialized into an AI2-THOR scene and must find an object of a given type (e.g. tomato, television, etc). An object is considered found if the agent takes an End action and the object is visible to the agent (see here for a definition of visibiliy in AI2-THOR). The actions available to an agent in this task are: Move ahead Moves agent ahead by 0.25 meters. Rotate left / rotate right Rotates the agent by 90 degrees counter-clockwise / clockwise. Look down / look up Changes agent view angle by 30 degrees up or down. An agent cannot look more than 30 degrees above horizontal or less than 60 degrees below horizontal. End Ends the task and the agent receives a positive reward if the object type is visible to the agent, otherwise it receives a negative reward. Attributes env : The ai2thor environment. sensor_suite : Collection of sensors formed from the sensors argument in the initializer. task_info : The task info. Must contain a field \"object_type\" that specifies, as a string, the goal object type. max_steps : The maximum number of steps an agent can take an in the task before it is considered failed. observation_space : The observation space returned on each step from the sensors.","title":"ObjectNavTask"},{"location":"api/plugins/ithor_plugin/ithor_tasks/#judge","text":"ObjectNavTask . judge ( self ) -> float Compute the reward after having taken a step.","title":"judge"},{"location":"api/plugins/ithor_plugin/ithor_util/","text":"plugins.ithor_plugin.ithor_util [source] # round_to_factor # round_to_factor ( num : float , base : int ) -> int Rounds floating point number to the nearest integer multiple of the given base. E.g., for floating number 90.1 and integer base 45, the result is 90. Attributes num : floating point number to be rounded. base : integer base","title":"ithor_util"},{"location":"api/plugins/ithor_plugin/ithor_util/#pluginsithor_pluginithor_util-source","text":"","title":"plugins.ithor_plugin.ithor_util [source]"},{"location":"api/plugins/ithor_plugin/ithor_util/#round_to_factor","text":"round_to_factor ( num : float , base : int ) -> int Rounds floating point number to the nearest integer multiple of the given base. E.g., for floating number 90.1 and integer base 45, the result is 90. Attributes num : floating point number to be rounded. base : integer base","title":"round_to_factor"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/","text":"plugins.lighthouse_plugin.lighthouse_environment [source] # LightHouseEnvironment # LightHouseEnvironment ( self , world_dim : int , world_radius : int , ** kwargs ) EMPTY # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 GOAL # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SPACE_LEVELS # list() -> new empty list list(iterable) -> new list initialized from iterable's items WALL # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 WRONG_CORNER # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"lighthouse_environment"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#pluginslighthouse_pluginlighthouse_environment-source","text":"","title":"plugins.lighthouse_plugin.lighthouse_environment [source]"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#lighthouseenvironment","text":"LightHouseEnvironment ( self , world_dim : int , world_radius : int , ** kwargs )","title":"LightHouseEnvironment"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#empty","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"EMPTY"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#goal","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"GOAL"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#space_levels","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"SPACE_LEVELS"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#wall","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"WALL"},{"location":"api/plugins/lighthouse_plugin/lighthouse_environment/#wrong_corner","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"WRONG_CORNER"},{"location":"api/plugins/lighthouse_plugin/lighthouse_models/","text":"plugins.lighthouse_plugin.lighthouse_models [source] #","title":"lighthouse_models"},{"location":"api/plugins/lighthouse_plugin/lighthouse_models/#pluginslighthouse_pluginlighthouse_models-source","text":"","title":"plugins.lighthouse_plugin.lighthouse_models [source]"},{"location":"api/plugins/lighthouse_plugin/lighthouse_util/","text":"plugins.lighthouse_plugin.lighthouse_util [source] #","title":"lighthouse_util"},{"location":"api/plugins/lighthouse_plugin/lighthouse_util/#pluginslighthouse_pluginlighthouse_util-source","text":"","title":"plugins.lighthouse_plugin.lighthouse_util [source]"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/","text":"plugins.minigrid_plugin.minigrid_environments [source] # AskForHelpSimpleCrossing # AskForHelpSimpleCrossing ( self , size = 9 , num_crossings = 1 , obstacle_type = < class ' gym_minigrid . minigrid . Wall '>, seed = None , exploration_reward : Optional [ float ] = None , death_penalty : Optional [ float ] = None , toggle_is_permenant : bool = False , ) Corresponds to WC FAULTY SWITCH environment. step # AskForHelpSimpleCrossing . step ( self , action : int ) Reveal the observation only if the toggle action is executed. FastCrossing # FastCrossing ( self , size = 9 , num_crossings = 1 , obstacle_type = < class ' gym_minigrid . minigrid . Lava '>, seed = None , ) Similar to CrossingEnv , but to support faster task sampling as per repeat_failed_task_for_min_steps flag in MiniGridTaskSampler.","title":"minigrid_environments"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#pluginsminigrid_pluginminigrid_environments-source","text":"","title":"plugins.minigrid_plugin.minigrid_environments [source]"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#askforhelpsimplecrossing","text":"AskForHelpSimpleCrossing ( self , size = 9 , num_crossings = 1 , obstacle_type = < class ' gym_minigrid . minigrid . Wall '>, seed = None , exploration_reward : Optional [ float ] = None , death_penalty : Optional [ float ] = None , toggle_is_permenant : bool = False , ) Corresponds to WC FAULTY SWITCH environment.","title":"AskForHelpSimpleCrossing"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#step","text":"AskForHelpSimpleCrossing . step ( self , action : int ) Reveal the observation only if the toggle action is executed.","title":"step"},{"location":"api/plugins/minigrid_plugin/minigrid_environments/#fastcrossing","text":"FastCrossing ( self , size = 9 , num_crossings = 1 , obstacle_type = < class ' gym_minigrid . minigrid . Lava '>, seed = None , ) Similar to CrossingEnv , but to support faster task sampling as per repeat_failed_task_for_min_steps flag in MiniGridTaskSampler.","title":"FastCrossing"},{"location":"api/plugins/minigrid_plugin/minigrid_models/","text":"plugins.minigrid_plugin.minigrid_models [source] #","title":"minigrid_models"},{"location":"api/plugins/minigrid_plugin/minigrid_models/#pluginsminigrid_pluginminigrid_models-source","text":"","title":"plugins.minigrid_plugin.minigrid_models [source]"},{"location":"api/plugins/minigrid_plugin/minigrid_offpolicy/","text":"plugins.minigrid_plugin.minigrid_offpolicy [source] #","title":"minigrid_offpolicy"},{"location":"api/plugins/minigrid_plugin/minigrid_offpolicy/#pluginsminigrid_pluginminigrid_offpolicy-source","text":"","title":"plugins.minigrid_plugin.minigrid_offpolicy [source]"},{"location":"api/plugins/minigrid_plugin/minigrid_sensors/","text":"plugins.minigrid_plugin.minigrid_sensors [source] #","title":"minigrid_sensors"},{"location":"api/plugins/minigrid_plugin/minigrid_sensors/#pluginsminigrid_pluginminigrid_sensors-source","text":"","title":"plugins.minigrid_plugin.minigrid_sensors [source]"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/","text":"plugins.minigrid_plugin.minigrid_tasks [source] # MiniGridTask # MiniGridTask ( self , env : gym_minigrid . envs . crossing . CrossingEnv , sensors : Union [ core . base_abstractions . sensor . SensorSuite , List [ core . base_abstractions . sensor . Sensor ]], task_info : Dict [ str , Any ], max_steps : int , task_cache_uid : Optional [ str ] = None , corrupt_expert_within_actions_of_goal : Optional [ int ] = None , kwargs , ) generate_graph # MiniGridTask . generate_graph ( self ) -> networkx . classes . digraph . DiGraph The generated graph is based on the fully observable grid (as the expert sees it all). env: environment to generate the graph over","title":"minigrid_tasks"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/#pluginsminigrid_pluginminigrid_tasks-source","text":"","title":"plugins.minigrid_plugin.minigrid_tasks [source]"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/#minigridtask","text":"MiniGridTask ( self , env : gym_minigrid . envs . crossing . CrossingEnv , sensors : Union [ core . base_abstractions . sensor . SensorSuite , List [ core . base_abstractions . sensor . Sensor ]], task_info : Dict [ str , Any ], max_steps : int , task_cache_uid : Optional [ str ] = None , corrupt_expert_within_actions_of_goal : Optional [ int ] = None , kwargs , )","title":"MiniGridTask"},{"location":"api/plugins/minigrid_plugin/minigrid_tasks/#generate_graph","text":"MiniGridTask . generate_graph ( self ) -> networkx . classes . digraph . DiGraph The generated graph is based on the fully observable grid (as the expert sees it all). env: environment to generate the graph over","title":"generate_graph"},{"location":"api/plugins/minigrid_plugin/configs/minigrid_nomemory/","text":"plugins.minigrid_plugin.configs.minigrid_nomemory [source] # Experiment Config for MiniGrid tutorial.","title":"minigrid_nomemory"},{"location":"api/plugins/minigrid_plugin/configs/minigrid_nomemory/#pluginsminigrid_pluginconfigsminigrid_nomemory-source","text":"Experiment Config for MiniGrid tutorial.","title":"plugins.minigrid_plugin.configs.minigrid_nomemory [source]"},{"location":"api/plugins/robothor_plugin/robothor_constants/","text":"plugins.robothor_plugin.robothor_constants [source] #","title":"robothor_constants"},{"location":"api/plugins/robothor_plugin/robothor_constants/#pluginsrobothor_pluginrobothor_constants-source","text":"","title":"plugins.robothor_plugin.robothor_constants [source]"},{"location":"api/plugins/robothor_plugin/robothor_environment/","text":"plugins.robothor_plugin.robothor_environment [source] # RoboThorCachedEnvironment # RoboThorCachedEnvironment ( self , ** kwargs ) Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration agent_state # RoboThorCachedEnvironment . agent_state ( self , ) -> Dict [ str , Union [ Dict [ str , float ], float ]] Return agent position, rotation and horizon. all_objects # RoboThorCachedEnvironment . all_objects ( self ) -> List [ Dict [ str , Any ]] Return all object metadata. all_objects_with_properties # RoboThorCachedEnvironment . all_objects_with_properties ( self , properties : Dict [ str , Any ], ) -> List [ Dict [ str , Any ]] Find all objects with the given properties. current_depth # Returns depth image corresponding to the agent's egocentric view. current_frame # Returns rgb image corresponding to the agent's egocentric view. currently_reachable_points # List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. last_action # Last action, as a string, taken by the agent. last_action_return # Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . last_action_success # In the cached environment, all actions succeed. last_event # Last event returned by the controller. reset # RoboThorCachedEnvironment . reset ( self , scene_name : str = None ) -> None Resets scene to a known initial state. scene_name # Current ai2thor scene. step # RoboThorCachedEnvironment . step ( self , action_dict : Dict [ str , Union [ str , int , float ]], ) -> ai2thor . server . Event Take a step in the ai2thor environment. stop # RoboThorCachedEnvironment . stop ( self ) Stops the ai2thor controller. visible_objects # RoboThorCachedEnvironment . visible_objects ( self ) -> List [ Dict [ str , Any ]] Return all visible objects. RoboThorEnvironment # RoboThorEnvironment ( self , ** kwargs ) Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration access_grid # RoboThorEnvironment . access_grid ( self , target : str ) -> float Returns the geodesic distance from the quantized location of the agent in the current scene's grid to the target object of given type. agent_state # RoboThorEnvironment . agent_state ( self ) -> Dict Return agent position, rotation and horizon. all_objects # RoboThorEnvironment . all_objects ( self ) -> List [ Dict [ str , Any ]] Return all object metadata. all_objects_with_properties # RoboThorEnvironment . all_objects_with_properties ( self , properties : Dict [ str , Any ], ) -> List [ Dict [ str , Any ]] Find all objects with the given properties. current_depth # Returns depth image corresponding to the agent's egocentric view. current_frame # Returns rgb image corresponding to the agent's egocentric view. currently_reachable_points # List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable. dist_to_object # RoboThorEnvironment . dist_to_object ( self , object_type : str ) -> float Minimal geodesic distance to object of given type from agent's current location. It might return -1.0 for unreachable targets. dist_to_point # RoboThorEnvironment . dist_to_point ( self , xyz : Dict [ str , float ]) -> float Minimal geodesic distance to end point from agent's current location. It might return -1.0 for unreachable targets. initialize_grid # RoboThorEnvironment . initialize_grid ( self ) -> None Initializes grid for current scene if not already initialized. initialize_grid_dimensions # RoboThorEnvironment . initialize_grid_dimensions ( self , reachable_points : Collection [ Dict [ str , float ]], ) -> Tuple [ int , int , int , int ] Computes bounding box for reachable points quantized with the current gridSize. last_action # Last action, as a string, taken by the agent. last_action_return # Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" . last_action_success # Was the last action taken by the agent a success? last_event # Last event returned by the controller. object_reachable # RoboThorEnvironment . object_reachable ( self , object_type : str ) -> bool Determines whether a path can be computed from the discretized current agent location to the target object of given type. path_corners # RoboThorEnvironment . path_corners ( self , target : Union [ str , Dict [ str , float ]], ) -> List [ Dict [ str , float ]] Returns an array with a sequence of xyz dictionaries objects representing the corners of the shortest path to the object of given type or end point location. path_corners_to_dist # RoboThorEnvironment . path_corners_to_dist ( self , corners : Sequence [ Dict [ str , float ]], ) -> float Computes the distance covered by the given path described by its corners. point_reachable # RoboThorEnvironment . point_reachable ( self , xyz : Dict [ str , float ]) -> bool Determines whether a path can be computed from the current agent location to the target point. quantized_agent_state # RoboThorEnvironment . quantized_agent_state ( self , xz_subsampling : int = 1 , rot_subsampling : int = 1 , ) -> Tuple [ int , int , int ] Quantizes agent location (x, z) to a (subsampled) position in a fixed size grid derived from the initial set of reachable points; and rotation (around y axis) as a (subsampled) discretized angle given the current rotateStepDegrees . random_reachable_state # RoboThorEnvironment . random_reachable_state ( self , seed : Optional [ int ] = None , ) -> Dict [ str , Union [ Dict [ str , float ], float ]] Returns a random reachable location in the scene. randomize_agent_location # RoboThorEnvironment . randomize_agent_location ( self , seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None , ) -> Dict [ str , Union [ Dict [ str , float ], float ]] Teleports the agent to a random reachable location in the scene. reset # RoboThorEnvironment . reset ( self , scene_name : str = None ) -> None Resets scene to a known initial state. scene_name # Current ai2thor scene. step # RoboThorEnvironment . step ( self , action_dict : Dict ) -> ai2thor . server . Event Take a step in the ai2thor environment. stop # RoboThorEnvironment . stop ( self ) Stops the ai2thor controller. visible_objects # RoboThorEnvironment . visible_objects ( self ) -> List [ Dict [ str , Any ]] Return all visible objects.","title":"robothor_environment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#pluginsrobothor_pluginrobothor_environment-source","text":"","title":"plugins.robothor_plugin.robothor_environment [source]"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorcachedenvironment","text":"RoboThorCachedEnvironment ( self , ** kwargs ) Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration","title":"RoboThorCachedEnvironment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#agent_state","text":"RoboThorCachedEnvironment . agent_state ( self , ) -> Dict [ str , Union [ Dict [ str , float ], float ]] Return agent position, rotation and horizon.","title":"agent_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#all_objects","text":"RoboThorCachedEnvironment . all_objects ( self ) -> List [ Dict [ str , Any ]] Return all object metadata.","title":"all_objects"},{"location":"api/plugins/robothor_plugin/robothor_environment/#all_objects_with_properties","text":"RoboThorCachedEnvironment . all_objects_with_properties ( self , properties : Dict [ str , Any ], ) -> List [ Dict [ str , Any ]] Find all objects with the given properties.","title":"all_objects_with_properties"},{"location":"api/plugins/robothor_plugin/robothor_environment/#current_depth","text":"Returns depth image corresponding to the agent's egocentric view.","title":"current_depth"},{"location":"api/plugins/robothor_plugin/robothor_environment/#current_frame","text":"Returns rgb image corresponding to the agent's egocentric view.","title":"current_frame"},{"location":"api/plugins/robothor_plugin/robothor_environment/#currently_reachable_points","text":"List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"currently_reachable_points"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_action","text":"Last action, as a string, taken by the agent.","title":"last_action"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_action_return","text":"Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"last_action_return"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_action_success","text":"In the cached environment, all actions succeed.","title":"last_action_success"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_event","text":"Last event returned by the controller.","title":"last_event"},{"location":"api/plugins/robothor_plugin/robothor_environment/#reset","text":"RoboThorCachedEnvironment . reset ( self , scene_name : str = None ) -> None Resets scene to a known initial state.","title":"reset"},{"location":"api/plugins/robothor_plugin/robothor_environment/#scene_name","text":"Current ai2thor scene.","title":"scene_name"},{"location":"api/plugins/robothor_plugin/robothor_environment/#step","text":"RoboThorCachedEnvironment . step ( self , action_dict : Dict [ str , Union [ str , int , float ]], ) -> ai2thor . server . Event Take a step in the ai2thor environment.","title":"step"},{"location":"api/plugins/robothor_plugin/robothor_environment/#stop","text":"RoboThorCachedEnvironment . stop ( self ) Stops the ai2thor controller.","title":"stop"},{"location":"api/plugins/robothor_plugin/robothor_environment/#visible_objects","text":"RoboThorCachedEnvironment . visible_objects ( self ) -> List [ Dict [ str , Any ]] Return all visible objects.","title":"visible_objects"},{"location":"api/plugins/robothor_plugin/robothor_environment/#robothorenvironment","text":"RoboThorEnvironment ( self , ** kwargs ) Wrapper for the robo2thor controller providing additional functionality and bookkeeping. See here for comprehensive documentation on RoboTHOR. Attributes controller : The AI2THOR controller. config : The AI2THOR controller configuration","title":"RoboThorEnvironment"},{"location":"api/plugins/robothor_plugin/robothor_environment/#access_grid","text":"RoboThorEnvironment . access_grid ( self , target : str ) -> float Returns the geodesic distance from the quantized location of the agent in the current scene's grid to the target object of given type.","title":"access_grid"},{"location":"api/plugins/robothor_plugin/robothor_environment/#agent_state_1","text":"RoboThorEnvironment . agent_state ( self ) -> Dict Return agent position, rotation and horizon.","title":"agent_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#all_objects_1","text":"RoboThorEnvironment . all_objects ( self ) -> List [ Dict [ str , Any ]] Return all object metadata.","title":"all_objects"},{"location":"api/plugins/robothor_plugin/robothor_environment/#all_objects_with_properties_1","text":"RoboThorEnvironment . all_objects_with_properties ( self , properties : Dict [ str , Any ], ) -> List [ Dict [ str , Any ]] Find all objects with the given properties.","title":"all_objects_with_properties"},{"location":"api/plugins/robothor_plugin/robothor_environment/#current_depth_1","text":"Returns depth image corresponding to the agent's egocentric view.","title":"current_depth"},{"location":"api/plugins/robothor_plugin/robothor_environment/#current_frame_1","text":"Returns rgb image corresponding to the agent's egocentric view.","title":"current_frame"},{"location":"api/plugins/robothor_plugin/robothor_environment/#currently_reachable_points_1","text":"List of {\"x\": x, \"y\": y, \"z\": z} locations in the scene that are currently reachable.","title":"currently_reachable_points"},{"location":"api/plugins/robothor_plugin/robothor_environment/#dist_to_object","text":"RoboThorEnvironment . dist_to_object ( self , object_type : str ) -> float Minimal geodesic distance to object of given type from agent's current location. It might return -1.0 for unreachable targets.","title":"dist_to_object"},{"location":"api/plugins/robothor_plugin/robothor_environment/#dist_to_point","text":"RoboThorEnvironment . dist_to_point ( self , xyz : Dict [ str , float ]) -> float Minimal geodesic distance to end point from agent's current location. It might return -1.0 for unreachable targets.","title":"dist_to_point"},{"location":"api/plugins/robothor_plugin/robothor_environment/#initialize_grid","text":"RoboThorEnvironment . initialize_grid ( self ) -> None Initializes grid for current scene if not already initialized.","title":"initialize_grid"},{"location":"api/plugins/robothor_plugin/robothor_environment/#initialize_grid_dimensions","text":"RoboThorEnvironment . initialize_grid_dimensions ( self , reachable_points : Collection [ Dict [ str , float ]], ) -> Tuple [ int , int , int , int ] Computes bounding box for reachable points quantized with the current gridSize.","title":"initialize_grid_dimensions"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_action_1","text":"Last action, as a string, taken by the agent.","title":"last_action"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_action_return_1","text":"Get the value returned by the last action (if applicable). For an example of an action that returns a value, see \"GetReachablePositions\" .","title":"last_action_return"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_action_success_1","text":"Was the last action taken by the agent a success?","title":"last_action_success"},{"location":"api/plugins/robothor_plugin/robothor_environment/#last_event_1","text":"Last event returned by the controller.","title":"last_event"},{"location":"api/plugins/robothor_plugin/robothor_environment/#object_reachable","text":"RoboThorEnvironment . object_reachable ( self , object_type : str ) -> bool Determines whether a path can be computed from the discretized current agent location to the target object of given type.","title":"object_reachable"},{"location":"api/plugins/robothor_plugin/robothor_environment/#path_corners","text":"RoboThorEnvironment . path_corners ( self , target : Union [ str , Dict [ str , float ]], ) -> List [ Dict [ str , float ]] Returns an array with a sequence of xyz dictionaries objects representing the corners of the shortest path to the object of given type or end point location.","title":"path_corners"},{"location":"api/plugins/robothor_plugin/robothor_environment/#path_corners_to_dist","text":"RoboThorEnvironment . path_corners_to_dist ( self , corners : Sequence [ Dict [ str , float ]], ) -> float Computes the distance covered by the given path described by its corners.","title":"path_corners_to_dist"},{"location":"api/plugins/robothor_plugin/robothor_environment/#point_reachable","text":"RoboThorEnvironment . point_reachable ( self , xyz : Dict [ str , float ]) -> bool Determines whether a path can be computed from the current agent location to the target point.","title":"point_reachable"},{"location":"api/plugins/robothor_plugin/robothor_environment/#quantized_agent_state","text":"RoboThorEnvironment . quantized_agent_state ( self , xz_subsampling : int = 1 , rot_subsampling : int = 1 , ) -> Tuple [ int , int , int ] Quantizes agent location (x, z) to a (subsampled) position in a fixed size grid derived from the initial set of reachable points; and rotation (around y axis) as a (subsampled) discretized angle given the current rotateStepDegrees .","title":"quantized_agent_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#random_reachable_state","text":"RoboThorEnvironment . random_reachable_state ( self , seed : Optional [ int ] = None , ) -> Dict [ str , Union [ Dict [ str , float ], float ]] Returns a random reachable location in the scene.","title":"random_reachable_state"},{"location":"api/plugins/robothor_plugin/robothor_environment/#randomize_agent_location","text":"RoboThorEnvironment . randomize_agent_location ( self , seed : int = None , partial_position : Optional [ Dict [ str , float ]] = None , ) -> Dict [ str , Union [ Dict [ str , float ], float ]] Teleports the agent to a random reachable location in the scene.","title":"randomize_agent_location"},{"location":"api/plugins/robothor_plugin/robothor_environment/#reset_1","text":"RoboThorEnvironment . reset ( self , scene_name : str = None ) -> None Resets scene to a known initial state.","title":"reset"},{"location":"api/plugins/robothor_plugin/robothor_environment/#scene_name_1","text":"Current ai2thor scene.","title":"scene_name"},{"location":"api/plugins/robothor_plugin/robothor_environment/#step_1","text":"RoboThorEnvironment . step ( self , action_dict : Dict ) -> ai2thor . server . Event Take a step in the ai2thor environment.","title":"step"},{"location":"api/plugins/robothor_plugin/robothor_environment/#stop_1","text":"RoboThorEnvironment . stop ( self ) Stops the ai2thor controller.","title":"stop"},{"location":"api/plugins/robothor_plugin/robothor_environment/#visible_objects_1","text":"RoboThorEnvironment . visible_objects ( self ) -> List [ Dict [ str , Any ]] Return all visible objects.","title":"visible_objects"},{"location":"api/plugins/robothor_plugin/robothor_models/","text":"plugins.robothor_plugin.robothor_models [source] # ResnetFasterRCNNTensorsGoalEncoder # ResnetFasterRCNNTensorsGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , detector_preprocessor_uuid : str , class_dims : int = 32 , max_dets : int = 3 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), box_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), class_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None get_object_type_encoding # ResnetFasterRCNNTensorsGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. ResnetFasterRCNNTensorsObjectNavActorCritic # ResnetFasterRCNNTensorsObjectNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , detector_preprocessor_uuid : str , rnn_hidden_size = 512 , goal_dims : int = 32 , max_dets : int = 3 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), box_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), class_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) get_object_type_encoding # ResnetFasterRCNNTensorsObjectNavActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. is_blind # True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). num_recurrent_layers # Number of recurrent hidden layers. recurrent_hidden_state_size # The recurrent hidden state size of the model. ResnetTensorGoalEncoder # ResnetTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , class_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None get_object_type_encoding # ResnetTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. ResnetTensorObjectNavActorCritic # ResnetTensorObjectNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , rnn_hidden_size : int = 512 , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) get_object_type_encoding # ResnetTensorObjectNavActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. is_blind # True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). num_recurrent_layers # Number of recurrent hidden layers. recurrent_hidden_state_size # The recurrent hidden state size of the model.","title":"robothor_models"},{"location":"api/plugins/robothor_plugin/robothor_models/#pluginsrobothor_pluginrobothor_models-source","text":"","title":"plugins.robothor_plugin.robothor_models [source]"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsgoalencoder","text":"ResnetFasterRCNNTensorsGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , detector_preprocessor_uuid : str , class_dims : int = 32 , max_dets : int = 3 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), box_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), class_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None","title":"ResnetFasterRCNNTensorsGoalEncoder"},{"location":"api/plugins/robothor_plugin/robothor_models/#get_object_type_encoding","text":"ResnetFasterRCNNTensorsGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnetfasterrcnntensorsobjectnavactorcritic","text":"ResnetFasterRCNNTensorsObjectNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , detector_preprocessor_uuid : str , rnn_hidden_size = 512 , goal_dims : int = 32 , max_dets : int = 3 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), box_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), class_embedder_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), )","title":"ResnetFasterRCNNTensorsObjectNavActorCritic"},{"location":"api/plugins/robothor_plugin/robothor_models/#get_object_type_encoding_1","text":"ResnetFasterRCNNTensorsObjectNavActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#is_blind","text":"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"is_blind"},{"location":"api/plugins/robothor_plugin/robothor_models/#num_recurrent_layers","text":"Number of recurrent hidden layers.","title":"num_recurrent_layers"},{"location":"api/plugins/robothor_plugin/robothor_models/#recurrent_hidden_state_size","text":"The recurrent hidden state size of the model.","title":"recurrent_hidden_state_size"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorgoalencoder","text":"ResnetTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , class_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None","title":"ResnetTensorGoalEncoder"},{"location":"api/plugins/robothor_plugin/robothor_models/#get_object_type_encoding_2","text":"ResnetTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#resnettensorobjectnavactorcritic","text":"ResnetTensorObjectNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , rnn_hidden_size : int = 512 , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), )","title":"ResnetTensorObjectNavActorCritic"},{"location":"api/plugins/robothor_plugin/robothor_models/#get_object_type_encoding_3","text":"ResnetTensorObjectNavActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/plugins/robothor_plugin/robothor_models/#is_blind_1","text":"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"is_blind"},{"location":"api/plugins/robothor_plugin/robothor_models/#num_recurrent_layers_1","text":"Number of recurrent hidden layers.","title":"num_recurrent_layers"},{"location":"api/plugins/robothor_plugin/robothor_models/#recurrent_hidden_state_size_1","text":"The recurrent hidden state size of the model.","title":"recurrent_hidden_state_size"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/","text":"plugins.robothor_plugin.robothor_preprocessors [source] # BatchedFasterRCNN # BatchedFasterRCNN ( self , thres = 0.12 , maxdets = 3 , res = 7 ) COCO_INSTANCE_CATEGORY_NAMES # list() -> new empty list list(iterable) -> new list initialized from iterable's items FasterRCNNPreProcessorRoboThor # FasterRCNNPreProcessorRoboThor ( self , input_uuids : List [ str ], output_uuid : str , input_height : int , input_width : int , max_dets : int , detector_spatial_res : int , detector_thres : float , parallel : bool = False , device : Optional [ torch . device ] = None , device_ids : Optional [ List [ torch . device ]] = None , kwargs : Any , ) Preprocess RGB image using a ResNet model. COCO_INSTANCE_CATEGORY_NAMES # list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"robothor_preprocessors"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#pluginsrobothor_pluginrobothor_preprocessors-source","text":"","title":"plugins.robothor_plugin.robothor_preprocessors [source]"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#batchedfasterrcnn","text":"BatchedFasterRCNN ( self , thres = 0.12 , maxdets = 3 , res = 7 )","title":"BatchedFasterRCNN"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#coco_instance_category_names","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"COCO_INSTANCE_CATEGORY_NAMES"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#fasterrcnnpreprocessorrobothor","text":"FasterRCNNPreProcessorRoboThor ( self , input_uuids : List [ str ], output_uuid : str , input_height : int , input_width : int , max_dets : int , detector_spatial_res : int , detector_thres : float , parallel : bool = False , device : Optional [ torch . device ] = None , device_ids : Optional [ List [ torch . device ]] = None , kwargs : Any , ) Preprocess RGB image using a ResNet model.","title":"FasterRCNNPreProcessorRoboThor"},{"location":"api/plugins/robothor_plugin/robothor_preprocessors/#coco_instance_category_names_1","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"COCO_INSTANCE_CATEGORY_NAMES"},{"location":"api/plugins/robothor_plugin/robothor_sensors/","text":"plugins.robothor_plugin.robothor_sensors [source] # GPSCompassSensorRoboThor # GPSCompassSensorRoboThor ( self , uuid : str = 'target_coordinates_ind' , ** kwargs : Any ) quaternion_from_coeff # GPSCompassSensorRoboThor . quaternion_from_coeff ( coeffs : numpy . ndarray , ) -> quaternion . quaternion Creates a quaternions from coeffs in [x, y, z, w] format quaternion_from_y_angle # GPSCompassSensorRoboThor . quaternion_from_y_angle ( angle : float , ) -> quaternion . quaternion Creates a quaternion from rotation angle around y axis quaternion_rotate_vector # GPSCompassSensorRoboThor . quaternion_rotate_vector ( quat : quaternion . quaternion , v : < built - in function array > , ) -> < built - in function array > Rotates a vector by a quaternion Args: quat: The quaternion to rotate by v: The vector to rotate Returns: np.array: The rotated vector RGBSensorRoboThor # RGBSensorRoboThor ( self , use_resnet_normalization : bool = False , mean : Optional [ numpy . ndarray ] = [[[ 0.485 0.456 0.406 ]]], stdev : Optional [ numpy . ndarray ] = [[[ 0.229 0.224 0.225 ]]], height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'rgb' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , kwargs : Any , ) Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"robothor_sensors"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#pluginsrobothor_pluginrobothor_sensors-source","text":"","title":"plugins.robothor_plugin.robothor_sensors [source]"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#gpscompasssensorrobothor","text":"GPSCompassSensorRoboThor ( self , uuid : str = 'target_coordinates_ind' , ** kwargs : Any )","title":"GPSCompassSensorRoboThor"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#quaternion_from_coeff","text":"GPSCompassSensorRoboThor . quaternion_from_coeff ( coeffs : numpy . ndarray , ) -> quaternion . quaternion Creates a quaternions from coeffs in [x, y, z, w] format","title":"quaternion_from_coeff"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#quaternion_from_y_angle","text":"GPSCompassSensorRoboThor . quaternion_from_y_angle ( angle : float , ) -> quaternion . quaternion Creates a quaternion from rotation angle around y axis","title":"quaternion_from_y_angle"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#quaternion_rotate_vector","text":"GPSCompassSensorRoboThor . quaternion_rotate_vector ( quat : quaternion . quaternion , v : < built - in function array > , ) -> < built - in function array > Rotates a vector by a quaternion Args: quat: The quaternion to rotate by v: The vector to rotate Returns: np.array: The rotated vector","title":"quaternion_rotate_vector"},{"location":"api/plugins/robothor_plugin/robothor_sensors/#rgbsensorrobothor","text":"RGBSensorRoboThor ( self , use_resnet_normalization : bool = False , mean : Optional [ numpy . ndarray ] = [[[ 0.485 0.456 0.406 ]]], stdev : Optional [ numpy . ndarray ] = [[[ 0.229 0.224 0.225 ]]], height : Optional [ int ] = None , width : Optional [ int ] = None , uuid : str = 'rgb' , output_shape : Optional [ Tuple [ int , ... ]] = None , output_channels : int = 3 , unnormalized_infimum : float = 0.0 , unnormalized_supremum : float = 1.0 , scale_first : bool = True , kwargs : Any , ) Sensor for RGB images in RoboTHOR. Returns from a running RoboThorEnvironment instance, the current RGB frame corresponding to the agent's egocentric view.","title":"RGBSensorRoboThor"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/","text":"plugins.robothor_plugin.robothor_task_samplers [source] # ObjectNavDatasetTaskSampler # ObjectNavDatasetTaskSampler ( self , scenes : List [ str ], scene_directory : str , sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , loop_dataset : bool = True , allow_flipping = False , env_class = < class ' plugins . robothor_plugin . robothor_environment . RoboThorEnvironment '>, args , kwargs , ) -> None all_observation_spaces_equal # Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. length # Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). ObjectNavTaskSampler # ObjectNavTaskSampler ( self , scenes : Union [ List [ str ], str ], object_types : List [ str ], sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , scene_period : Optional [ int , str ] = None , max_tasks : Optional [ int ] = None , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , allow_flipping : bool = False , dataset_first : int = - 1 , dataset_last : int = - 1 , args , kwargs , ) -> None all_observation_spaces_equal # Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. length # Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavDatasetTaskSampler # PointNavDatasetTaskSampler ( self , scenes : List [ str ], scene_directory : str , sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , loop_dataset : bool = True , shuffle_dataset : bool = True , allow_flipping = False , env_class = < class ' plugins . robothor_plugin . robothor_environment . RoboThorEnvironment '>, args , kwargs , ) -> None all_observation_spaces_equal # Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. length # Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf'). PointNavTaskSampler # PointNavTaskSampler ( self , scenes : List [ str ], sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , scene_period : Optional [ int , str ] = None , max_tasks : Optional [ int ] = None , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , fixed_tasks : Optional [ List [ Dict [ str , Any ]]] = None , args , kwargs , ) -> None all_observation_spaces_equal # Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False. length # Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"robothor_task_samplers"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pluginsrobothor_pluginrobothor_task_samplers-source","text":"","title":"plugins.robothor_plugin.robothor_task_samplers [source]"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavdatasettasksampler","text":"ObjectNavDatasetTaskSampler ( self , scenes : List [ str ], scene_directory : str , sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , loop_dataset : bool = True , allow_flipping = False , env_class = < class ' plugins . robothor_plugin . robothor_environment . RoboThorEnvironment '>, args , kwargs , ) -> None","title":"ObjectNavDatasetTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#all_observation_spaces_equal","text":"Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#length","text":"Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"length"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#objectnavtasksampler","text":"ObjectNavTaskSampler ( self , scenes : Union [ List [ str ], str ], object_types : List [ str ], sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , scene_period : Optional [ int , str ] = None , max_tasks : Optional [ int ] = None , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , allow_flipping : bool = False , dataset_first : int = - 1 , dataset_last : int = - 1 , args , kwargs , ) -> None","title":"ObjectNavTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#all_observation_spaces_equal_1","text":"Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#length_1","text":"Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"length"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavdatasettasksampler","text":"PointNavDatasetTaskSampler ( self , scenes : List [ str ], scene_directory : str , sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , loop_dataset : bool = True , shuffle_dataset : bool = True , allow_flipping = False , env_class = < class ' plugins . robothor_plugin . robothor_environment . RoboThorEnvironment '>, args , kwargs , ) -> None","title":"PointNavDatasetTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#all_observation_spaces_equal_2","text":"Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#length_2","text":"Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"length"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#pointnavtasksampler","text":"PointNavTaskSampler ( self , scenes : List [ str ], sensors : List [ core . base_abstractions . sensor . Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . spaces . space . Space , rewards_config : Dict , scene_period : Optional [ int , str ] = None , max_tasks : Optional [ int ] = None , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , fixed_tasks : Optional [ List [ Dict [ str , Any ]]] = None , args , kwargs , ) -> None","title":"PointNavTaskSampler"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#all_observation_spaces_equal_3","text":"Check if observation spaces equal. Returns True if all Tasks that can be sampled by this sampler have the same observation space. Otherwise False.","title":"all_observation_spaces_equal"},{"location":"api/plugins/robothor_plugin/robothor_task_samplers/#length_3","text":"Length. Returns Number of total tasks remaining that can be sampled. Can be float('inf').","title":"length"},{"location":"api/plugins/robothor_plugin/robothor_tasks/","text":"plugins.robothor_plugin.robothor_tasks [source] # ObjectNavTask # ObjectNavTask ( self , env : plugins . robothor_plugin . robothor_environment . RoboThorEnvironment , sensors : List [ core . base_abstractions . sensor . Sensor ], task_info : Dict [ str , Any ], max_steps : int , reward_configs : Dict [ str , Any ], distance_cache : Optional [ Dict [ str , Any ]] = None , kwargs , ) -> None judge # ObjectNavTask . judge ( self ) -> float Judge the last event. PointNavTask # PointNavTask ( self , env : plugins . robothor_plugin . robothor_environment . RoboThorEnvironment , sensors : List [ core . base_abstractions . sensor . Sensor ], task_info : Dict [ str , Any ], max_steps : int , reward_configs : Dict [ str , Any ], distance_cache : Optional [ Dict [ str , Any ]] = None , episode_info : Optional [ Dict [ str , Any ]] = None , kwargs , ) -> None judge # PointNavTask . judge ( self ) -> float Judge the last event.","title":"robothor_tasks"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#pluginsrobothor_pluginrobothor_tasks-source","text":"","title":"plugins.robothor_plugin.robothor_tasks [source]"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#objectnavtask","text":"ObjectNavTask ( self , env : plugins . robothor_plugin . robothor_environment . RoboThorEnvironment , sensors : List [ core . base_abstractions . sensor . Sensor ], task_info : Dict [ str , Any ], max_steps : int , reward_configs : Dict [ str , Any ], distance_cache : Optional [ Dict [ str , Any ]] = None , kwargs , ) -> None","title":"ObjectNavTask"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#judge","text":"ObjectNavTask . judge ( self ) -> float Judge the last event.","title":"judge"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#pointnavtask","text":"PointNavTask ( self , env : plugins . robothor_plugin . robothor_environment . RoboThorEnvironment , sensors : List [ core . base_abstractions . sensor . Sensor ], task_info : Dict [ str , Any ], max_steps : int , reward_configs : Dict [ str , Any ], distance_cache : Optional [ Dict [ str , Any ]] = None , episode_info : Optional [ Dict [ str , Any ]] = None , kwargs , ) -> None","title":"PointNavTask"},{"location":"api/plugins/robothor_plugin/robothor_tasks/#judge_1","text":"PointNavTask . judge ( self ) -> float Judge the last event.","title":"judge"},{"location":"api/plugins/robothor_plugin/robothor_viz/","text":"plugins.robothor_plugin.robothor_viz [source] #","title":"robothor_viz"},{"location":"api/plugins/robothor_plugin/robothor_viz/#pluginsrobothor_pluginrobothor_viz-source","text":"","title":"plugins.robothor_plugin.robothor_viz [source]"},{"location":"api/plugins/robothor_plugin/configs/nav_base/","text":"plugins.robothor_plugin.configs.nav_base [source] # NavBaseConfig # NavBaseConfig ( self , / , * args , ** kwargs ) A Navigation base configuration in RoboTHOR. ADVANCE_SCENE_ROLLOUT_PERIOD # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 NUM_PROCESSES # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 TRAIN_SCENES # list() -> new empty list list(iterable) -> new list initialized from iterable's items VALID_SCENES # list() -> new empty list list(iterable) -> new list initialized from iterable's items VALIDATION_SAMPLES_PER_SCENE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"nav_base"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#pluginsrobothor_pluginconfigsnav_base-source","text":"","title":"plugins.robothor_plugin.configs.nav_base [source]"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#navbaseconfig","text":"NavBaseConfig ( self , / , * args , ** kwargs ) A Navigation base configuration in RoboTHOR.","title":"NavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#advance_scene_rollout_period","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ADVANCE_SCENE_ROLLOUT_PERIOD"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#num_processes","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_PROCESSES"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#train_scenes","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"TRAIN_SCENES"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#valid_scenes","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"VALID_SCENES"},{"location":"api/plugins/robothor_plugin/configs/nav_base/#validation_samples_per_scene","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"VALIDATION_SAMPLES_PER_SCENE"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/","text":"plugins.robothor_plugin.configs.objectnav_base [source] # ObjectNavBaseConfig # ObjectNavBaseConfig ( self , / , * args , ** kwargs ) An Object Navigation base configuration. CAMERA_HEIGHT # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 CAMERA_WIDTH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ENV_ARGS # dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) MAX_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SCREEN_SIZE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SENSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items TARGET_TYPES # list() -> new empty list list(iterable) -> new list initialized from iterable's items TARGET_UUID # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. VISION_UUID # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"objectnav_base"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#pluginsrobothor_pluginconfigsobjectnav_base-source","text":"","title":"plugins.robothor_plugin.configs.objectnav_base [source]"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#objectnavbaseconfig","text":"ObjectNavBaseConfig ( self , / , * args , ** kwargs ) An Object Navigation base configuration.","title":"ObjectNavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#camera_height","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_HEIGHT"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#camera_width","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_WIDTH"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#env_args","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"ENV_ARGS"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#max_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"MAX_STEPS"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#screen_size","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"SCREEN_SIZE"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#sensors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"SENSORS"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#target_types","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"TARGET_TYPES"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#target_uuid","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"TARGET_UUID"},{"location":"api/plugins/robothor_plugin/configs/objectnav_base/#vision_uuid","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"VISION_UUID"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/","text":"plugins.robothor_plugin.configs.pointnav_base [source] # PointNavBaseConfig # PointNavBaseConfig ( self , / , * args , ** kwargs ) A Point Navigation base configuration. CAMERA_HEIGHT # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 CAMERA_WIDTH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ENV_ARGS # dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) MAX_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SCREEN_SIZE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SENSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items TARGET_UUID # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. VISION_UUID # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"pointnav_base"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#pluginsrobothor_pluginconfigspointnav_base-source","text":"","title":"plugins.robothor_plugin.configs.pointnav_base [source]"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#pointnavbaseconfig","text":"PointNavBaseConfig ( self , / , * args , ** kwargs ) A Point Navigation base configuration.","title":"PointNavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#camera_height","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_HEIGHT"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#camera_width","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_WIDTH"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#env_args","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"ENV_ARGS"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#max_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"MAX_STEPS"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#screen_size","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"SCREEN_SIZE"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#sensors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"SENSORS"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#target_uuid","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"TARGET_UUID"},{"location":"api/plugins/robothor_plugin/configs/pointnav_base/#vision_uuid","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"VISION_UUID"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/","text":"plugins.robothor_plugin.configs.resnet18_nav_base [source] # Resnet18NavBaseConfig # Resnet18NavBaseConfig ( self ) A Navigation base configuration in RoboTHOR. RESNET_OUTPUT_UUID # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"resnet18_nav_base"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/#pluginsrobothor_pluginconfigsresnet18_nav_base-source","text":"","title":"plugins.robothor_plugin.configs.resnet18_nav_base [source]"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/#resnet18navbaseconfig","text":"Resnet18NavBaseConfig ( self ) A Navigation base configuration in RoboTHOR.","title":"Resnet18NavBaseConfig"},{"location":"api/plugins/robothor_plugin/configs/resnet18_nav_base/#resnet_output_uuid","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"RESNET_OUTPUT_UUID"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/","text":"plugins.robothor_plugin.configs.resnet18_objectnav [source] # Resnet18ObjectNavExperimentConfig # Resnet18ObjectNavExperimentConfig ( self ) An Object Navigation experiment configuration. OBSERVATIONS # list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"resnet18_objectnav"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/#pluginsrobothor_pluginconfigsresnet18_objectnav-source","text":"","title":"plugins.robothor_plugin.configs.resnet18_objectnav [source]"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/#resnet18objectnavexperimentconfig","text":"Resnet18ObjectNavExperimentConfig ( self ) An Object Navigation experiment configuration.","title":"Resnet18ObjectNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/resnet18_objectnav/#observations","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"OBSERVATIONS"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/","text":"plugins.robothor_plugin.configs.resnet18_pointnav [source] # Resnet18PointNavExperimentConfig # Resnet18PointNavExperimentConfig ( self ) A Point Navigation experiment configuration. OBSERVATIONS # list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"resnet18_pointnav"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/#pluginsrobothor_pluginconfigsresnet18_pointnav-source","text":"","title":"plugins.robothor_plugin.configs.resnet18_pointnav [source]"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/#resnet18pointnavexperimentconfig","text":"Resnet18PointNavExperimentConfig ( self ) A Point Navigation experiment configuration.","title":"Resnet18PointNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/resnet18_pointnav/#observations","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"OBSERVATIONS"},{"location":"api/plugins/robothor_plugin/configs/simple_objectnav/","text":"plugins.robothor_plugin.configs.simple_objectnav [source] # SimpleObjectNavExperimentConfig # SimpleObjectNavExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration.","title":"simple_objectnav"},{"location":"api/plugins/robothor_plugin/configs/simple_objectnav/#pluginsrobothor_pluginconfigssimple_objectnav-source","text":"","title":"plugins.robothor_plugin.configs.simple_objectnav [source]"},{"location":"api/plugins/robothor_plugin/configs/simple_objectnav/#simpleobjectnavexperimentconfig","text":"SimpleObjectNavExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration.","title":"SimpleObjectNavExperimentConfig"},{"location":"api/plugins/robothor_plugin/configs/simple_pointnav/","text":"plugins.robothor_plugin.configs.simple_pointnav [source] # SimplePointNavExperimentConfig # SimplePointNavExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration.","title":"simple_pointnav"},{"location":"api/plugins/robothor_plugin/configs/simple_pointnav/#pluginsrobothor_pluginconfigssimple_pointnav-source","text":"","title":"plugins.robothor_plugin.configs.simple_pointnav [source]"},{"location":"api/plugins/robothor_plugin/configs/simple_pointnav/#simplepointnavexperimentconfig","text":"SimplePointNavExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration.","title":"SimplePointNavExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/base/","text":"projects.babyai_baselines.experiments.base [source] # BaseBabyAIExperimentConfig # BaseBabyAIExperimentConfig ( self , / , * args , ** kwargs ) Base experimental config. AGENT_VIEW_SIZE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 DEFAULT_LR # float(x) -> floating point number Convert a string or number to a floating point number, if possible. NUM_CKPTS_TO_SAVE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 PPO_NUM_MINI_BATCH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SHOULD_LOG # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. TEST_SEED_OFFSET # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/base/#projectsbabyai_baselinesexperimentsbase-source","text":"","title":"projects.babyai_baselines.experiments.base [source]"},{"location":"api/projects/babyai_baselines/experiments/base/#basebabyaiexperimentconfig","text":"BaseBabyAIExperimentConfig ( self , / , * args , ** kwargs ) Base experimental config.","title":"BaseBabyAIExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/base/#agent_view_size","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"AGENT_VIEW_SIZE"},{"location":"api/projects/babyai_baselines/experiments/base/#default_lr","text":"float(x) -> floating point number Convert a string or number to a floating point number, if possible.","title":"DEFAULT_LR"},{"location":"api/projects/babyai_baselines/experiments/base/#num_ckpts_to_save","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_CKPTS_TO_SAVE"},{"location":"api/projects/babyai_baselines/experiments/base/#ppo_num_mini_batch","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"PPO_NUM_MINI_BATCH"},{"location":"api/projects/babyai_baselines/experiments/base/#should_log","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"SHOULD_LOG"},{"location":"api/projects/babyai_baselines/experiments/base/#test_seed_offset","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"TEST_SEED_OFFSET"},{"location":"api/projects/babyai_baselines/experiments/base/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/","text":"projects.babyai_baselines.experiments.go_to_local.a2c [source] # A2CBabyAIGoToLocalExperimentConfig # A2CBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) A2C only. DEFAULT_LR # float(x) -> floating point number Convert a string or number to a floating point number, if possible. NUM_TRAIN_SAMPLERS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ROLLOUT_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 USE_LR_DECAY # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#projectsbabyai_baselinesexperimentsgo_to_locala2c-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.a2c [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#a2cbabyaigotolocalexperimentconfig","text":"A2CBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) A2C only.","title":"A2CBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#default_lr","text":"float(x) -> floating point number Convert a string or number to a floating point number, if possible.","title":"DEFAULT_LR"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#num_train_samplers","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_TRAIN_SAMPLERS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#rollout_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ROLLOUT_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/a2c/#use_lr_decay","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_LR_DECAY"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/","text":"projects.babyai_baselines.experiments.go_to_local.base [source] # BaseBabyAIGoToLocalExperimentConfig # BaseBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Base experimental config. ARCH # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. INCLUDE_AUXILIARY_HEAD # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. INSTR_LEN # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 LEVEL # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. NUM_CKPTS_TO_SAVE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 NUM_TEST_TASKS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 NUM_TRAIN_SAMPLERS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 PPO_NUM_MINI_BATCH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ROLLOUT_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 TOTAL_IL_TRAIN_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 TOTAL_RL_TRAIN_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 USE_INSTR # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. USE_LR_DECAY # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#projectsbabyai_baselinesexperimentsgo_to_localbase-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.base [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#basebabyaigotolocalexperimentconfig","text":"BaseBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Base experimental config.","title":"BaseBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#arch","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"ARCH"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#include_auxiliary_head","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"INCLUDE_AUXILIARY_HEAD"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#instr_len","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"INSTR_LEN"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#level","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"LEVEL"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#num_ckpts_to_save","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_CKPTS_TO_SAVE"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#num_test_tasks","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_TEST_TASKS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#num_train_samplers","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_TRAIN_SAMPLERS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#ppo_num_mini_batch","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"PPO_NUM_MINI_BATCH"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#rollout_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ROLLOUT_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#total_il_train_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"TOTAL_IL_TRAIN_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#total_rl_train_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"TOTAL_RL_TRAIN_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#use_instr","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_INSTR"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/base/#use_lr_decay","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_LR_DECAY"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/","text":"projects.babyai_baselines.experiments.go_to_local.bc [source] # PPOBabyAIGoToLocalExperimentConfig # PPOBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone then PPO. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#projectsbabyai_baselinesexperimentsgo_to_localbc-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.bc [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#ppobabyaigotolocalexperimentconfig","text":"PPOBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone then PPO.","title":"PPOBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_offpolicy/","text":"projects.babyai_baselines.experiments.go_to_local.bc_offpolicy [source] # BCOffPolicyBabyAIGoToLocalExperimentConfig # BCOffPolicyBabyAIGoToLocalExperimentConfig ( self ) BC Off policy imitation.","title":"bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_offpolicy/#projectsbabyai_baselinesexperimentsgo_to_localbc_offpolicy-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.bc_offpolicy [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_offpolicy/#bcoffpolicybabyaigotolocalexperimentconfig","text":"BCOffPolicyBabyAIGoToLocalExperimentConfig ( self ) BC Off policy imitation.","title":"BCOffPolicyBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_local.bc_teacher_forcing [source] # BCTeacherForcingBabyAIGoToLocalExperimentConfig # BCTeacherForcingBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone with teacher forcing. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_localbc_teacher_forcing-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.bc_teacher_forcing [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#bcteacherforcingbabyaigotolocalexperimentconfig","text":"BCTeacherForcingBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone with teacher forcing.","title":"BCTeacherForcingBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/bc_teacher_forcing/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/","text":"projects.babyai_baselines.experiments.go_to_local.dagger [source] # DaggerBabyAIGoToLocalExperimentConfig # DaggerBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Find goal in lighthouse env using imitation learning. Training with Dagger. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#projectsbabyai_baselinesexperimentsgo_to_localdagger-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.dagger [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#daggerbabyaigotolocalexperimentconfig","text":"DaggerBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"DaggerBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/dagger/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/","text":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_offpolicy [source] # DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig # DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig ( self ) Distributed Off policy imitation.","title":"distributed_bc_offpolicy"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/#projectsbabyai_baselinesexperimentsgo_to_localdistributed_bc_offpolicy-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_offpolicy [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_offpolicy/#distributedbcoffpolicybabyaigotolocalexperimentconfig","text":"DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig ( self ) Distributed Off policy imitation.","title":"DistributedBCOffPolicyBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_teacher_forcing [source] # DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig # DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig ( self , args , kwargs , ) Distributed behavior clone with teacher forcing. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"distributed_bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_localdistributed_bc_teacher_forcing-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.distributed_bc_teacher_forcing [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#distributedbcteacherforcingbabyaigotolocalexperimentconfig","text":"DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig ( self , args , kwargs , ) Distributed behavior clone with teacher forcing.","title":"DistributedBCTeacherForcingBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/distributed_bc_teacher_forcing/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/","text":"projects.babyai_baselines.experiments.go_to_local.ppo [source] # PPOBabyAIGoToLocalExperimentConfig # PPOBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) PPO only. DEFAULT_LR # float(x) -> floating point number Convert a string or number to a floating point number, if possible. NUM_TRAIN_SAMPLERS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ROLLOUT_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 USE_LR_DECAY # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#projectsbabyai_baselinesexperimentsgo_to_localppo-source","text":"","title":"projects.babyai_baselines.experiments.go_to_local.ppo [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#ppobabyaigotolocalexperimentconfig","text":"PPOBabyAIGoToLocalExperimentConfig ( self , / , * args , ** kwargs ) PPO only.","title":"PPOBabyAIGoToLocalExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#default_lr","text":"float(x) -> floating point number Convert a string or number to a floating point number, if possible.","title":"DEFAULT_LR"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#num_train_samplers","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_TRAIN_SAMPLERS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#rollout_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ROLLOUT_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_local/ppo/#use_lr_decay","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_LR_DECAY"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/","text":"projects.babyai_baselines.experiments.go_to_obj.a2c [source] # A2CBabyAIGoToObjExperimentConfig # A2CBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) A2C only. TOTAL_RL_TRAIN_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"a2c"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#projectsbabyai_baselinesexperimentsgo_to_obja2c-source","text":"","title":"projects.babyai_baselines.experiments.go_to_obj.a2c [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#a2cbabyaigotoobjexperimentconfig","text":"A2CBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) A2C only.","title":"A2CBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/a2c/#total_rl_train_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"TOTAL_RL_TRAIN_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/","text":"projects.babyai_baselines.experiments.go_to_obj.base [source] # BaseBabyAIGoToObjExperimentConfig # BaseBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Base experimental config. ARCH # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. DEFAULT_LR # float(x) -> floating point number Convert a string or number to a floating point number, if possible. INSTR_LEN # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 LEVEL # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. NUM_TEST_TASKS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 NUM_TRAIN_SAMPLERS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 PPO_NUM_MINI_BATCH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ROLLOUT_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 TOTAL_IL_TRAIN_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 TOTAL_RL_TRAIN_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 USE_INSTR # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. USE_LR_DECAY # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"base"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#projectsbabyai_baselinesexperimentsgo_to_objbase-source","text":"","title":"projects.babyai_baselines.experiments.go_to_obj.base [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#basebabyaigotoobjexperimentconfig","text":"BaseBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Base experimental config.","title":"BaseBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#arch","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"ARCH"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#default_lr","text":"float(x) -> floating point number Convert a string or number to a floating point number, if possible.","title":"DEFAULT_LR"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#instr_len","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"INSTR_LEN"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#level","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"LEVEL"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#num_test_tasks","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_TEST_TASKS"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#num_train_samplers","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_TRAIN_SAMPLERS"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#ppo_num_mini_batch","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"PPO_NUM_MINI_BATCH"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#rollout_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ROLLOUT_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#total_il_train_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"TOTAL_IL_TRAIN_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#total_rl_train_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"TOTAL_RL_TRAIN_STEPS"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#use_instr","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_INSTR"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/base/#use_lr_decay","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_LR_DECAY"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/","text":"projects.babyai_baselines.experiments.go_to_obj.bc [source] # PPOBabyAIGoToObjExperimentConfig # PPOBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone then PPO. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"bc"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#projectsbabyai_baselinesexperimentsgo_to_objbc-source","text":"","title":"projects.babyai_baselines.experiments.go_to_obj.bc [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#ppobabyaigotoobjexperimentconfig","text":"PPOBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone then PPO.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/","text":"projects.babyai_baselines.experiments.go_to_obj.bc_teacher_forcing [source] # PPOBabyAIGoToObjExperimentConfig # PPOBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone (with teacher forcing) then PPO. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"bc_teacher_forcing"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#projectsbabyai_baselinesexperimentsgo_to_objbc_teacher_forcing-source","text":"","title":"projects.babyai_baselines.experiments.go_to_obj.bc_teacher_forcing [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#ppobabyaigotoobjexperimentconfig","text":"PPOBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Behavior clone (with teacher forcing) then PPO.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/bc_teacher_forcing/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/","text":"projects.babyai_baselines.experiments.go_to_obj.dagger [source] # DaggerBabyAIGoToObjExperimentConfig # DaggerBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Find goal in lighthouse env using imitation learning. Training with Dagger. USE_EXPERT # bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"dagger"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#projectsbabyai_baselinesexperimentsgo_to_objdagger-source","text":"","title":"projects.babyai_baselines.experiments.go_to_obj.dagger [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#daggerbabyaigotoobjexperimentconfig","text":"DaggerBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) Find goal in lighthouse env using imitation learning. Training with Dagger.","title":"DaggerBabyAIGoToObjExperimentConfig"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/dagger/#use_expert","text":"bool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed.","title":"USE_EXPERT"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/","text":"projects.babyai_baselines.experiments.go_to_obj.ppo [source] # PPOBabyAIGoToObjExperimentConfig # PPOBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) PPO only.","title":"ppo"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/#projectsbabyai_baselinesexperimentsgo_to_objppo-source","text":"","title":"projects.babyai_baselines.experiments.go_to_obj.ppo [source]"},{"location":"api/projects/babyai_baselines/experiments/go_to_obj/ppo/#ppobabyaigotoobjexperimentconfig","text":"PPOBabyAIGoToObjExperimentConfig ( self , / , * args , ** kwargs ) PPO only.","title":"PPOBabyAIGoToObjExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/","text":"projects.objectnav_baselines.experiments.objectnav_base [source] # ObjectNavBaseConfig # ObjectNavBaseConfig ( self ) The base object navigation configuration file.","title":"objectnav_base"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/#projectsobjectnav_baselinesexperimentsobjectnav_base-source","text":"","title":"projects.objectnav_baselines.experiments.objectnav_base [source]"},{"location":"api/projects/objectnav_baselines/experiments/objectnav_base/#objectnavbaseconfig","text":"ObjectNavBaseConfig ( self ) The base object navigation configuration file.","title":"ObjectNavBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_base [source] # ObjectNaviThorBaseConfig # ObjectNaviThorBaseConfig ( self ) The base config for all iTHOR ObjectNav experiments.","title":"objectnav_ithor_base"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_base-source","text":"","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_base [source]"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_base/#objectnavithorbaseconfig","text":"ObjectNaviThorBaseConfig ( self ) The base config for all iTHOR ObjectNav experiments.","title":"ObjectNaviThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_depth_resnetgru_ddppo [source] # ObjectNavRoboThorRGBPPOExperimentConfig # ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in iThor with Depth input.","title":"objectnav_ithor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_depth_resnetgru_ddppo-source","text":"","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_depth_resnetgru_ddppo [source]"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_depth_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in iThor with Depth input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgb_resnetgru_ddppo [source] # ObjectNaviThorRGBPPOExperimentConfig # ObjectNaviThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in iThor with RGB input.","title":"objectnav_ithor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_rgb_resnetgru_ddppo-source","text":"","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgb_resnetgru_ddppo [source]"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgb_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"ObjectNaviThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in iThor with RGB input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgbd_resnetgru_ddppo [source] # ObjectNavRoboThorRGBPPOExperimentConfig # ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in iThor with RGBD input.","title":"objectnav_ithor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsithorobjectnav_ithor_rgbd_resnetgru_ddppo-source","text":"","title":"projects.objectnav_baselines.experiments.ithor.objectnav_ithor_rgbd_resnetgru_ddppo [source]"},{"location":"api/projects/objectnav_baselines/experiments/ithor/objectnav_ithor_rgbd_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in iThor with RGBD input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_base [source] # ObjectNavRoboThorBaseConfig # ObjectNavRoboThorBaseConfig ( self ) The base config for all RoboTHOR ObjectNav experiments.","title":"objectnav_robothor_base"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_base-source","text":"","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_base [source]"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_base/#objectnavrobothorbaseconfig","text":"ObjectNavRoboThorBaseConfig ( self ) The base config for all RoboTHOR ObjectNav experiments.","title":"ObjectNavRoboThorBaseConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_depth_resnetgru_ddppo [source] # ObjectNavRoboThorRGBPPOExperimentConfig # ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in RoboThor with Depth input.","title":"objectnav_robothor_depth_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_depth_resnetgru_ddppo-source","text":"","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_depth_resnetgru_ddppo [source]"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_depth_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in RoboThor with Depth input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo [source] # ObjectNaviThorRGBPPOExperimentConfig # ObjectNaviThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in RoboThor with RGB input.","title":"objectnav_robothor_rgb_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgb_resnetgru_ddppo-source","text":"","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgb_resnetgru_ddppo [source]"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgb_resnetgru_ddppo/#objectnavithorrgbppoexperimentconfig","text":"ObjectNaviThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in RoboThor with RGB input.","title":"ObjectNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/","text":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgbd_resnetgru_ddppo [source] # ObjectNavRoboThorRGBPPOExperimentConfig # ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in RoboThor with RGBD input.","title":"objectnav_robothor_rgbd_resnetgru_ddppo"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/#projectsobjectnav_baselinesexperimentsrobothorobjectnav_robothor_rgbd_resnetgru_ddppo-source","text":"","title":"projects.objectnav_baselines.experiments.robothor.objectnav_robothor_rgbd_resnetgru_ddppo [source]"},{"location":"api/projects/objectnav_baselines/experiments/robothor/objectnav_robothor_rgbd_resnetgru_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"ObjectNavRoboThorRGBPPOExperimentConfig ( self ) An Object Navigation experiment configuration in RoboThor with RGBD input.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/","text":"projects.objectnav_baselines.models.object_nav_models [source] # Baseline models for use in the object navigation task. Object navigation is currently available as a Task in AI2-THOR and Facebook's Habitat. ObjectNavBaselineActorCritic # ObjectNavBaselineActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = 'GRU' , ) Baseline recurrent actor critic model for object-navigation. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal goal_sensor_uuid . goal_sensor_uuid : The uuid of the sensor of the goal object. See GoalObjectTypeThorSensor as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim : The dimensionality of the embedding corresponding to the goal object type. forward # ObjectNavBaselineActorCritic . forward ( self , observations : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], memory : core . base_abstractions . misc . Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ core . base_abstractions . misc . ActorCriticOutput [ ~ DistributionType ], Optional [ core . base_abstractions . misc . Memory ]] Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an ActorCriticOutput object containing the model's policy (distribution over actions) and evaluation of the current state (value). Parameters observations : Batched input observations. rnn_hidden_states : Hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See RNNStateEncoder . Returns Tuple of the ActorCriticOutput and recurrent hidden state. get_object_type_encoding # ObjectNavBaselineActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. is_blind # True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). num_recurrent_layers # Number of recurrent hidden layers. recurrent_hidden_state_size # The recurrent hidden state size of the model. ResnetDualTensorGoalEncoder # ResnetDualTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : str , depth_resnet_preprocessor_uuid : str , class_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None get_object_type_encoding # ResnetDualTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. ResnetTensorGoalEncoder # ResnetTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , class_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None get_object_type_encoding # ResnetTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. ResnetTensorObjectNavActorCritic # ResnetTensorObjectNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : Optional [ str ], depth_resnet_preprocessor_uuid : Optional [ str ] = None , hidden_size : int = 512 , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) get_object_type_encoding # ResnetTensorObjectNavActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. is_blind # True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). num_recurrent_layers # Number of recurrent hidden layers. recurrent_hidden_state_size # The recurrent hidden state size of the model.","title":"object_nav_models"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#projectsobjectnav_baselinesmodelsobject_nav_models-source","text":"Baseline models for use in the object navigation task. Object navigation is currently available as a Task in AI2-THOR and Facebook's Habitat.","title":"projects.objectnav_baselines.models.object_nav_models [source]"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#objectnavbaselineactorcritic","text":"ObjectNavBaselineActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , trainable_masked_hidden_state : bool = False , num_rnn_layers = 1 , rnn_type = 'GRU' , ) Baseline recurrent actor critic model for object-navigation. Attributes action_space : The space of actions available to the agent. Currently only discrete actions are allowed (so this space will always be of type gym.spaces.Discrete ). observation_space : The observation space expected by the agent. This observation space should include (optionally) 'rgb' images and 'depth' images and is required to have a component corresponding to the goal goal_sensor_uuid . goal_sensor_uuid : The uuid of the sensor of the goal object. See GoalObjectTypeThorSensor as an example of such a sensor. hidden_size : The hidden size of the GRU RNN. object_type_embedding_dim : The dimensionality of the embedding corresponding to the goal object type.","title":"ObjectNavBaselineActorCritic"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#forward","text":"ObjectNavBaselineActorCritic . forward ( self , observations : Dict [ str , Union [ torch . Tensor , Dict [ str , Any ]]], memory : core . base_abstractions . misc . Memory , prev_actions : torch . Tensor , masks : torch . FloatTensor , ) -> Tuple [ core . base_abstractions . misc . ActorCriticOutput [ ~ DistributionType ], Optional [ core . base_abstractions . misc . Memory ]] Processes input batched observations to produce new actor and critic values. Processes input batched observations (along with prior hidden states, previous actions, and masks denoting which recurrent hidden states should be masked) and returns an ActorCriticOutput object containing the model's policy (distribution over actions) and evaluation of the current state (value). Parameters observations : Batched input observations. rnn_hidden_states : Hidden states from initial timepoints. prev_actions : Tensor of previous actions taken. masks : Masks applied to hidden states. See RNNStateEncoder . Returns Tuple of the ActorCriticOutput and recurrent hidden state.","title":"forward"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#get_object_type_encoding","text":"ObjectNavBaselineActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#is_blind","text":"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"is_blind"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#num_recurrent_layers","text":"Number of recurrent hidden layers.","title":"num_recurrent_layers"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#recurrent_hidden_state_size","text":"The recurrent hidden state size of the model.","title":"recurrent_hidden_state_size"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnetdualtensorgoalencoder","text":"ResnetDualTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : str , depth_resnet_preprocessor_uuid : str , class_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None","title":"ResnetDualTensorGoalEncoder"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#get_object_type_encoding_1","text":"ResnetDualTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorgoalencoder","text":"ResnetTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , class_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None","title":"ResnetTensorGoalEncoder"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#get_object_type_encoding_2","text":"ResnetTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#resnettensorobjectnavactorcritic","text":"ResnetTensorObjectNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : Optional [ str ], depth_resnet_preprocessor_uuid : Optional [ str ] = None , hidden_size : int = 512 , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), )","title":"ResnetTensorObjectNavActorCritic"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#get_object_type_encoding_3","text":"ResnetTensorObjectNavActorCritic . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#is_blind_1","text":"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"is_blind"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#num_recurrent_layers_1","text":"Number of recurrent hidden layers.","title":"num_recurrent_layers"},{"location":"api/projects/objectnav_baselines/models/object_nav_models/#recurrent_hidden_state_size_1","text":"The recurrent hidden state size of the model.","title":"recurrent_hidden_state_size"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/","text":"projects.pointnav_baselines.experiments.pointnav_base [source] # PointNavBaseConfig # PointNavBaseConfig ( self ) An Object Navigation experiment configuration in iThor.","title":"pointnav_base"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/#projectspointnav_baselinesexperimentspointnav_base-source","text":"","title":"projects.pointnav_baselines.experiments.pointnav_base [source]"},{"location":"api/projects/pointnav_baselines/experiments/pointnav_base/#pointnavbaseconfig","text":"PointNavBaseConfig ( self ) An Object Navigation experiment configuration in iThor.","title":"PointNavBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_base [source] # PointNaviThorBaseConfig # PointNaviThorBaseConfig ( self ) The base config for all iTHOR PointNav experiments.","title":"pointnav_ithor_base"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/#projectspointnav_baselinesexperimentsithorpointnav_ithor_base-source","text":"","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_base [source]"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_base/#pointnavithorbaseconfig","text":"PointNaviThorBaseConfig ( self ) The base config for all iTHOR PointNav experiments.","title":"PointNaviThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo [source] # PointNaviThorRGBPPOExperimentConfig # PointNaviThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in iThor with Depth input.","title":"pointnav_ithor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_depth_simpleconvgru_ddppo-source","text":"","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_depth_simpleconvgru_ddppo [source]"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_depth_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"PointNaviThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in iThor with Depth input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgb_simpleconvgru_ddppo [source] # PointNaviThorRGBPPOExperimentConfig # PointNaviThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in iThor with RGB input.","title":"pointnav_ithor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_rgb_simpleconvgru_ddppo-source","text":"","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgb_simpleconvgru_ddppo [source]"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgb_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"PointNaviThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in iThor with RGB input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgbd_simpleconvgru_ddppo [source] # PointNaviThorRGBPPOExperimentConfig # PointNaviThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in iThor with RGBD input.","title":"pointnav_ithor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsithorpointnav_ithor_rgbd_simpleconvgru_ddppo-source","text":"","title":"projects.pointnav_baselines.experiments.ithor.pointnav_ithor_rgbd_simpleconvgru_ddppo [source]"},{"location":"api/projects/pointnav_baselines/experiments/ithor/pointnav_ithor_rgbd_simpleconvgru_ddppo/#pointnavithorrgbppoexperimentconfig","text":"PointNaviThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in iThor with RGBD input.","title":"PointNaviThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_base [source] # PointNavRoboThorBaseConfig # PointNavRoboThorBaseConfig ( self ) The base config for all iTHOR PointNav experiments. ADVANCE_SCENE_ROLLOUT_PERIOD # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"pointnav_robothor_base"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_base-source","text":"","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_base [source]"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#pointnavrobothorbaseconfig","text":"PointNavRoboThorBaseConfig ( self ) The base config for all iTHOR PointNav experiments.","title":"PointNavRoboThorBaseConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_base/#advance_scene_rollout_period","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ADVANCE_SCENE_ROLLOUT_PERIOD"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_depth_simpleconvgru_ddppo [source] # PointNavRoboThorRGBPPOExperimentConfig # PointNavRoboThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in RoboTHOR with Depth input.","title":"pointnav_robothor_depth_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_depth_simpleconvgru_ddppo-source","text":"","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_depth_simpleconvgru_ddppo [source]"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_depth_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"PointNavRoboThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in RoboTHOR with Depth input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo [source] # PointNavRoboThorRGBPPOExperimentConfig # PointNavRoboThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in RoboThor with RGB input.","title":"pointnav_robothor_rgb_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgb_simpleconvgru_ddppo-source","text":"","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgb_simpleconvgru_ddppo [source]"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgb_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"PointNavRoboThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in RoboThor with RGB input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/","text":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgbd_simpleconvgru_ddppo [source] # PointNavRoboThorRGBPPOExperimentConfig # PointNavRoboThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in RoboThor with RGBD input.","title":"pointnav_robothor_rgbd_simpleconvgru_ddppo"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/#projectspointnav_baselinesexperimentsrobothorpointnav_robothor_rgbd_simpleconvgru_ddppo-source","text":"","title":"projects.pointnav_baselines.experiments.robothor.pointnav_robothor_rgbd_simpleconvgru_ddppo [source]"},{"location":"api/projects/pointnav_baselines/experiments/robothor/pointnav_robothor_rgbd_simpleconvgru_ddppo/#pointnavrobothorrgbppoexperimentconfig","text":"PointNavRoboThorRGBPPOExperimentConfig ( self ) An Point Navigation experiment configuration in RoboThor with RGBD input.","title":"PointNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/","text":"projects.pointnav_baselines.models.point_nav_models [source] # ResnetDualTensorGoalEncoder # ResnetDualTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : str , depth_resnet_preprocessor_uuid : str , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None get_object_type_encoding # ResnetDualTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. ResnetTensorGoalEncoder # ResnetTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None get_object_type_encoding # ResnetTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations. ResnetTensorPointNavActorCritic # ResnetTensorPointNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : Optional [ str ] = None , depth_resnet_preprocessor_uuid : Optional [ str ] = None , hidden_size : int = 512 , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) is_blind # True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type). num_recurrent_layers # Number of recurrent hidden layers. recurrent_hidden_state_size # The recurrent hidden state size of the model.","title":"point_nav_models"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#projectspointnav_baselinesmodelspoint_nav_models-source","text":"","title":"projects.pointnav_baselines.models.point_nav_models [source]"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnetdualtensorgoalencoder","text":"ResnetDualTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : str , depth_resnet_preprocessor_uuid : str , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None","title":"ResnetDualTensorGoalEncoder"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#get_object_type_encoding","text":"ResnetDualTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorgoalencoder","text":"ResnetTensorGoalEncoder ( self , observation_spaces : gym . spaces . dict . Dict , goal_sensor_uuid : str , resnet_preprocessor_uuid : str , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), ) -> None","title":"ResnetTensorGoalEncoder"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#get_object_type_encoding_1","text":"ResnetTensorGoalEncoder . get_object_type_encoding ( self , observations : Dict [ str , torch . FloatTensor ], ) -> torch . FloatTensor Get the object type encoding from input batched observations.","title":"get_object_type_encoding"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#resnettensorpointnavactorcritic","text":"ResnetTensorPointNavActorCritic ( self , action_space : gym . spaces . discrete . Discrete , observation_space : gym . spaces . dict . Dict , goal_sensor_uuid : str , rgb_resnet_preprocessor_uuid : Optional [ str ] = None , depth_resnet_preprocessor_uuid : Optional [ str ] = None , hidden_size : int = 512 , goal_dims : int = 32 , resnet_compressor_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), combiner_hidden_out_dims : Tuple [ int , int ] = ( 128 , 32 ), )","title":"ResnetTensorPointNavActorCritic"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#is_blind","text":"True if the model is blind (e.g. neither 'depth' or 'rgb' is an input observation type).","title":"is_blind"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#num_recurrent_layers","text":"Number of recurrent hidden layers.","title":"num_recurrent_layers"},{"location":"api/projects/pointnav_baselines/models/point_nav_models/#recurrent_hidden_state_size","text":"The recurrent hidden state size of the model.","title":"recurrent_hidden_state_size"},{"location":"api/projects/tutorials/minigrid_tutorial/","text":"projects.tutorials.minigrid_tutorial [source] # Experiment Config for MiniGrid tutorial. MiniGridTutorialExperimentConfig # MiniGridTutorialExperimentConfig ( self , / , * args , ** kwargs ) SENSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"minigrid_tutorial"},{"location":"api/projects/tutorials/minigrid_tutorial/#projectstutorialsminigrid_tutorial-source","text":"Experiment Config for MiniGrid tutorial.","title":"projects.tutorials.minigrid_tutorial [source]"},{"location":"api/projects/tutorials/minigrid_tutorial/#minigridtutorialexperimentconfig","text":"MiniGridTutorialExperimentConfig ( self , / , * args , ** kwargs )","title":"MiniGridTutorialExperimentConfig"},{"location":"api/projects/tutorials/minigrid_tutorial/#sensors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"SENSORS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/","text":"projects.tutorials.pointnav_ithor_rgb_ddppo [source] # ObjectNavRoboThorRGBPPOExperimentConfig # ObjectNavRoboThorRGBPPOExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration in RoboThor. ADVANCE_SCENE_ROLLOUT_PERIOD # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 CAMERA_HEIGHT # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 CAMERA_WIDTH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ENV_ARGS # dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) MAX_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 NUM_PROCESSES # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 OBSERVATIONS # list() -> new empty list list(iterable) -> new list initialized from iterable's items PREPROCESSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items REWARD_CONFIG # dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) SCREEN_SIZE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SENSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items TESTING_GPUS # list() -> new empty list list(iterable) -> new list initialized from iterable's items TRAIN_DATASET_DIR # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. TRAINING_GPUS # list() -> new empty list list(iterable) -> new list initialized from iterable's items VAL_DATASET_DIR # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. VALIDATION_GPUS # list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"pointnav_ithor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#projectstutorialspointnav_ithor_rgb_ddppo-source","text":"","title":"projects.tutorials.pointnav_ithor_rgb_ddppo [source]"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"ObjectNavRoboThorRGBPPOExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration in RoboThor.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#advance_scene_rollout_period","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ADVANCE_SCENE_ROLLOUT_PERIOD"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#camera_height","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_HEIGHT"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#camera_width","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_WIDTH"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#env_args","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"ENV_ARGS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#max_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"MAX_STEPS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#num_processes","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_PROCESSES"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#observations","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"OBSERVATIONS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#preprocessors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"PREPROCESSORS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#reward_config","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"REWARD_CONFIG"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#screen_size","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"SCREEN_SIZE"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#sensors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"SENSORS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#testing_gpus","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"TESTING_GPUS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#train_dataset_dir","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"TRAIN_DATASET_DIR"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#training_gpus","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"TRAINING_GPUS"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#val_dataset_dir","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"VAL_DATASET_DIR"},{"location":"api/projects/tutorials/pointnav_ithor_rgb_ddppo/#validation_gpus","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"VALIDATION_GPUS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/","text":"projects.tutorials.pointnav_robothor_rgb_ddppo [source] # ObjectNavRoboThorRGBPPOExperimentConfig # ObjectNavRoboThorRGBPPOExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration in RoboThor. ADVANCE_SCENE_ROLLOUT_PERIOD # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 CAMERA_HEIGHT # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 CAMERA_WIDTH # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 ENV_ARGS # dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) MAX_STEPS # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 NUM_PROCESSES # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 OBSERVATIONS # list() -> new empty list list(iterable) -> new list initialized from iterable's items PREPROCESSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items REWARD_CONFIG # dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2) SCREEN_SIZE # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 SENSORS # list() -> new empty list list(iterable) -> new list initialized from iterable's items TESTING_GPUS # list() -> new empty list list(iterable) -> new list initialized from iterable's items TRAIN_DATASET_DIR # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. TRAINING_GPUS # list() -> new empty list list(iterable) -> new list initialized from iterable's items VAL_DATASET_DIR # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. VALIDATION_GPUS # list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"pointnav_robothor_rgb_ddppo"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#projectstutorialspointnav_robothor_rgb_ddppo-source","text":"","title":"projects.tutorials.pointnav_robothor_rgb_ddppo [source]"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#objectnavrobothorrgbppoexperimentconfig","text":"ObjectNavRoboThorRGBPPOExperimentConfig ( self , / , * args , ** kwargs ) A Point Navigation experiment configuration in RoboThor.","title":"ObjectNavRoboThorRGBPPOExperimentConfig"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#advance_scene_rollout_period","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"ADVANCE_SCENE_ROLLOUT_PERIOD"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#camera_height","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_HEIGHT"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#camera_width","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"CAMERA_WIDTH"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#env_args","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"ENV_ARGS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#max_steps","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"MAX_STEPS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#num_processes","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NUM_PROCESSES"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#observations","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"OBSERVATIONS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#preprocessors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"PREPROCESSORS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#reward_config","text":"dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)","title":"REWARD_CONFIG"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#screen_size","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"SCREEN_SIZE"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#sensors","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"SENSORS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#testing_gpus","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"TESTING_GPUS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#train_dataset_dir","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"TRAIN_DATASET_DIR"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#training_gpus","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"TRAINING_GPUS"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#val_dataset_dir","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"VAL_DATASET_DIR"},{"location":"api/projects/tutorials/pointnav_robothor_rgb_ddppo/#validation_gpus","text":"list() -> new empty list list(iterable) -> new list initialized from iterable's items","title":"VALIDATION_GPUS"},{"location":"api/tests/multiprocessing/test_frozen_attribs/","text":"tests.multiprocessing.test_frozen_attribs [source] # MyConfig # MyConfig ( self , / , * args , ** kwargs ) MY_VAR # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 MySpecConfig # MySpecConfig ( self , / , * args , ** kwargs ) MY_VAR # int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"test_frozen_attribs"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#testsmultiprocessingtest_frozen_attribs-source","text":"","title":"tests.multiprocessing.test_frozen_attribs [source]"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#myconfig","text":"MyConfig ( self , / , * args , ** kwargs )","title":"MyConfig"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#my_var","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"MY_VAR"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#myspecconfig","text":"MySpecConfig ( self , / , * args , ** kwargs )","title":"MySpecConfig"},{"location":"api/tests/multiprocessing/test_frozen_attribs/#my_var_1","text":"int(x=0) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"MY_VAR"},{"location":"api/tests/sync_algs_cpu/test_to_to_obj_trains/","text":"tests.sync_algs_cpu.test_to_to_obj_trains [source] #","title":"test_to_to_obj_trains"},{"location":"api/tests/sync_algs_cpu/test_to_to_obj_trains/#testssync_algs_cputest_to_to_obj_trains-source","text":"","title":"tests.sync_algs_cpu.test_to_to_obj_trains [source]"},{"location":"api/utils/cache_utils/","text":"utils.cache_utils [source] #","title":"cache_utils"},{"location":"api/utils/cache_utils/#utilscache_utils-source","text":"","title":"utils.cache_utils [source]"},{"location":"api/utils/cacheless_frcnn/","text":"utils.cacheless_frcnn [source] #","title":"cacheless_frcnn"},{"location":"api/utils/cacheless_frcnn/#utilscacheless_frcnn-source","text":"","title":"utils.cacheless_frcnn [source]"},{"location":"api/utils/experiment_utils/","text":"utils.experiment_utils [source] # Utility classes and functions for running and designing experiments. Builder # Builder ( self , / , * args , ** kwargs ) Used to instantiate a given class with (default) parameters. Helper class that stores a class, default parameters for that class, and key word arguments that (possibly) overwrite the defaults. When calling this an object of the Builder class it generates a class of type class_type with parameters specified by the attributes default and kwargs (and possibly additional, overwriting, keyword arguments). Attributes class_type : The class to be instantiated when calling the object. kwargs : Keyword arguments used to instantiate an object of type class_type . default : Default parameters used when instantiating the class. EarlyStoppingCriterion # EarlyStoppingCriterion ( self , / , * args , ** kwargs ) Abstract class for class who determines if training should stop early in a particular pipeline stage. LinearDecay # LinearDecay ( self , steps : int , startp : float = 1.0 , endp : float = 0.0 ) -> None Linearly decay between two values over some number of steps. Obtain the value corresponding to the i th step by calling an instantiation of this object with the value i . Parameters steps : The number of steps over which to decay. startp : The starting value. endp : The ending value. NeverEarlyStoppingCriterion # NeverEarlyStoppingCriterion ( self , / , * args , ** kwargs ) Implementation of EarlyStoppingCriterion which never stops early. OffPolicyPipelineComponent # OffPolicyPipelineComponent ( self , / , * args , ** kwargs ) OffPolicyPipelineComponent(data_iterator_builder, loss_names, updates, loss_weights, data_iterator_kwargs_generator) data_iterator_builder # Alias for field number 0 data_iterator_kwargs_generator # Alias for field number 4 loss_names # Alias for field number 1 loss_weights # Alias for field number 3 updates # Alias for field number 2 PipelineStage # PipelineStage ( self , loss_names : List [ str ], max_stage_steps : Union [ int , Callable ], early_stopping_criterion : Optional [ utils . experiment_utils . EarlyStoppingCriterion ] = None , loss_weights : Optional [ Sequence [ float ]] = None , teacher_forcing : Optional [ utils . experiment_utils . LinearDecay ] = None , offpolicy_component : Optional [ utils . experiment_utils . OffPolicyPipelineComponent ] = None , ) A single stage in a training pipeline. Attributes loss_name : A collection of unique names assigned to losses. These will reference the Loss objects in a TrainingPipeline instance. max_stage_steps : Either the total number of steps agents should take in this stage or a Callable object (e.g. a function) early_stopping_criterion : An EarlyStoppingCriterion object which determines if training in this stage should be stopped early. If None then no early stopping occurs. If early_stopping_criterion is not None then we do not guarantee reproducibility when restarting a model from a checkpoint (as the EarlyStoppingCriterion object may store internal state which is not saved in the checkpoint). loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_name . Should be the same length as loss_name . If this is None , all weights will be assumed to be one. teacher_forcing : If applicable, defines the probability an agent will take the expert action (as opposed to its own sampled action) at a given time point. recursive_update # recursive_update ( original : collections . abc . MutableMapping , update : collections . abc . MutableMapping , ) Recursively updates original dictionary with entries form update dict. Parameters original : Original dictionary to be updated. update : Dictionary with additional or replacement entries. Returns Updated original dictionary. ScalarMeanTracker # ScalarMeanTracker ( self ) -> None Track a collection scalar key -> mean pairs. add_scalars # ScalarMeanTracker . add_scalars ( self , scalars : Dict [ str , Union [ float , int ]], n : int = 1 , ) -> None Add additional scalars to track. Parameters scalars : A dictionary of scalar key -> value pairs. pop_and_reset # ScalarMeanTracker . pop_and_reset ( self ) -> Dict [ str , float ] Return tracked means and reset. On resetting all previously tracked values are discarded. Returns A dictionary of scalar key -> current mean pairs corresponding to those values added with add_scalars . set_deterministic_cudnn # set_deterministic_cudnn () -> None Makes cudnn deterministic. This may slow down computations. set_seed # set_seed ( seed : Union [ int , NoneType ] = None ) -> None Set seeds for multiple (cpu) sources of randomness. Sets seeds for (cpu) pytorch , base random , and numpy . Parameters seed : The seed to set. If set to None, keep using the current seed. TrainingPipeline # TrainingPipeline ( self , named_losses : Dict [ str , Union [ core . base_abstractions . misc . Loss , utils . experiment_utils . Builder [ core . base_abstractions . misc . Loss ]]], pipeline_stages : List [ utils . experiment_utils . PipelineStage ], optimizer_builder : utils . experiment_utils . Builder , num_mini_batch : int , update_repeats : int , max_grad_norm : float , num_steps : int , gamma : float , use_gae : bool , gae_lambda : float , advance_scene_rollout_period : Optional [ int ], save_interval : Optional [ int ], metric_accumulate_interval : int , should_log : bool = True , lr_scheduler_builder : Optional [ utils . experiment_utils . Builder [ torch . optim . lr_scheduler . _LRScheduler ]] = None , ) Class defining the stages (and global parameters) in a training pipeline. The training pipeline can be used as an iterator to go through the pipeline stages in, for instance, a loop. Attributes named_losses : Dictionary mapping a the name of a loss to either an instantiation of that loss or a Builder that, when called, will return that loss. pipeline_stages : A list of PipelineStages. Each of these define how the agent will be trained and are executed sequentially. optimizer_builder : Builder object to instantiate the optimizer to use during training. num_mini_batch : The number of mini-batches to break a rollout into. update_repeats : The number of times we will cycle through the mini-batches corresponding to a single rollout doing gradient updates. max_grad_norm : The maximum \"inf\" norm of any gradient step (gradients are clipped to not exceed this). num_steps : Total number of steps a single agent takes in a rollout. gamma : Discount factor applied to rewards (should be in [0, 1]). use_gae : Whether or not to use generalized advantage estimation (GAE). gae_lambda : The additional parameter used in GAE. save_interval : The frequency with which to save (in total agent steps taken). If None then no checkpoints will be saved. Otherwise, in addition to the checkpoints being saved every save_interval steps, a checkpoint will always be saved at the end of each pipeline stage. If save_interval <= 0 then checkpoints will only be saved at the end of each pipeline stage. metric_accumulate_interval : The frequency with which training/validation metrics are accumulated (in total agent steps). Metrics accumulated in an interval are logged (if should_log is True ) and used by the stage's early stopping criterion (if any). should_log : True if metrics accumulated during training should be logged to the console as well as to a tensorboard file. lr_scheduler_builder : Optional builder object to instantiate the learning rate scheduler used through the pipeline.","title":"experiment_utils"},{"location":"api/utils/experiment_utils/#utilsexperiment_utils-source","text":"Utility classes and functions for running and designing experiments.","title":"utils.experiment_utils [source]"},{"location":"api/utils/experiment_utils/#builder","text":"Builder ( self , / , * args , ** kwargs ) Used to instantiate a given class with (default) parameters. Helper class that stores a class, default parameters for that class, and key word arguments that (possibly) overwrite the defaults. When calling this an object of the Builder class it generates a class of type class_type with parameters specified by the attributes default and kwargs (and possibly additional, overwriting, keyword arguments). Attributes class_type : The class to be instantiated when calling the object. kwargs : Keyword arguments used to instantiate an object of type class_type . default : Default parameters used when instantiating the class.","title":"Builder"},{"location":"api/utils/experiment_utils/#earlystoppingcriterion","text":"EarlyStoppingCriterion ( self , / , * args , ** kwargs ) Abstract class for class who determines if training should stop early in a particular pipeline stage.","title":"EarlyStoppingCriterion"},{"location":"api/utils/experiment_utils/#lineardecay","text":"LinearDecay ( self , steps : int , startp : float = 1.0 , endp : float = 0.0 ) -> None Linearly decay between two values over some number of steps. Obtain the value corresponding to the i th step by calling an instantiation of this object with the value i . Parameters steps : The number of steps over which to decay. startp : The starting value. endp : The ending value.","title":"LinearDecay"},{"location":"api/utils/experiment_utils/#neverearlystoppingcriterion","text":"NeverEarlyStoppingCriterion ( self , / , * args , ** kwargs ) Implementation of EarlyStoppingCriterion which never stops early.","title":"NeverEarlyStoppingCriterion"},{"location":"api/utils/experiment_utils/#offpolicypipelinecomponent","text":"OffPolicyPipelineComponent ( self , / , * args , ** kwargs ) OffPolicyPipelineComponent(data_iterator_builder, loss_names, updates, loss_weights, data_iterator_kwargs_generator)","title":"OffPolicyPipelineComponent"},{"location":"api/utils/experiment_utils/#data_iterator_builder","text":"Alias for field number 0","title":"data_iterator_builder"},{"location":"api/utils/experiment_utils/#data_iterator_kwargs_generator","text":"Alias for field number 4","title":"data_iterator_kwargs_generator"},{"location":"api/utils/experiment_utils/#loss_names","text":"Alias for field number 1","title":"loss_names"},{"location":"api/utils/experiment_utils/#loss_weights","text":"Alias for field number 3","title":"loss_weights"},{"location":"api/utils/experiment_utils/#updates","text":"Alias for field number 2","title":"updates"},{"location":"api/utils/experiment_utils/#pipelinestage","text":"PipelineStage ( self , loss_names : List [ str ], max_stage_steps : Union [ int , Callable ], early_stopping_criterion : Optional [ utils . experiment_utils . EarlyStoppingCriterion ] = None , loss_weights : Optional [ Sequence [ float ]] = None , teacher_forcing : Optional [ utils . experiment_utils . LinearDecay ] = None , offpolicy_component : Optional [ utils . experiment_utils . OffPolicyPipelineComponent ] = None , ) A single stage in a training pipeline. Attributes loss_name : A collection of unique names assigned to losses. These will reference the Loss objects in a TrainingPipeline instance. max_stage_steps : Either the total number of steps agents should take in this stage or a Callable object (e.g. a function) early_stopping_criterion : An EarlyStoppingCriterion object which determines if training in this stage should be stopped early. If None then no early stopping occurs. If early_stopping_criterion is not None then we do not guarantee reproducibility when restarting a model from a checkpoint (as the EarlyStoppingCriterion object may store internal state which is not saved in the checkpoint). loss_weights : A list of floating point numbers describing the relative weights applied to the losses referenced by loss_name . Should be the same length as loss_name . If this is None , all weights will be assumed to be one. teacher_forcing : If applicable, defines the probability an agent will take the expert action (as opposed to its own sampled action) at a given time point.","title":"PipelineStage"},{"location":"api/utils/experiment_utils/#recursive_update","text":"recursive_update ( original : collections . abc . MutableMapping , update : collections . abc . MutableMapping , ) Recursively updates original dictionary with entries form update dict. Parameters original : Original dictionary to be updated. update : Dictionary with additional or replacement entries. Returns Updated original dictionary.","title":"recursive_update"},{"location":"api/utils/experiment_utils/#scalarmeantracker","text":"ScalarMeanTracker ( self ) -> None Track a collection scalar key -> mean pairs.","title":"ScalarMeanTracker"},{"location":"api/utils/experiment_utils/#add_scalars","text":"ScalarMeanTracker . add_scalars ( self , scalars : Dict [ str , Union [ float , int ]], n : int = 1 , ) -> None Add additional scalars to track. Parameters scalars : A dictionary of scalar key -> value pairs.","title":"add_scalars"},{"location":"api/utils/experiment_utils/#pop_and_reset","text":"ScalarMeanTracker . pop_and_reset ( self ) -> Dict [ str , float ] Return tracked means and reset. On resetting all previously tracked values are discarded. Returns A dictionary of scalar key -> current mean pairs corresponding to those values added with add_scalars .","title":"pop_and_reset"},{"location":"api/utils/experiment_utils/#set_deterministic_cudnn","text":"set_deterministic_cudnn () -> None Makes cudnn deterministic. This may slow down computations.","title":"set_deterministic_cudnn"},{"location":"api/utils/experiment_utils/#set_seed","text":"set_seed ( seed : Union [ int , NoneType ] = None ) -> None Set seeds for multiple (cpu) sources of randomness. Sets seeds for (cpu) pytorch , base random , and numpy . Parameters seed : The seed to set. If set to None, keep using the current seed.","title":"set_seed"},{"location":"api/utils/experiment_utils/#trainingpipeline","text":"TrainingPipeline ( self , named_losses : Dict [ str , Union [ core . base_abstractions . misc . Loss , utils . experiment_utils . Builder [ core . base_abstractions . misc . Loss ]]], pipeline_stages : List [ utils . experiment_utils . PipelineStage ], optimizer_builder : utils . experiment_utils . Builder , num_mini_batch : int , update_repeats : int , max_grad_norm : float , num_steps : int , gamma : float , use_gae : bool , gae_lambda : float , advance_scene_rollout_period : Optional [ int ], save_interval : Optional [ int ], metric_accumulate_interval : int , should_log : bool = True , lr_scheduler_builder : Optional [ utils . experiment_utils . Builder [ torch . optim . lr_scheduler . _LRScheduler ]] = None , ) Class defining the stages (and global parameters) in a training pipeline. The training pipeline can be used as an iterator to go through the pipeline stages in, for instance, a loop. Attributes named_losses : Dictionary mapping a the name of a loss to either an instantiation of that loss or a Builder that, when called, will return that loss. pipeline_stages : A list of PipelineStages. Each of these define how the agent will be trained and are executed sequentially. optimizer_builder : Builder object to instantiate the optimizer to use during training. num_mini_batch : The number of mini-batches to break a rollout into. update_repeats : The number of times we will cycle through the mini-batches corresponding to a single rollout doing gradient updates. max_grad_norm : The maximum \"inf\" norm of any gradient step (gradients are clipped to not exceed this). num_steps : Total number of steps a single agent takes in a rollout. gamma : Discount factor applied to rewards (should be in [0, 1]). use_gae : Whether or not to use generalized advantage estimation (GAE). gae_lambda : The additional parameter used in GAE. save_interval : The frequency with which to save (in total agent steps taken). If None then no checkpoints will be saved. Otherwise, in addition to the checkpoints being saved every save_interval steps, a checkpoint will always be saved at the end of each pipeline stage. If save_interval <= 0 then checkpoints will only be saved at the end of each pipeline stage. metric_accumulate_interval : The frequency with which training/validation metrics are accumulated (in total agent steps). Metrics accumulated in an interval are logged (if should_log is True ) and used by the stage's early stopping criterion (if any). should_log : True if metrics accumulated during training should be logged to the console as well as to a tensorboard file. lr_scheduler_builder : Optional builder object to instantiate the learning rate scheduler used through the pipeline.","title":"TrainingPipeline"},{"location":"api/utils/misc_utils/","text":"utils.misc_utils [source] # HashableDict # HashableDict ( self , * args , ** kwargs ) A dictionary which is hashable so long as all of its values are hashable. A HashableDict object will allow setting / deleting of items until the first time that __hash__() is called on it after which attempts to set or delete items will throw RuntimeError exceptions.","title":"misc_utils"},{"location":"api/utils/misc_utils/#utilsmisc_utils-source","text":"","title":"utils.misc_utils [source]"},{"location":"api/utils/misc_utils/#hashabledict","text":"HashableDict ( self , * args , ** kwargs ) A dictionary which is hashable so long as all of its values are hashable. A HashableDict object will allow setting / deleting of items until the first time that __hash__() is called on it after which attempts to set or delete items will throw RuntimeError exceptions.","title":"HashableDict"},{"location":"api/utils/model_utils/","text":"utils.model_utils [source] # Functions used to initialize and manipulate pytorch models. compute_cnn_output # compute_cnn_output ( cnn : torch . nn . modules . module . Module , cnn_input : torch . Tensor , permute_order : Optional [ Tuple [ int , ... ]] = ( 0 , 3 , 1 , 2 ), ) Computes CNN outputs for given inputs. Parameters cnn : A torch CNN. cnn_input : A torch Tensor with inputs. permute_order : A permutation Tuple to provide PyTorch dimension order, default (0, 3, 1, 2), where 0 corresponds to the flattened batch dimensions (combining step, sampler and agent) Returns CNN output with dimensions [STEP, SAMPLER, AGENT, CHANNEL, (HEIGHT, WIDTH)]. Flatten # Flatten ( self ) Flatten input tensor so that it is of shape (FLATTENED_BATCH x -1). forward # Flatten . forward ( self , x ) Flatten input tensor. Parameters x : Tensor of size (FLATTENED_BATCH x ...) to flatten to size (FLATTENED_BATCH x -1) Returns Flattened tensor. init_linear_layer # init_linear_layer ( module : torch . nn . modules . linear . Linear , weight_init : collections . abc . Callable , bias_init : collections . abc . Callable , gain = 1 , ) Initialize a torch.nn.Linear layer. Parameters module : A torch linear layer. weight_init : Function used to initialize the weight parameters of the linear layer. Should take the weight data tensor and gain as input. bias_init : Function used to initialize the bias parameters of the linear layer. Should take the bias data tensor and gain as input. gain : The gain to apply. Returns The initialized linear layer.","title":"model_utils"},{"location":"api/utils/model_utils/#utilsmodel_utils-source","text":"Functions used to initialize and manipulate pytorch models.","title":"utils.model_utils [source]"},{"location":"api/utils/model_utils/#compute_cnn_output","text":"compute_cnn_output ( cnn : torch . nn . modules . module . Module , cnn_input : torch . Tensor , permute_order : Optional [ Tuple [ int , ... ]] = ( 0 , 3 , 1 , 2 ), ) Computes CNN outputs for given inputs. Parameters cnn : A torch CNN. cnn_input : A torch Tensor with inputs. permute_order : A permutation Tuple to provide PyTorch dimension order, default (0, 3, 1, 2), where 0 corresponds to the flattened batch dimensions (combining step, sampler and agent) Returns CNN output with dimensions [STEP, SAMPLER, AGENT, CHANNEL, (HEIGHT, WIDTH)].","title":"compute_cnn_output"},{"location":"api/utils/model_utils/#flatten","text":"Flatten ( self ) Flatten input tensor so that it is of shape (FLATTENED_BATCH x -1).","title":"Flatten"},{"location":"api/utils/model_utils/#forward","text":"Flatten . forward ( self , x ) Flatten input tensor. Parameters x : Tensor of size (FLATTENED_BATCH x ...) to flatten to size (FLATTENED_BATCH x -1) Returns Flattened tensor.","title":"forward"},{"location":"api/utils/model_utils/#init_linear_layer","text":"init_linear_layer ( module : torch . nn . modules . linear . Linear , weight_init : collections . abc . Callable , bias_init : collections . abc . Callable , gain = 1 , ) Initialize a torch.nn.Linear layer. Parameters module : A torch linear layer. weight_init : Function used to initialize the weight parameters of the linear layer. Should take the weight data tensor and gain as input. bias_init : Function used to initialize the bias parameters of the linear layer. Should take the bias data tensor and gain as input. gain : The gain to apply. Returns The initialized linear layer.","title":"init_linear_layer"},{"location":"api/utils/system/","text":"utils.system [source] #","title":"system"},{"location":"api/utils/system/#utilssystem-source","text":"","title":"utils.system [source]"},{"location":"api/utils/tensor_utils/","text":"utils.tensor_utils [source] # Functions used to manipulate pytorch tensors and numpy arrays. batch_observations # batch_observations ( observations : List [ Dict ], device : Optional [ torch . device ] = None , ) -> Dict [ str , Union [ Dict , torch . Tensor ]] Transpose a batch of observation dicts to a dict of batched observations. Arguments observations : List of dicts of observations. device : The torch.device to put the resulting tensors on. Will not move the tensors if None. Returns Transposed dict of lists of observations. detach_recursively # detach_recursively ( input : Any , inplace = True ) Recursively detaches tensors in some data structure from their computation graph. image # image ( tag , tensor , rescale = 1 , dataformats = 'CHW' ) Outputs a Summary protocol buffer with images. The summary has up to max_images summary values containing images. The images are built from tensor which must be 3-D with shape [height, width, channels] and where channels can be: 1: tensor is interpreted as Grayscale. 3: tensor is interpreted as RGB. 4: tensor is interpreted as RGBA. Args: tag: A name for the generated node. Will also serve as a series name in TensorBoard. tensor: A 3-D uint8 or float32 Tensor of shape [height, width, channels] where channels is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). Returns: A scalar Tensor of type string . The serialized Summary protocol buffer. ScaleBothSides # ScaleBothSides ( self , width : int , height : int , interpolation = 2 ) Rescales the input PIL.Image to the given 'width' and height . Attributes width: new width height: new height interpolation: Default: PIL.Image.BILINEAR tile_images # tile_images ( images : List [ numpy . ndarray ]) -> numpy . ndarray Tile multiple images into single image. Parameters images : list of images where each image has dimension (height x width x channels) Returns Tiled image (new_height x width x channels). to_device_recursively # to_device_recursively ( input : Any , device : Union [ str , torch . device , int ], inplace : bool = True , ) Recursively places tensors on the appropriate device. to_tensor # to_tensor ( v ) -> torch . Tensor Return a torch.Tensor version of the input. Parameters v : Input values that can be coerced into being a tensor. Returns A tensor version of the input.","title":"tensor_utils"},{"location":"api/utils/tensor_utils/#utilstensor_utils-source","text":"Functions used to manipulate pytorch tensors and numpy arrays.","title":"utils.tensor_utils [source]"},{"location":"api/utils/tensor_utils/#batch_observations","text":"batch_observations ( observations : List [ Dict ], device : Optional [ torch . device ] = None , ) -> Dict [ str , Union [ Dict , torch . Tensor ]] Transpose a batch of observation dicts to a dict of batched observations. Arguments observations : List of dicts of observations. device : The torch.device to put the resulting tensors on. Will not move the tensors if None. Returns Transposed dict of lists of observations.","title":"batch_observations"},{"location":"api/utils/tensor_utils/#detach_recursively","text":"detach_recursively ( input : Any , inplace = True ) Recursively detaches tensors in some data structure from their computation graph.","title":"detach_recursively"},{"location":"api/utils/tensor_utils/#image","text":"image ( tag , tensor , rescale = 1 , dataformats = 'CHW' ) Outputs a Summary protocol buffer with images. The summary has up to max_images summary values containing images. The images are built from tensor which must be 3-D with shape [height, width, channels] and where channels can be: 1: tensor is interpreted as Grayscale. 3: tensor is interpreted as RGB. 4: tensor is interpreted as RGBA. Args: tag: A name for the generated node. Will also serve as a series name in TensorBoard. tensor: A 3-D uint8 or float32 Tensor of shape [height, width, channels] where channels is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). Returns: A scalar Tensor of type string . The serialized Summary protocol buffer.","title":"image"},{"location":"api/utils/tensor_utils/#scalebothsides","text":"ScaleBothSides ( self , width : int , height : int , interpolation = 2 ) Rescales the input PIL.Image to the given 'width' and height . Attributes width: new width height: new height interpolation: Default: PIL.Image.BILINEAR","title":"ScaleBothSides"},{"location":"api/utils/tensor_utils/#tile_images","text":"tile_images ( images : List [ numpy . ndarray ]) -> numpy . ndarray Tile multiple images into single image. Parameters images : list of images where each image has dimension (height x width x channels) Returns Tiled image (new_height x width x channels).","title":"tile_images"},{"location":"api/utils/tensor_utils/#to_device_recursively","text":"to_device_recursively ( input : Any , device : Union [ str , torch . device , int ], inplace : bool = True , ) Recursively places tensors on the appropriate device.","title":"to_device_recursively"},{"location":"api/utils/tensor_utils/#to_tensor","text":"to_tensor ( v ) -> torch . Tensor Return a torch.Tensor version of the input. Parameters v : Input values that can be coerced into being a tensor. Returns A tensor version of the input.","title":"to_tensor"},{"location":"api/utils/viz_utils/","text":"utils.viz_utils [source] #","title":"viz_utils"},{"location":"api/utils/viz_utils/#utilsviz_utils-source","text":"","title":"utils.viz_utils [source]"},{"location":"getting_started/abstractions/","text":"Primary abstractions # Our package relies on a collection of fundamental abstractions to define how, and in what task, the model should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstract (if relevant). The following provides a high-level description of how these abstractions interact. Experiment configuration # In allenact , experiments are definied by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation . Task sampler # A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation . Task # Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation . Sensor # Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation . Actor critic model # The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation . Training pipeline # The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied . Losses # Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Primary abstractions"},{"location":"getting_started/abstractions/#primary-abstractions","text":"Our package relies on a collection of fundamental abstractions to define how, and in what task, the model should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstract (if relevant). The following provides a high-level description of how these abstractions interact.","title":"Primary abstractions"},{"location":"getting_started/abstractions/#experiment-configuration","text":"In allenact , experiments are definied by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation .","title":"Experiment configuration"},{"location":"getting_started/abstractions/#task-sampler","text":"A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation .","title":"Task sampler"},{"location":"getting_started/abstractions/#task","text":"Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation .","title":"Task"},{"location":"getting_started/abstractions/#sensor","text":"Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation .","title":"Sensor"},{"location":"getting_started/abstractions/#actor-critic-model","text":"The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation .","title":"Actor critic model"},{"location":"getting_started/abstractions/#training-pipeline","text":"The training pipeline, defined in the ExperimentConfig 's training_pipeline method , contains one or more training stages where different losses can be combined or sequentially applied .","title":"Training pipeline"},{"location":"getting_started/abstractions/#losses","text":"Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the AbstractActorCriticLoss class and an example implementation . Off-policy losses implement generic training iterations in which a batch of data is run through a model (that can be a subgraph of an ActorCriticModel ) and a loss is computed on the model's output. See the AbstractOffPolicyLoss class and an example implementation .","title":"Losses"},{"location":"getting_started/running-your-first-experiment/","text":"Running your first experiment # Assuming you have installed all of the requirements , you can run your first experiment by calling python main.py minigrid_tutorial -b projects/tutorials -m 8 -o minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder minigrid_output will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir minigrid_output/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created designed can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with python main.py my_custom_experiment -b projects/tutorials .","title":"Run your first experiment"},{"location":"getting_started/running-your-first-experiment/#running-your-first-experiment","text":"Assuming you have installed all of the requirements , you can run your first experiment by calling python main.py minigrid_tutorial -b projects/tutorials -m 8 -o minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o minigrid_output we set the output folder. With -s 12345 we set the random seed. If everything was installed correctly, a simple model will be trained (and validated) in the MiniGrid environment and a new folder minigrid_output will be created containing: a checkpoints/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with model weight checkpoints, a used_configs/MiniGridTutorial/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/MiniGridTutorial/LOCAL_TIME_STR/ . Here LOCAL_TIME_STR is a string that records the time when the experiment was started (e.g. the string \"2020-08-21_18-19-47\" corresponds to an experiment started on August 21st 2020, 47 seconds past 6:19pm. If we have Tensorboard installed, we can track training progress with tensorboard --logdir minigrid_output/tb which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: A detailed tutorial describing how the minigrid_tutorial experiment configuration was created designed can be found here . To run your own custom experiment simply define a new experiment configuration in a file projects/YOUR_PROJECT_NAME/experiments/my_custom_experiment.py after which you may run it with python main.py my_custom_experiment -b projects/tutorials .","title":"Running your first experiment"},{"location":"getting_started/structure/","text":"Structure of the codebase # The codebase consists of the following directories: core , docs , plugins , projects , scripts , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized. core directory # Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. core.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. core.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. core.models includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations. docs directory # Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation. plugins directory # Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: 1. configs to host useful configuration for the environment or tasks. 1. data to store data to be consumed by the environment or tasks. 1. scripts to setup the plugin or gather and process data. projects directory # Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data. scripts directory # Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments with super-user privileges in Linux with NVIDIA drivers and xserver-xorg installed. tests directory # Includes implementations of tests. utils directory # It includes different types of utilities, mainly divided into: utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. utils.viz_utils , including a SimpleViz class that can be instantiated with different visualization plugins during inference. utils.system , including logging and networking helpers. Other utils files, including utils.misc_utils , contain a number of helper functions for different purposes.","title":"Structure of the codebase"},{"location":"getting_started/structure/#structure-of-the-codebase","text":"The codebase consists of the following directories: core , docs , plugins , projects , scripts , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized.","title":"Structure of the codebase"},{"location":"getting_started/structure/#core-directory","text":"Contains runtime algorithms for on-policy and off-policy training and inference, base abstractions used throughout the code base and basic models to be used as building blocks in future models. core.algorithms includes on-policy and off-policy training nd inference algorithms and abstractions for losses, policies, rollout storage, etc. core.base_abstractions includes the base ExperimentConfig , distributions, base Sensor , TaskSampler , Task , etc. core.models includes basic CNN, and RNN state encoders, besides basic ActorCriticModel implementations.","title":"core directory"},{"location":"getting_started/structure/#docs-directory","text":"Contains documentation for the framework, including guides for installation and first experiments, how-to's for the definition and usage of different abstractions, tutorials and per-project documentation.","title":"docs directory"},{"location":"getting_started/structure/#plugins-directory","text":"Contains implementations of ActorCriticModel s and Task s in different environments. Each plugin folder is named as {environment}_plugin and contains three subfolders: 1. configs to host useful configuration for the environment or tasks. 1. data to store data to be consumed by the environment or tasks. 1. scripts to setup the plugin or gather and process data.","title":"plugins directory"},{"location":"getting_started/structure/#projects-directory","text":"Contains project-specific code like experiment configurations and scripts to process results, generate visualizations or prepare data.","title":"projects directory"},{"location":"getting_started/structure/#scripts-directory","text":"Includes framework-wide scripts to build the documentation, format code, run_tests and start an xserver. The latter can be used for OpenGL-based environments with super-user privileges in Linux with NVIDIA drivers and xserver-xorg installed.","title":"scripts directory"},{"location":"getting_started/structure/#tests-directory","text":"Includes implementations of tests.","title":"tests directory"},{"location":"getting_started/structure/#utils-directory","text":"It includes different types of utilities, mainly divided into: utils.experiment_utils , including the TrainingPipeline , PipelineStage and other utilities to configure an experiment. utils.model_utils , including generic CNN creation, forward-pass helpers and other utilities. utils.tensor_utils , including functions to batch observations, convert tensors into video, scale image tensors, etc. utils.viz_utils , including a SimpleViz class that can be instantiated with different visualization plugins during inference. utils.system , including logging and networking helpers. Other utils files, including utils.misc_utils , contain a number of helper functions for different purposes.","title":"utils directory"},{"location":"howtos/changing-rewards-and-losses/","text":"Changing rewards and losses # In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level. Rewards # We will use the object navigation task in AI2-THOR as a running example. We can see how the ObjectNavTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward Losses # Currently we support A2C , PPO , and imitation losses. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . imitation . Imitation , ), \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Change rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#changing-rewards-and-losses","text":"In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level.","title":"Changing rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#rewards","text":"We will use the object navigation task in AI2-THOR as a running example. We can see how the ObjectNavTask._step(self, action: int) -> RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask ): def judge ( self ) -> float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward","title":"Rewards"},{"location":"howtos/changing-rewards-and-losses/#losses","text":"Currently we support A2C , PPO , and imitation losses. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { \"imitation_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . imitation . Imitation , ), \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"imitation_loss\" , \"ppo_loss\" ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps2 , ), ], )","title":"Losses"},{"location":"howtos/defining-a-new-model/","text":"Defining a new model # All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example. Actor-critic model interface # As an example, let's build an object navigation agent. class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , ): super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . recurrent_hidden_state_size ) + object_type_embedding_dim , self . recurrent_hidden_state_size , ) self . actor = LinearActorHead ( self . recurrent_hidden_state_size , action_space . n ) self . critic = LinearCriticHead ( self . recurrent_hidden_state_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ): return self . _hidden_size def forward ( self , observations , rnn_hidden_states , prev_actions , masks ): target_encoding = self . object_type_embedding ( observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x = torch . cat ( x , dim = 1 ) x , rnn_hidden_states = self . state_encoder ( x , rnn_hidden_states , masks ) return ( ActorCriticOutput ( distributions = self . actor ( x ), values = self . critic ( x ), extras = {} ), rnn_hidden_states , ) ... On-policy RL engine requirements # Apart from the interface expected by all actor-critic models, we also need to provide a utility function to allow the on-policy RL engine to properly initalize the rollouts storage, num_recurrent_layers : class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): ... @property def num_recurrent_layers ( self ): return self . state_encoder . num_recurrent_layers","title":"Define a new model"},{"location":"howtos/defining-a-new-model/#defining-a-new-model","text":"All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_memory_specification , returning a description of the model's recurrent memory; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example.","title":"Defining a new model"},{"location":"howtos/defining-a-new-model/#actor-critic-model-interface","text":"As an example, let's build an object navigation agent. class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , ): super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . recurrent_hidden_state_size ) + object_type_embedding_dim , self . recurrent_hidden_state_size , ) self . actor = LinearActorHead ( self . recurrent_hidden_state_size , action_space . n ) self . critic = LinearCriticHead ( self . recurrent_hidden_state_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ): return self . _hidden_size def forward ( self , observations , rnn_hidden_states , prev_actions , masks ): target_encoding = self . object_type_embedding ( observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x = torch . cat ( x , dim = 1 ) x , rnn_hidden_states = self . state_encoder ( x , rnn_hidden_states , masks ) return ( ActorCriticOutput ( distributions = self . actor ( x ), values = self . critic ( x ), extras = {} ), rnn_hidden_states , ) ...","title":"Actor-critic model interface"},{"location":"howtos/defining-a-new-model/#on-policy-rl-engine-requirements","text":"Apart from the interface expected by all actor-critic models, we also need to provide a utility function to allow the on-policy RL engine to properly initalize the rollouts storage, num_recurrent_layers : class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): ... @property def num_recurrent_layers ( self ): return self . state_encoder . num_recurrent_layers","title":"On-policy RL engine requirements"},{"location":"howtos/defining-a-new-task/","text":"Defining a new task # In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing. Task # Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition. Initialization, action space and termination # Let's start with the definition of the action space and task initialization: class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( 'MOVE_AHEAD' , 'ROTATE_LEFT' , 'ROTATE_RIGHT' , 'LOOK_DOWN' , 'LOOK_UP' , 'END' ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ... Step method # Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : int ) -> RLStepResult : action_str = self . class_action_names ()[ action ] if action_str == 'END' : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result def _is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Metrics, rendering and expert actions # Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self ) TaskSampler # We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR. Initialization and termination # class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env Task sampling # Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () object_types_in_scene = set ( [ o [ \"objectType\" ] for o in self . env . last_event . metadata [ \"objects\" ]] ) task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Define a new task"},{"location":"howtos/defining-a-new-task/#defining-a-new-task","text":"In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing.","title":"Defining a new task"},{"location":"howtos/defining-a-new-task/#task","text":"Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition.","title":"Task"},{"location":"howtos/defining-a-new-task/#initialization-action-space-and-termination","text":"Let's start with the definition of the action space and task initialization: class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( 'MOVE_AHEAD' , 'ROTATE_LEFT' , 'ROTATE_RIGHT' , 'LOOK_DOWN' , 'LOOK_UP' , 'END' ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) -> None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) -> Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) -> bool : return self . _took_end_action def close ( self ) -> None : self . env . stop () ...","title":"Initialization, action space and termination"},{"location":"howtos/defining-a-new-task/#step-method","text":"Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : int ) -> RLStepResult : action_str = self . class_action_names ()[ action ] if action_str == 'END' : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ \"action\" : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { \"last_action_success\" : self . last_action_success }, ) return step_result def _is_goal_object_visible ( self ) -> bool : return any ( o [ \"objectType\" ] == self . task_info [ \"object_type\" ] for o in self . env . visible_objects () ) def judge ( self ) -> float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward )","title":"Step method"},{"location":"howtos/defining-a-new-task/#metrics-rendering-and-expert-actions","text":"Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = \"rgb\" , * args , ** kwargs ) -> numpy . ndarray : assert mode == \"rgb\" , \"only rgb rendering is implemented\" return self . env . current_frame def metrics ( self ) -> Dict [ str , Any ]: if not self . is_done (): return {} else : return { \"success\" : self . _success , \"ep_length\" : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) -> Tuple [ int , bool ]: return my_objnav_expert_implementation ( self )","title":"Metrics, rendering and expert actions"},{"location":"howtos/defining-a-new-task/#tasksampler","text":"We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR.","title":"TaskSampler"},{"location":"howtos/defining-a-new-task/#initialization-and-termination","text":"class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) -> None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) -> None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) -> IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env","title":"Initialization and termination"},{"location":"howtos/defining-a-new-task/#task-sampling","text":"Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) -> Union [ int , float ]: return float ( \"inf\" ) @property def total_unique ( self ) -> Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) -> Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) -> bool : return True def next_task ( self ) -> Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () object_types_in_scene = set ( [ o [ \"objectType\" ] for o in self . env . last_event . metadata [ \"objects\" ]] ) task_info = { \"object_type\" : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Task sampling"},{"location":"howtos/defining-a-new-training-pipeline/","text":"Defining a new training pipeline # Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. On-policy # We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return TrainingPipeline ( named_losses = { \"ppo_loss\" : PPO ( ** PPOConfig ), }, optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , # Log after every step (in practice after every rollout) num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Imitation (), # We add an imitation loss. \"ppo_loss\" : PPO ( ** PPOConfig ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e5 ) ), ], ) Off-policy # We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Define a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#defining-a-new-training-pipeline","text":"Defining a new training pipeline, or even new learning algorithms, is straightforward with the modular design in AllenAct . A convenience Builder object allows us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers.","title":"Defining a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#on-policy","text":"We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return TrainingPipeline ( named_losses = { \"ppo_loss\" : PPO ( ** PPOConfig ), }, optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , # Log after every step (in practice after every rollout) num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complex pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows: class ObjectNavThorDaggerPPOExperimentConfig ( ExperimentConfig ): ... SENSORS = [ ... ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Imitation (), # We add an imitation loss. \"ppo_loss\" : PPO ( ** PPOConfig ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e5 ) ), ], )","title":"On-policy"},{"location":"howtos/defining-a-new-training-pipeline/#off-policy","text":"We can also define off-policy stages where an external dataset is used, in this case, for Behavior Cloning: class BCOffPolicyBabyAIGoToLocalExperimentConfig ( ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): total_train_steps = int ( 1e7 ) num_steps = 128 return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = 1 , optimizer_builder = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 0 , # no on-policy training update_repeats = 0 , # no on-policy training num_steps = num_steps // 4 , # rollouts from environment tasks named_losses = { \"offpolicy_expert_ce_loss\" : MiniGridOffPolicyExpertCELoss ( total_episodes_in_epoch = int ( 1e6 ) # dataset contains 1M episodes ), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , advance_scene_rollout_period = None , pipeline_stages = [ PipelineStage ( loss_names = [], # no on-policy losses max_stage_steps = total_train_steps , # We only train from off-policy data: offpolicy_component = OffPolicyPipelineComponent ( data_iterator_builder = lambda ** kwargs : create_minigrid_offpolicy_data_iterator ( path = DATASET_PATH , # external dataset nrollouts = 128 , # per trainer batch size rollout_len = num_steps , # For truncated-BTT instr_len = 5 , ** kwargs , ), loss_names = [ \"offpolicy_expert_ce_loss\" ], # off-policy losses updates = 16 , # 16 batches per rollout ), ), ], ) Note that, in this example, 128 / 4 = 32 steps will be sampled from tasks in a MiniGrid environment (which can be useful to track the agent's performance), while a subgraph of the model (in this case the entire Actor) is trained from batches of 128-step truncated episodes sampled from an offline dataset stored under DATASET_PATH .","title":"Off-policy"},{"location":"howtos/defining-an-experiment/","text":"Defining an experiment # Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. The interface to be implemented by the experiment specification is defined in core.base_abstractions.experiment_config . The first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ... Model creation # Next, create_model will be used to instantiate object navigation baseline actor-critic models : class ObjectNavThorExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... SCREEN_SIZE = 224 ... OBJECT_TYPES = sorted ([ \"Tomato\" ]) ... SENSORS = [ plugins . ithor_plugin . ithor_sensors . RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), plugins . ithor_plugin . ithor_sensors . GoalObjectTypeThorSensor ( { \"object_types\" : OBJECT_TYPES } ), ] @classmethod def create_model ( cls , ** kwargs ) -> torch . nn . Module : return models . object_nav_models . ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask . class_action_names ()) ), observation_space = core . base_abstractions . sensor . SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ... Training pipeline # In this section we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return utils . experiment_utils . TrainingPipeline ( named_losses = { \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, optimizer = utils . experiment_utils . Builder ( torch . optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks num_mini_batch = 1 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complicated pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), GoalObjectTypeThorSensor ({ \"object_types\" : OBJECT_TYPES }), ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks optimizer = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 if not torch . cuda . is_available () else 30 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Builder ( Imitation ,), # We add an imitation loss. \"ppo_loss\" : Builder ( PPO , default = PPOConfig ,), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e4 ) ), ], ) Note that, in order for the saved configs in the experiment output folder to be fully usable, we currently need to import the module with the parent experiment config relative to the current location. Machine configuration # In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): on_server = torch . cuda . is_available () if mode == \"train\" : nprocesses = 6 if not on_server else 20 gpu_ids = [] if not on_server else [ 0 ] elif mode == \"valid\" : nprocesses = 0 gpu_ids = [] if not on_server else [ 1 ] elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not on_server else [ 0 ] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.is_available() ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers. Task sampling # The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> core . base_abstractions . task . TaskSampler : return plugins . ithor_plugin . ithor_task_samplers . ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : typing . Optional [ typing . List [ int ]] = None , seeds : typing . Optional [ typing . List [ int ]] = None , deterministic_cudnn : bool = False , ) -> typing . Dict [ str , typing . Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = self . SCENE_PERIOD res [ \"env_args\" ][ \"x_display\" ] = None return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Define an experiment"},{"location":"howtos/defining-an-experiment/#defining-an-experiment","text":"Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. The interface to be implemented by the experiment specification is defined in core.base_abstractions.experiment_config . The first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): @classmethod def tag ( cls ): return \"ObjectNavThorPPO\" ...","title":"Defining an  experiment"},{"location":"howtos/defining-an-experiment/#model-creation","text":"Next, create_model will be used to instantiate object navigation baseline actor-critic models : class ObjectNavThorExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... SCREEN_SIZE = 224 ... OBJECT_TYPES = sorted ([ \"Tomato\" ]) ... SENSORS = [ plugins . ithor_plugin . ithor_sensors . RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), plugins . ithor_plugin . ithor_sensors . GoalObjectTypeThorSensor ( { \"object_types\" : OBJECT_TYPES } ), ] @classmethod def create_model ( cls , ** kwargs ) -> torch . nn . Module : return models . object_nav_models . ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( plugins . ithor_plugin . ithor_tasks . ObjectNavTask . class_action_names ()) ), observation_space = core . base_abstractions . sensor . SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = \"goal_object_type_ind\" , hidden_size = 512 , object_type_embedding_dim = 8 , ) ...","title":"Model creation"},{"location":"howtos/defining-an-experiment/#training-pipeline","text":"In this section we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return utils . experiment_utils . TrainingPipeline ( named_losses = { \"ppo_loss\" : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, optimizer = utils . experiment_utils . Builder ( torch . optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks num_mini_batch = 1 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complicated pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( { \"height\" : SCREEN_SIZE , \"width\" : SCREEN_SIZE , \"use_resnet_normalization\" : True , } ), GoalObjectTypeThorSensor ({ \"object_types\" : OBJECT_TYPES }), ExpertActionSensor ({ \"nactions\" : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks optimizer = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 if not torch . cuda . is_available () else 30 , update_repeats = 4 , num_steps = 128 , named_losses = { \"imitation_loss\" : Builder ( Imitation ,), # We add an imitation loss. \"ppo_loss\" : Builder ( PPO , default = PPOConfig ,), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ \"imitation_loss\" ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ \"ppo_loss\" , \"imitation_loss\" ], max_stage_steps = int ( 3e4 ) ), ], ) Note that, in order for the saved configs in the experiment output folder to be fully usable, we currently need to import the module with the parent experiment config relative to the current location.","title":"Training pipeline"},{"location":"howtos/defining-an-experiment/#machine-configuration","text":"In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ): on_server = torch . cuda . is_available () if mode == \"train\" : nprocesses = 6 if not on_server else 20 gpu_ids = [] if not on_server else [ 0 ] elif mode == \"valid\" : nprocesses = 0 gpu_ids = [] if not on_server else [ 1 ] elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not on_server else [ 0 ] else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.is_available() ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers.","title":"Machine configuration"},{"location":"howtos/defining-an-experiment/#task-sampling","text":"The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( core . base_abstractions . experiment_config . ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) -> core . base_abstractions . task . TaskSampler : return plugins . ithor_plugin . ithor_task_samplers . ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : typing . Optional [ typing . List [ int ]] = None , seeds : typing . Optional [ typing . List [ int ]] = None , deterministic_cudnn : bool = False , ) -> typing . Dict [ str , typing . Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_period\" ] = self . SCENE_PERIOD res [ \"env_args\" ][ \"x_display\" ] = None return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Task sampling"},{"location":"howtos/running-a-multi-agent-experiment/","text":"To-do #","title":"To-do"},{"location":"howtos/running-a-multi-agent-experiment/#to-do","text":"","title":"To-do"},{"location":"howtos/visualizing-results/","text":"To-do #","title":"To-do"},{"location":"howtos/visualizing-results/#to-do","text":"","title":"To-do"},{"location":"installation/download-datasets/","text":"Downloading datasets # TODO #","title":"Download datasets"},{"location":"installation/download-datasets/#downloading-datasets","text":"","title":"Downloading datasets"},{"location":"installation/download-datasets/#todo","text":"","title":"TODO"},{"location":"installation/installation-allenact/","text":"Installation of AllenAct # Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. In order to install requirements we recommend using pipenv but also include instructions if you would prefer to install things directly using pip . Installing requirements with pipenv ( recommended ) # If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev Installing requirements with pip # Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer using pip , you may install all requirements as follows pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above. Installing supported environments # We also provide installation instructions for the environments supported in AllenAct here .","title":"Install AllenAct"},{"location":"installation/installation-allenact/#installation-of-allenact","text":"Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. In order to install requirements we recommend using pipenv but also include instructions if you would prefer to install things directly using pip .","title":"Installation of AllenAct"},{"location":"installation/installation-allenact/#installing-requirements-with-pipenv-recommended","text":"If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev","title":"Installing requirements with pipenv (recommended)"},{"location":"installation/installation-allenact/#installing-requirements-with-pip","text":"Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer using pip , you may install all requirements as follows pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above.","title":"Installing requirements with pip"},{"location":"installation/installation-allenact/#installing-supported-environments","text":"We also provide installation instructions for the environments supported in AllenAct here .","title":"Installing supported environments"},{"location":"installation/installation-framework/","text":"Installation of supported environments # Below we provide installation instruction for a number of environments that we support. Installation of Minigrid # pip install gym-minigrid Note that gym-minigrid is listed a dependency of allenact and it will be automatically installed along with the framework. Installation of iTHOR # To install iTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install iTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine). Installation of RoboTHOR # To install RoboTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with RoboTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install RoboTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine). Installation of Habitat # To run experiments using Habitat please use our docker image using the following command: docker pull klemenkotar/allenact-habitat:latest This container includes the 0.1.0 release of allenact , the 0.1.5 release of habitat as well as the Gibson point navigation dataset (this is a set of start and goal positions provided by habitat). You then need to launch the container and attach into it: docker run klemenkotar/allenact-habitat --runtime = nvidia -it Once inside the container activate the conda environment: conda activate allenact From within the container download the Gibson scene files into the dataset folder using the instructions provided by the authors . Then proceed to run your experiments using allenact as you normally would.","title":"Install environments"},{"location":"installation/installation-framework/#installation-of-supported-environments","text":"Below we provide installation instruction for a number of environments that we support.","title":"Installation of supported environments"},{"location":"installation/installation-framework/#installation-of-minigrid","text":"pip install gym-minigrid Note that gym-minigrid is listed a dependency of allenact and it will be automatically installed along with the framework.","title":"Installation of Minigrid"},{"location":"installation/installation-framework/#installation-of-ithor","text":"To install iTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with iTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install iTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine).","title":"Installation of iTHOR"},{"location":"installation/installation-framework/#installation-of-robothor","text":"To install RoboTHOR for use with a machine with a screen you simply need to run: pip install ai2thor Note that ai2thor is listed a dependency of allenact and it will be automatically installed along with the framework. The first time you will run an experiment with RoboTHOR (or any script that uses ai2thor ) the library will download all of the assets it requires to render the scenes To install RoboTHOR for use with a machine without a screen (such as a remote server) you will also need to run a script that launches xserver with the following command: sudo python scripts/startx.py Notice that you need to run the command with sudo . If you do not have sudo access (for example if you are running this on a shared university machine) you can ask your administrator to run it for you. You only need to run it once (as long as you do not turn off your machine).","title":"Installation of RoboTHOR"},{"location":"installation/installation-framework/#installation-of-habitat","text":"To run experiments using Habitat please use our docker image using the following command: docker pull klemenkotar/allenact-habitat:latest This container includes the 0.1.0 release of allenact , the 0.1.5 release of habitat as well as the Gibson point navigation dataset (this is a set of start and goal positions provided by habitat). You then need to launch the container and attach into it: docker run klemenkotar/allenact-habitat --runtime = nvidia -it Once inside the container activate the conda environment: conda activate allenact From within the container download the Gibson scene files into the dataset folder using the instructions provided by the authors . Then proceed to run your experiments using allenact as you normally would.","title":"Installation of Habitat"},{"location":"notebooks/firstbook/","text":"To-do #","title":"To-do"},{"location":"notebooks/firstbook/#to-do","text":"","title":"To-do"},{"location":"projects/advisor_2020/","text":"Experiments for Advisor # TODO: # Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#experiments-for-advisor","text":"","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#todo","text":"Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"TODO:"},{"location":"projects/babyai_baselines/","text":"Baseline experiments for the BabyAI environment # We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"BabyAI baselines"},{"location":"projects/babyai_baselines/#baseline-experiments-for-the-babyai-environment","text":"We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python main.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp \"machine_params.gpu_id = 0\" to the above command.","title":"Baseline experiments for the BabyAI environment"},{"location":"projects/objectnav_baselines/","text":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments # TODO: # Add some details. Cite the RoboTHOR paper. Give a list of things you can run with bash commands","title":"ObjectNav baselines"},{"location":"projects/objectnav_baselines/#baseline-models-for-the-object-navigation-task-in-the-robothor-and-ithor-environments","text":"","title":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments"},{"location":"projects/objectnav_baselines/#todo","text":"Add some details. Cite the RoboTHOR paper. Give a list of things you can run with bash commands","title":"TODO:"},{"location":"projects/pointnav_baselines/","text":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments # TODO: # Add some details. Cite the Habitat, Navigation and RoboTHOR paper. Give a list of things you can run with bash commands.","title":"PointNav baselines"},{"location":"projects/pointnav_baselines/#baseline-models-for-the-point-navigation-task-in-the-habitat-robothor-and-ithor-environments","text":"","title":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments"},{"location":"projects/pointnav_baselines/#todo","text":"Add some details. Cite the Habitat, Navigation and RoboTHOR paper. Give a list of things you can run with bash commands.","title":"TODO:"},{"location":"projects/two_body_problem_2019/","text":"Experiments for the Two Body Problem paper # TODO: # Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#experiments-for-the-two-body-problem-paper","text":"","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#todo","text":"Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"TODO:"},{"location":"tutorials/","text":"AllenAct Tutorials # We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework. Navigation in MiniGrid # We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here. PointNav in RoboTHOR # We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here. Swapping in a new environment # This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"AllenAct Tutorials"},{"location":"tutorials/#allenact-tutorials","text":"We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework.","title":"AllenAct Tutorials"},{"location":"tutorials/#navigation-in-minigrid","text":"We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here.","title":"Navigation in MiniGrid"},{"location":"tutorials/#pointnav-in-robothor","text":"We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here.","title":"PointNav in RoboTHOR"},{"location":"tutorials/#swapping-in-a-new-environment","text":"This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"Swapping in a new environment"},{"location":"tutorials/minigrid-tutorial/","text":"Tutorial: Navigation in MiniGrid # In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known. The task # A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls. Experiment configuration file # Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc. Preliminaries # We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\" Sensors and Model # A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in https://github.com/maximecb/gym-minigrid#wrappers. The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , ) Task samplers # We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\" Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation. Machine parameters # Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"gpu_ids\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids . Training pipeline # The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = Builder ( PPO , kwargs = {}, default = PPOConfig ,)), pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known. Training and validation # We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o /PATH/TO/minigrid_output we set the output folder. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: Testing # The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python main.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Navigation in Minigrid"},{"location":"tutorials/minigrid-tutorial/#tutorial-navigation-in-minigrid","text":"In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known.","title":"Tutorial: Navigation in MiniGrid"},{"location":"tutorials/minigrid-tutorial/#the-task","text":"A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls.","title":"The task"},{"location":"tutorials/minigrid-tutorial/#experiment-configuration-file","text":"Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script main.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc.","title":"Experiment configuration file"},{"location":"tutorials/minigrid-tutorial/#preliminaries","text":"We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) -> str : return \"MiniGridTutorial\"","title":"Preliminaries"},{"location":"tutorials/minigrid-tutorial/#sensors-and-model","text":"A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in https://github.com/maximecb/gym-minigrid#wrappers. The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , )","title":"Sensors and Model"},{"location":"tutorials/minigrid-tutorial/#task-samplers","text":"We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"train\" ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"valid\" ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = \"test\" ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) -> Dict [ str , Any ]: \"\"\" Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` \"\"\" if mode == \"train\" : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == \"test\" ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation.","title":"Task samplers"},{"location":"tutorials/minigrid-tutorial/#machine-parameters","text":"Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = \"train\" , ** kwargs ) -> Dict [ str , Any ]: return { \"nprocesses\" : 128 if mode == \"train\" else 16 , \"gpu_ids\" : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids .","title":"Machine parameters"},{"location":"tutorials/minigrid-tutorial/#training-pipeline","text":"The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) -> TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = Builder ( PPO , kwargs = {}, default = PPOConfig ,)), pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known.","title":"Training pipeline"},{"location":"tutorials/minigrid-tutorial/#training-and-validation","text":"We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python main.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o /PATH/TO/minigrid_output we set the output folder. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to:","title":"Training and validation"},{"location":"tutorials/minigrid-tutorial/#testing","text":"The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python main.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Testing"},{"location":"tutorials/running_inference_on_a_pretrained_model/","text":"Tutorial: Inference with a pre-trained model #","title":"Using a pre-trained model"},{"location":"tutorials/running_inference_on_a_pretrained_model/#tutorial-inference-with-a-pre-trained-model","text":"","title":"Tutorial: Inference with a pre-trained model"},{"location":"tutorials/training-a-pointnav-model/","text":"Tutorial: PointNav in RoboTHOR # Introduction # One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short. Pointnav # At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the house to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings. What is an environment anyways? # Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment. Learning algorithm # Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While allenact offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward. Environemnt Setup # To setup the RoboTHOR environment please consult the installation guide . Dataset Setup # To train the model on the PointNav task, we need to download the dataset and precomputed cache of distances to the target. The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes. The precomputed cache of distances is a large dictionary containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). We can download and extract the data by navigating to the pointnav_baselines project and running the following script: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor Config File Setup # Now comes the most important part of the tutorial, we are going to write an experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. We will start by creating a new directory inside the projects directory. We can name this whatever we want but for now, we will go with robothor_pointnav_tutuorial . Then we can create a directory called experiments inside the new directory we just created. This hierarchy is not necessary but it helps keep our experiments neatly organized. Now we create a file called pointnav_robothor_rgb_ddppo inside the experiments folder (again the name of this file is arbitrary). We start off by importing everything we will need: import glob from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) from plugins.ithor_plugin.ithor_sensors import RGBSensorThor from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.preprocessor import ObservationSet from core.base_abstractions.task import TaskSampler from plugins.habitat_plugin.habitat_preprocessors import ResnetPreProcessorHabitat from plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from plugins.robothor_plugin.robothor_task_samplers import PointNavDatasetTaskSampler from plugins.robothor_plugin.robothor_tasks import PointNavTask from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Next we define a new experiment config class: class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD = 10 ** 13 NUM_PROCESSES = 60 TRAINING_GPUS = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] VALIDATION_GPUS = [ 7 ] TESTING_GPUS = [ 7 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/val\" Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In allenact the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , \"parallel\" : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES >= ndevices , \"NUM_PROCESSES {} < ndevices\" . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = \"train\" , ** kwargs ): if mode == \"train\" : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS render_video = False elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS render_video = False elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS render_video = False else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) # Disable parallelization for validation process if mode == \"valid\" : for prep in self . PREPROCESSORS : prep . kwargs [ \"parallel\" ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == 'train' or nprocesses > 0 else None return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids , \"sampler_devices\" : sampler_devices if mode == \"train\" else gpu_ids , \"observation_set\" : observation_set , \"render_video\" : render_video , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"observation_set\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = scenes_dir + \"*.json.gz\" if scenes_dir [ - 1 ] == \"/\" else scenes_dir + \"/*.json.gz\" scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = \"10.0\" return res This is it! If we copy all of the code into a file we should be able to run our experiment! Training Model On Debug Dataset # We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. First we need to change the dataset path to point to our small debug dataset. Modify these lines in your file # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" Note that we changed both the train and test dataset to debug, so the model will train and validate on the same 4 episodes. We can not train a model by running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Training Model On Full Dataset # We can also train the model on the full dataset by changing back our dataset path and running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> But be aware, training this takes nearly 2 days on a machine with 8 GPU. For our current setup the following command would work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Conclusion # In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#tutorial-pointnav-in-robothor","text":"","title":"Tutorial: PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#introduction","text":"One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short.","title":"Introduction"},{"location":"tutorials/training-a-pointnav-model/#pointnav","text":"At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the house to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings.","title":"Pointnav"},{"location":"tutorials/training-a-pointnav-model/#what-is-an-environment-anyways","text":"Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment.","title":"What is an environment anyways?"},{"location":"tutorials/training-a-pointnav-model/#learning-algorithm","text":"Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While allenact offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy, and this book by Sutton and Barto. Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward.","title":"Learning algorithm"},{"location":"tutorials/training-a-pointnav-model/#environemnt-setup","text":"To setup the RoboTHOR environment please consult the installation guide .","title":"Environemnt Setup"},{"location":"tutorials/training-a-pointnav-model/#dataset-setup","text":"To train the model on the PointNav task, we need to download the dataset and precomputed cache of distances to the target. The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes. The precomputed cache of distances is a large dictionary containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). We can download and extract the data by navigating to the pointnav_baselines project and running the following script: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor","title":"Dataset Setup"},{"location":"tutorials/training-a-pointnav-model/#config-file-setup","text":"Now comes the most important part of the tutorial, we are going to write an experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called main.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. We will start by creating a new directory inside the projects directory. We can name this whatever we want but for now, we will go with robothor_pointnav_tutuorial . Then we can create a directory called experiments inside the new directory we just created. This hierarchy is not necessary but it helps keep our experiments neatly organized. Now we create a file called pointnav_robothor_rgb_ddppo inside the experiments folder (again the name of this file is arbitrary). We start off by importing everything we will need: import glob from math import ceil from typing import Dict , Any , List , Optional import gym import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim.lr_scheduler import LambdaLR from torchvision import models from core.algorithms.onpolicy_sync.losses import PPO from core.algorithms.onpolicy_sync.losses.ppo import PPOConfig from projects.pointnav_baselines.models.point_nav_models import ( ResnetTensorPointNavActorCritic , ) from plugins.ithor_plugin.ithor_sensors import RGBSensorThor from core.base_abstractions.experiment_config import ExperimentConfig from core.base_abstractions.preprocessor import ObservationSet from core.base_abstractions.task import TaskSampler from plugins.habitat_plugin.habitat_preprocessors import ResnetPreProcessorHabitat from plugins.robothor_plugin.robothor_sensors import GPSCompassSensorRoboThor from plugins.robothor_plugin.robothor_task_samplers import PointNavDatasetTaskSampler from plugins.robothor_plugin.robothor_tasks import PointNavTask from utils.experiment_utils import Builder , PipelineStage , TrainingPipeline , LinearDecay Next we define a new experiment config class: class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { \"step_penalty\" : - 0.01 , \"goal_success_reward\" : 10.0 , \"failed_stop_reward\" : 0.0 , \"shaping_weight\" : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD = 10 ** 13 NUM_PROCESSES = 60 TRAINING_GPUS = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] VALIDATION_GPUS = [ 7 ] TESTING_GPUS = [ 7 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/val\" Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = \"rgb_lowres\" , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In allenact the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { \"input_height\" : SCREEN_SIZE , \"input_width\" : SCREEN_SIZE , \"output_width\" : 7 , \"output_height\" : 7 , \"output_dims\" : 512 , \"pool\" : False , \"torchvision_resnet_model\" : models . resnet18 , \"input_uuids\" : [ \"rgb_lowres\" ], \"output_uuid\" : \"rgb_resnet\" , \"parallel\" : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ \"rgb_resnet\" , \"target_coordinates_ind\" , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { \"ppo_loss\" : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ \"ppo_loss\" ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { \"lr_lambda\" : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES >= ndevices , \"NUM_PROCESSES {} < ndevices\" . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = \"train\" , ** kwargs ): if mode == \"train\" : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS render_video = False elif mode == \"valid\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS render_video = False elif mode == \"test\" : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS render_video = False else : raise NotImplementedError ( \"mode must be 'train', 'valid', or 'test'.\" ) # Disable parallelization for validation process if mode == \"valid\" : for prep in self . PREPROCESSORS : prep . kwargs [ \"parallel\" ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == 'train' or nprocesses > 0 else None return { \"nprocesses\" : nprocesses , \"gpu_ids\" : gpu_ids , \"sampler_devices\" : sampler_devices if mode == \"train\" else gpu_ids , \"observation_set\" : observation_set , \"render_video\" : render_video , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) -> nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ \"observation_set\" ] . observation_spaces , goal_sensor_uuid = \"target_coordinates_ind\" , rgb_resnet_preprocessor_uuid = \"rgb_resnet\" , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: path = scenes_dir + \"*.json.gz\" if scenes_dir [ - 1 ] == \"/\" else scenes_dir + \"/*.json.gz\" scenes = [ scene . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for scene in glob . glob ( path )] if total_processes > len ( scenes ): # oversample some scenes -> bias if total_processes % len ( scenes ) != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisible by the number of scenes\" ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( \"Warning: oversampling some of the scenes to feed all processes.\" \" You can avoid this by setting a number of workers divisor of the number of scenes\" ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { \"scenes\" : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), \"seed\" : seeds [ process_ind ] if seeds is not None else None , \"deterministic_cudnn\" : deterministic_cudnn , \"rewards_config\" : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . TRAIN_DATASET_DIR res [ \"loop_dataset\" ] = True res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) res [ \"allow_flipping\" ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = ( ( \"0. %d \" % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) > 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + '/episodes/' , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ \"scene_directory\" ] = self . VAL_DATASET_DIR res [ \"loop_dataset\" ] = False res [ \"env_args\" ] = {} res [ \"env_args\" ] . update ( self . ENV_ARGS ) res [ \"env_args\" ][ \"x_display\" ] = \"10.0\" return res This is it! If we copy all of the code into a file we should be able to run our experiment!","title":"Config File Setup"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-debug-dataset","text":"We can test if our installation worked properly by training our model on a small dataset of 4 episodes. This should take about 20 minutes on a computer with a NVIDIA GPU. First we need to change the dataset path to point to our small debug dataset. Modify these lines in your file # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/robothor/debug\" Note that we changed both the train and test dataset to debug, so the model will train and validate on the same 4 episodes. We can not train a model by running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> If using the same configuration as we have set up, the following command should work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model On Debug Dataset"},{"location":"tutorials/training-a-pointnav-model/#training-model-on-full-dataset","text":"We can also train the model on the full dataset by changing back our dataset path and running: python main.py -o <PATH_TO_OUTPUT> -c -b <BASE_DIRECTORY_OF_YOUR_EXPERIMENT> <EXPERIMENT_NAME> But be aware, training this takes nearly 2 days on a machine with 8 GPU. For our current setup the following command would work: python main.py -o projects/tutorials/pointnav_robothor_rgb/storage/ -b projects/tutorials/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model On Full Dataset"},{"location":"tutorials/training-a-pointnav-model/#conclusion","text":"In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"Conclusion"},{"location":"tutorials/training-pipelines/","text":"Tutorial: IL to RL with a training pipeline #","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/training-pipelines/#tutorial-il-to-rl-with-a-training-pipeline","text":"","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/transfering-to-a-different-environment-framework/","text":"Tutorial: Swapping in a new environment # Introduction # This tutorial was designed as a continuation of the Robothor Pointnav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another. RoboTHOR to iTHOR # Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/val\" We also have to download the iTHOR-Pointnav dataset, if we have not done so already: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor That's it! We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\" RoboTHOR to Habitat # Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = habitat . get_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = \"habitat/habitat-api/data/scene_datasets/\" CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters. Conclusion # In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Swapping environments"},{"location":"tutorials/transfering-to-a-different-environment-framework/#tutorial-swapping-in-a-new-environment","text":"","title":"Tutorial: Swapping in a new environment"},{"location":"tutorials/transfering-to-a-different-environment-framework/#introduction","text":"This tutorial was designed as a continuation of the Robothor Pointnav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another.","title":"Introduction"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-ithor","text":"Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/train\" VAL_DATASET_DIR = \"projects/pointnav_baselines/dataset/ithor/val\" We also have to download the iTHOR-Pointnav dataset, if we have not done so already: cd projects/pointnav_baselines/dataset sh download_pointnav_dataset.sh robothor That's it! We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return \"PointNavRobothorRGBPPO\"","title":"RoboTHOR to iTHOR"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-habitat","text":"Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = habitat . get_config ( \"configs/gibson.yaml\" ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = \"habitat/habitat-api/data/scene_datasets/\" CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ \"*\" ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ \"RGB_SENSOR\" ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = \"Nav-v0\" CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ \"POINTGOAL_WITH_GPS_COMPASS_SENSOR\" ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = \"POLAR\" CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\" CONFIG . TASK . MEASUREMENTS = [ \"DISTANCE_TO_GOAL\" , \"SPL\" ] CONFIG . TASK . SPL . TYPE = \"SPL\" CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = \"train\" This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) -> TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = \"validate\" config . freeze () return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) -> Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { \"env_config\" : config , \"max_steps\" : self . MAX_STEPS , \"sensors\" : self . SENSORS , \"action_space\" : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), \"distance_to_goal\" : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters.","title":"RoboTHOR to Habitat"},{"location":"tutorials/transfering-to-a-different-environment-framework/#conclusion","text":"In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Conclusion"}]}